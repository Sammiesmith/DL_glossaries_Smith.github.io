Concise Machine Learning
Jonathan Richard Shewchuk
May 5, 2025
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley
Berkeley, California 94720
Abstract
This report contains lecture notes for UC Berkeley’s introductory class on Machine Learning. It covers
many methods for classification and regression, including five and a half lectures on neural networks, and
a few methods for clustering and dimensionality reduction. It is concise because nothing is included that
cannot be written or spoken in a single semester’s lectures (with whiteboard lectures and almost no slides!)
and because the choice of topics is limited to a small selection of particularly useful, popular algorithms.
Supported in part by the National Science Foundation under Awards CCF-1423560 and CCF-1909204, in part by the University of
California Lab Fees Research Program, and in part by an Alfred P. Sloan Research Fellowship. The claims in this document are
those of the author. They are not endorsed by the sponsors or the U.S. Government.
Keywords: machine learning, classification, regression, density estimation, dimensionality reduction, clus-
tering, perceptrons, support vector machines (SVMs), Gaussian discriminant analysis, linear discriminant
analysis (LDA), quadratic discriminant analysis (QDA), logistic regression, decision trees, random forests,
ensemble learning, bagging, boosting, AdaBoost, neural networks, convolutional neural networks (CNNs,
ConvNets), residual neural networks (ResNets), batch normalization, AdamW, nearest neighbor search,
least-squares linear regression, logistic regression, polynomial regression, ridge regression, Lasso, bias-
variance decomposition, maximum likelihood estimation (MLE), principal components analysis (PCA),
singular value decomposition (SVD), random projection, latent factor analysis, latent semantic indexing,
k-means clustering, hierarchical clustering, spectral graph clustering, the kernel trick, learning theory
Contents
1 Introduction; Classification; Train, Validate, Test 1
2 Linear Classifiers, the Centroid Method, and Perceptrons 7
3 Perceptron Learning; Maximum Margin Classifiers 13
4 Soft-Margin Support Vector Machines; Features 18
5 Machine Learning Abstractions and Numerical Optimization 25
6 Decision Theory; Generative and Discriminative Models 31
7 Gaussian Discriminant Analysis; Maximum Likelihood Estimation 36
8 Eigenvectors and the (Anisotropic) Multivariate Normal Distribution 41
9 Anisotropic Gaussians: MLE, QDA, and LDA Revisited 47
10 Regression, including Least-Squares Linear and Logistic Regression 54
11 Polynomial and Weighted Regression; Newton’s Method; ROC Curves 59
12 Statistical Justifications; the Bias-Variance Decomposition 65
13 Shrinkage: Ridge Regression, Subset Selection, and Lasso 71
14 Decision Trees 76
15 More Decision Trees, Ensemble Learning, and Random Forests 81
16 Neural Networks 89
17 Vanishing Gradients; ReLUs; Output Units and Losses; Neurobiology 96
18 Neurobiology; Faster Neural Network Training 102
19 Convolutional Neural Networks 109
20 Unsupervised Learning: Principal Components Analysis 117
21 The Singular Value Decomposition; Clustering 126
i
22 The Pseudoinverse; Better Generalization for Neural Nets 134
23 Residual Networks; Batch Normalization; AdamW 140
24 Boosting; Nearest Neighbor Classification 146
25 Nearest Neighbor Algorithms: Voronoi Diagrams and k-d Trees 151
A Bonus Lecture: Learning Theory 157
B Bonus Lecture: The Kernel Trick 163
C Bonus Lecture: Spectral Graph Clustering 168
D Bonus Lecture: Multiple Eigenvectors; Latent Factor Analysis 176
E Bonus Lecture: High Dimensions; Random Projection 183
ii
About this Report
This report compiles my lectures notes for UC Berkeley’s class CS 189 /289A, Machine Learning , which
is both an undergraduate and introductory graduate course. I hope it will serve as a fast introduction to
the subject for readers who are already comfortable with vector calculus, linear algebra, probability, and
statistics. Please consult my CS 189 /289A web page1as an addendum to this report; it includes an extended
description of each lecture and additional web links and reading assignments related to the lectures. Consider
this report and the web page to be living documents; both will be refined a bit every time I teach the class.
The term “lecture notes” has shifted to include long textbook-style treatments written by professors as
supplements to their classes. Not so here. This report compiles the actual notes that I lecture from. I call
itConcise Machine Learning because I include almost nothing that I do not have time to write or speak
during one fourteen-week semester of twice-weekly 80-minute lectures. (After holidays and the midterm
exam, that amounts to 25 lectures.) Words that appear [in brackets] are spoken; everything else is written on
the “whiteboard”—in my class, a tablet computer. My whiteboard software permits me to incorporate (and
write on) figures, included here. However, I am largely anti-Powerpoint and I resort to prepared slides for
just one brief segment during the semester (to discuss the V1 visual cortex).
These notes might be ideal for mathematically sophisticated readers who want to learn the basics of machine
learning as quickly as possible. But they’re not ideal for everybody. The time limitation necessitates that
many details are omitted. I think that the most mathematically well-prepared readers will be able to fill in
those details themselves. But many readers, including most students who take the class, will need additional
readings or discussion sections for greater detail. My class web page lists additional readings for most of
the lectures, many of them from three textbooks that have been kindly made available for free on the web:
An Introduction to Statistical Learning with Applications in R ,2second edition, by Gareth James, Daniela
Witten, Trevor Hastie, and Robert Tibshirani, Springer, New York, 2021, ISBN # 978-1-0716-1417-4; The
Elements of Statistical Learning: Data Mining, Inference, and Prediction ,3second edition, by Trevor Hastie,
Robert Tibshirani, and Jerome Friedman, Springer, New York, 2008; and Deep Learning4by Christopher
M. Bishop with Hugh Bishop, Springer, 2024. Readers wanting the verbose kind of “lecture notes” should
consider the fine ones written by Stanford University’s Andrew Ng.5I have no interest in duplicating these
efforts; instead, I’m aiming for the neglected niche of “shortest introduction.” (And perhaps also “best stolen
illustrations.”)
The other thing that makes this report concise is the choice of topics. CS 189 /289A was introduced at UC
Berkeley in the spring of 2013 by Prof. Jitendra Malik, and most of his topic choices remain intact here.
Jitendra told me that he only taught a machine learning algorithm if he or his collaborators had used it
successfully for some application. He said, “the machine learning course is too important to leave to the
machine learning experts”—that is, users of machine learning algorithms often have a more clear-eyed view
of their usefulness than inventors of machine learning algorithms.
I thank Peter Bartlett, Alyosha Efros, Isabelle Guyon, and Jitendra Malik—the previous teachers of CS
189/289A—for their lectures and lecture notes, from which I learned the topic myself. While I’ve given the
lectures my own twist and rearranged the material a lot, I am ultimately making incremental improvements
(and perhaps incremental worsenings) to a structure they handed down to me.
1https: //people.eecs.berkeley.edu /∼jrs/189/
2https: //www.statlearning.com
3https: //hastie.su.domains /ElemStatLearn /
4https: //www.bishopbook.com
5http://cs229.stanford.edu /notes2020spring /
iii
iv
1 Introduction; Classification; Train, Validate, Test
CS 189 /289A [Spring 2025]
Machine Learning
Jonathan Shewchuk
https: //people.eecs.berkeley.edu /∼jrs/189/
Homework 1 due next Wednesday.
Questions: Please use Ed Discussion, not email. [Ed Discussion has an option for private questions, but
please use public for most questions so other people can benefit.]
For personal matters only, jrs@berkeley.edu
Discussion sections (Tue & Wed):
Attend any section. [We’ll put up a list on Ed Discussion.]
[We might have a few advanced sections, including research discussion or exam problem preparation.]
Sections start Tuesday. [Next week.]
[Enrollment: 736 students max. 349 waitlisted. Expecting many drops. EECS grads have highest priority;
CD/DS undergrads second; non-EECS grads third; a few concurrent enrollment students will be admitted.]
[Textbooks: Available free online. Linked from class web page.]
Springer Texts in Statistics
Gareth James
Daniela Witten
Trevor Hastie
Robert Tibshirani
An Introduction 
to Statistical 
Learning
with Applications in R
Second/uni00A0Edition
Springer Series in Statistics
Trevor Hastie
Robert Tibshirani
Jerome FriedmanSpringer Series in StatisticsThe Elements of
Statistical Learning
Data Mining, Inference, and PredictionThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-
nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-
gy, finance, and marketing. The challenge of understanding these data has led to the devel-
opment of new tools in the field of statistics, and spawned new areas such as data mining,
machine learning, and bioinformatics. Many of these tools have common underpinnings but
are often expressed with different terminology. This book describes the important ideas in
these areas in a common conceptual framework. While the approach is statistical, the
emphasis is on concepts rather than mathematics. Many examples are given, with a liberal
use of color graphics. It should be a valuable resource for statisticians and anyone interested
in data mining in science or industry. The book’s coverage is broad, from supervised learning
(prediction) to unsupervised learning. The many topics include neural networks, support
vector machines, classification trees and boosting—the first comprehensive treatment of this
topic in any book.
This major new edition features many topics not covered in the original, including graphical
models, random forests, ensemble methods, least angle regression & path algorithms for the
lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on
methods for “wide” data (p bigger than n), including multiple testing and false discovery rates.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at
Stanford University. They are prominent researchers in this area: Hastie and Tibshirani
developed generalized additive models and wrote a popular book of that title. Hastie co-
developed much of the statistical modeling software and environment in R/S-PLUS and
invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the
very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-
mining tools including CART, MARS, projection pursuit and gradient boosting.
›springer.comSTATISTICS
ISBN 978 -0-387-84857 -0Trevor Hastie • Robert Tibshirani • Jerome Friedman
The Elements of Statictical Learning
Hastie • Tibshirani • Friedman
Second Edition
Prerequisites
Vector calculus: Math 53 [or another vector calculus course]
Linear algebra: Math 54, Math 110, or EE 16A +16B [or another linear algebra course]
Probability: CS 70, EECS 126, or Stat 134 [or another probability course]
Plentiful programming experience [TAs have no obligation to look at your code.]
NOT CS 188
2 Jonathan Richard Shewchuk
Grading: 189
40% 7 Homeworks. Late policy: 5 slip days total
20% Midterm: Monday, March 17, 7:00–9:00 PM
40% Final Exam: Friday, May 16, 3–6 PM (not on Berkeley time)
Grading: 289A
40% HW
20% Midterm
20% Final
20% Project
Cheating
– Discussion of HW problems is encouraged. Showing other students small amounts of code is okay.
– All homeworks, including programming, must be written individually.
– All code must be typed by you. Do not use LLMs, Autopilot, or chatbots to autocomplete or write
code, nor to answer math problems.
– You may use LLMs or chatbots to help debugging or understanding, but you MUST include a com-
plete transcript of the conversation in an appendix at the end of your homework.
– We will actively check for plagiarism.
– Typical penalty is a large NEGATIVE score, but I reserve right to give an instant F for even one
violation, and will always give an F for two.
[Last year, we had to punish 12 people for cheating. It was not fun. Please don’t make me do it again.]
CORE MATERIAL
– Finding patterns in data; using them to make predictions.
– Models and statistics help us understand patterns.
– Optimization algorithms “learn” the patterns.
[The most important part of this is the data. Data drives everything else.
You cannot learn much if you don’t have enough data.
You cannot learn much if your data has bad quality.
But it’s amazing what you can do if you have lots of good data.
Machine learning has changed a lot in the last two decades because the internet has made truly vast quantities
of data available. For instance, with a little patience you can download tens of millions of photographs. Then
you can build a 3D model of Paris.
Neural networks had fallen out of favor early this millennium, but they came back big around 2012 because
researchers found that they work so much better when you have vast quantities of data.]
Introduction; Classification; Train, Validate, Test 3
CLASSIFICATION
– Collect training points with class labels: reliable debtors & defaulted debtors
– Evaluate new applicants—predict their class
4.2 Why Not Linear Regression? 129
BalanceIncomeDefaultDefault0 500 1000 1500 2000 25000 20000 40000 60000No Yes0 500 1000 1500 2000 2500Balance
No Yes0 20000 40000 60000Income
FIGURE 4.1.TheDefaultdata set.Left:The annual incomes and monthlycredit card balances of a number of individuals. The individuals who defaulted ontheir credit card payments are shown in orange, and those who did not are shownin blue.Center:Boxplots ofbalanceas a function ofdefaultstatus.Right:Boxplots ofincomeas a function ofdefaultstatus.4.2 Why Not Linear Regression?We have stated that linear regression is not appropriate in the case of aqualitative response. Why not?Suppose that we are trying to predict the medical condition of a patientin the emergency room on the basis of her symptoms. In this simpliﬁedexample, there are three possible diagnoses:stroke,drug overdose,a n depileptic seizure.W ec o u l dc o n s i d e re n c o d i n gt h e s ev a l u e sa saq u a n t i t a -tive response variable,Y,a sf o l l o w s :Y=⎧⎪⎨⎪⎩1 ifstroke;2 ifdrug overdose;3 ifepileptic seizure.Using this coding, least squares could be used to ﬁt a linear regression modelto predictYon the basis of a set of predictorsX1,...,Xp.U n f o r t u n a t e l y ,this coding implies an ordering on the outcomes, puttingdrug overdoseinbetweenstrokeandepileptic seizure,a n di n s i s t i n gt h a tt h ed iﬀerencebetweenstrokeanddrug overdoseis the same as the diﬀerence betweendrug overdoseandepileptic seizure.I np r a c t i c et h e r ei sn op a r t i c u l a rreason that this needs to be the case. For instance, one could choose anequally reasonable coding,Y=⎧⎪⎨⎪⎩1 ifepileptic seizure;2 ifstroke;3 ifdrug overdose.
creditcardscrop.pdf (ISL, Figure 4.1) [The problem of classification. We are given data
points, each belonging to one of two classes: orange crosses represent people who de-
faulted on their credit cards, and blue circles represent those who didn’t. Then we are given
additional points whose class is unknown, and we are asked to predict what class each new
point is in. Given the credit card balance and annual income of new applicants, predict
whether they will default on their debt.]
decision boundary
[Draw this figure by hand. classify.pdf ]
[Draw 2 colors of dots, almost but not quite linearly separable.]
[“How do we classify a new point?” Draw a point in a third color.]
[One possibility: look at its nearest neighbor.]
[Another possibility: draw a linear decision boundary ; label it.]
[Those are two di fferent models for the nature of this data.]
4 Jonathan Richard Shewchuk
[We’ll learn some ways to compute linear decision boundaries in the next several lectures. But for now, let’s
compare these two methods.]
16 2. Overview of Supervised Learning1−Nearest Neighbor Classifier
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
ooooo
ooo
o
o
ooo
oooooo
oo
o
oo
ooo
o
FIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-
ure 2.1. The classes are coded as a binary variable (BLUE =0,ORANGE =1 ) ,a n d
then predicted by 1-nearest-neighbor classiﬁcation.
2.3.3 From Least Squares to Nearest Neighbors
The linear decision boundary from least squares is very smoo th, and ap-
parently stable to ﬁt. It does appear to rely heavily on the as sumption
that a linear decision boundary is appropriate. In language we will develop
shortly, it has low variance and potentially high bias.
On the other hand, the k-nearest-neighbor procedures do not appear to
rely on any stringent assumptions about the underlying data ,a n dc a na d a p t
to any situation. However, any particular subregion of the d ecision bound-
ary depends on a handful of input points and their particular positions,
and is thus wiggly and unstable—high variance and low bias.
Each method has its own situations for which it works best; in particular
linear regression is more appropriate for Scenario 1 above, while nearest
neighbors are more suitable for Scenario 2. The time has come to expose
the oracle! The data in fact were simulated from a model somew here be-
tween the two, but closer to Scenario 2. First we generated 10 means mk
from a bivariate Gaussian distribution N((1,0)T,I)a n dl a b e l e dt h i sc l a s s
BLUE.S i m i l a r l y ,1 0m o r ew e r ed r a w nf r o m N((0,1)T,I)a n dl a b e l e dc l a s s
ORANGE .T h e nf o re a c hc l a s sw eg e n e r a t e d1 0 0o b s e r v a t i o n sa sf o l l o w s: for
each observation, we picked an mkat random with probability 1 /10, and
2.3 Least Squares and Nearest Neighbors 13Linear Regression of 0/1 Response
.................................................................................................................................................................................................................................................................................................................. ........................................................ ............................................................. ................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................
................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... .................................................................... ............................................................... ......................................................... ...........................................................................................................................................................................................................................
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
FIGURE 2.1. A classiﬁcation example in two dimensions. The classes are code d
as a binary variable ( BLUE =0,ORANGE =1), and then ﬁt by linear regression.
The line is the decision boundary deﬁned by xTˆβ=0.5.T h eo r a n g es h a d e dr e g i o n
denotes that part of input space classiﬁed as ORANGE , while the blue region is
classiﬁed as BLUE .
The set of points in I R2classiﬁed as ORANGE corresponds to {x:xTˆβ>0.5},
indicated in Figure 2.1, and the two predicted classes are se parated by the
decision boundary {x:xTˆβ=0.5},w h i c hi sl i n e a ri nt h i sc a s e .W es e e
that for these data there are several misclassiﬁcations on b oth sides of the
decision boundary. Perhaps our linear model is too rigid— or are such errors
unavoidable? Remember that these are errors on the training data itself,
and we have not said where the constructed data came from. Con sider the
two possible scenarios:
Scenario 1: The training data in each class were generated from bivariat e
Gaussian distributions with uncorrelated components and d iﬀerent
means.
Scenario 2: The training data in each class came from a mixture of 10 low-
variance Gaussian distributions, with individual means th emselves
distributed as Gaussian.
Am i x t u r eo fG a u s s i a n si sb e s td e s c r i b e di nt e r m so ft h eg e n e r ative
model. One ﬁrst generates a discrete variable that determin es which of
classnear.pdf, classlinear.pdf (ESL, Figures 2.3 & 2.1) [Two examples of classifiers for
the same data: a nearest neighbor classifier (left) and a linear classifier (right). The decision
boundaries are in black.]
[At left we have a nearest neighbor classifier , which classifies a new point by finding the nearest point
in the training data, and assigning it the same class. At right we have a linear classifier , which guesses
that everything above the line is brown, and everything below the line is blue. At right, the linear decision
boundary—the black line—is explicitly computed by the classifier. At left, the decision boundary is not
computed; the classifier just takes a new point and computes the distances to all the training points.]
[The neighbor classifier at left has a big advantage: it classifies all the training data correctly, whereas the
linear classifier does not. But the linear classifier has an advantage too. Somebody please tell me what.]
16 2. Overview of Supervised Learning1−Nearest Neighbor Classifier
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
ooooo
ooo
o
o
ooo
oooooo
oo
o
oo
ooo
o
FIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-
ure 2.1. The classes are coded as a binary variable (BLUE =0,ORANGE =1 ) ,a n d
then predicted by 1-nearest-neighbor classiﬁcation.
2.3.3 From Least Squares to Nearest Neighbors
The linear decision boundary from least squares is very smoo th, and ap-
parently stable to ﬁt. It does appear to rely heavily on the as sumption
that a linear decision boundary is appropriate. In language we will develop
shortly, it has low variance and potentially high bias.
On the other hand, the k-nearest-neighbor procedures do not appear to
rely on any stringent assumptions about the underlying data ,a n dc a na d a p t
to any situation. However, any particular subregion of the d ecision bound-
ary depends on a handful of input points and their particular positions,
and is thus wiggly and unstable—high variance and low bias.
Each method has its own situations for which it works best; in particular
linear regression is more appropriate for Scenario 1 above, while nearest
neighbors are more suitable for Scenario 2. The time has come to expose
the oracle! The data in fact were simulated from a model somew here be-
tween the two, but closer to Scenario 2. First we generated 10 means mk
from a bivariate Gaussian distribution N((1,0)T,I)a n dl a b e l e dt h i sc l a s s
BLUE.S i m i l a r l y ,1 0m o r ew e r ed r a w nf r o m N((0,1)T,I)a n dl a b e l e dc l a s s
ORANGE .T h e nf o re a c hc l a s sw eg e n e r a t e d1 0 0o b s e r v a t i o n sa sf o l l o w s: for
each observation, we picked an mkat random with probability 1 /10, and
2.3 Least Squares and Nearest Neighbors 1515-Nearest Neighbor Classifier
................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... .................................................................... .................................................................... ................................................................... ................................................................... ................................................................... ................................................................... .................................................................. .................................................................. .................................................................. ................................................................ ............................................................... .............................................................. ............................................................. ............................................................ ............................................................ ............................................................ .......... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..................................... .................... ..... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................. .................................................. ................................................ ............................................. ........................................... ........................................... ......................................... ........................................ ....................................... .................................. .................................... .................................... ................................... .................................. .................................. ................................. ................................. ................................ .............................. ............................ ........................... ......................... ........................ . . . . . . . . . . . . . . . . . . . . ............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
............................................................................................. ............ ................ . .................. .................. ..................... ....................... .......................... .......................... ........................... ........................... .............................. .................................. .................................. ................................ ................................. ................................... ................................... .................................... .................................... .................................... ...................................... ........................................ .......................................... ............................................ ............................................. ..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
FIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-
ure 2.1. The classes are coded as a binary variable (BLUE =0,ORANGE =1 ) and
then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted clas si sh e n c e
chosen by majority vote amongst the 15-nearest neighbors.
In Figure 2.2 we see that far fewer training observations are misclassiﬁed
than in Figure 2.1. This should not give us too much comfort, t hough, since
in Figure 2.3 none of the training data are misclassiﬁed. A little thought
suggests that for k-nearest-neighbor ﬁts, the error on the training data
should be approximately an increasing function of k,a n dw i l la l w a y sb e0
fork=1 .A ni n d e p e n d e n tt e s ts e tw o u l dg i v eu sam o r es a t i s f a c t o r y means
for comparing the di ﬀerent methods.
It appears that k-nearest-neighbor ﬁts have a single parameter, the num-
ber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-
though this is the case, we will see that the eﬀective number of parameters
ofk-nearest neighbors is N/kand is generally bigger than p,a n dd e c r e a s e s
with increasing k.T og e ta ni d e ao fw h y ,n o t et h a ti ft h en e i g h b o r h o o d s
were nonoverlapping, there would be N/k neighborhoods and we would ﬁt
one parameter (a mean) in each neighborhood.
It is also clear that we cannot use sum-of-squared errors on t he training
set as a criterion for picking k, since we would always pick k=1 !I tw o u l d
seem that k-nearest-neighbor methods would be more appropriate for th e
mixture Scenario 2 described above, while for Gaussian data the decision
boundaries of k-nearest neighbors would be unnecessarily noisy.
classnear.pdf, classnear15.pdf (ESL, Figures 2.3 & 2.2) [A 1-nearest neighbor classifier
and a 15-nearest neighbor classifier .
[The 15-nearest neighbor classifier classifies a new point by looking at its 15 nearest neighbors and letting
them vote for the correct class.]
[The left figure is an example of what’s called overfitting . In the left figure, observe how intricate the
decision boundary is that separates the positive examples from the negative examples. It’s a bit too intricate
to reflect reality. In the right figure, the decision boundary is smoother. Intuitively, that smoothness is
probably more likely to correspond to reality.]
Introduction; Classification; Train, Validate, Test 5
Classifying Digits
Classiﬁca9on(Pipeline(• Collect(Training(Images(– Posi9ve:((– Nega9ve:((• Training(Time(– Compute(feature(vectors(for(posi9ve(and(nega9ve(example(images(– Train(a(classiﬁer(• Test(Time(– Compute(feature(vector(on(new(test(image:((– Evaluate(classiﬁer((
sevensones.pdf [In the MNIST digit recognition problem, we are given handwritten digits,
and we are asked to learn to distinguish them. See Homework 1.]
Express these images as vectors
3333
0023
0013
3333→3
3
3
3
0
0
2
3
0
0
1
3
3
3
3
3
Images are points in 16-dimensional space. Linear decision boundary is a hyperplane .
TRAIN, V ALIDATE, TEST
How we classify:
– We are given labeled data —sample points with class labels.
– Hold back a subset of the labeled points, called the validation set . Maybe 20%. The other 80% is the
training set . [Warning: the term training data is not used consistently. Often “training data” refers to
allthe labeled data. You have to judge from context.]
– Train one or more classifiers : they learn to distinguish 7 from not 7. Use training set to learn model
weights. Do NOT use validation set to train!!!
– Usually, train multiple learning algorithms, or one algorithm with multiple hyperparameter settings,
or both [using the same training set for each].
– Validate the trained classifiers on the validation set. Choose classifier /hyperparameters with lowest
validation error. Called validation . [When we do validation, we are not learning any more. We are
checking what classes our trained classifiers assign to our validation set, and counting how often
they’re right. We use this to judge our models—not how well they remember the training set labels.]
– Optional: Test the best classifier on a test set of NEW data. Final evaluation. Typically you do NOT
have the labels. [But somebody else might have them, and assign you a score!]
6 Jonathan Richard Shewchuk
[When I underline a word or phrase, that usually means it’s a definition. My advice to you is to memorize
the definitions I cover in class.]
3 kinds of error:
– Training error : fraction of training set not classified correctly. [This is zero with the 1-nearest neighbor
classifier, but nonzero with the 15-nearest neighbor and linear classifiers. But that doesn’t mean the
1-nearest neighbor classifier is always better. Remember that you cannot include the validation data
in this calculation, even if somebody calls it “training data.”]
– Validation error : fraction of validation set misclassified. Use this to choose classifier /hyperparameters.
[You didn’t use the validation set to train, so even the 1-nearest neighbor classifier can classify these
points wrong. Validation error is almost always higher than training error.]
– Test error : fraction of test set misclassified. Used to evaluate you.
Most ML algorithms have a few hyperparameters that control over /underfitting, e.g. kink-nearest neighbors.
error 
ratek: # of nearest neighbors0.10 0.15 0.20 0.25 0.30151 101  69  45  31  21  11   7   5   3   1
Train
Test
BayesLinear
overﬁt!
best (7)underﬁt
test error
training error
overfitlabeled.pdf (modified from ESL, Figure 2.4)
– overfitting : when the validation /test error deteriorates because the classifier becomes too sensitive to
outliers or other spurious patterns.
– underfitting : when the validation /test error deteriorates because the classifier is not flexible enough to
fit patterns.
– outliers : points with atypical labels (e.g., rich borrower who defaulted anyway). Increase risk of
overfitting.
[In machine learning, the goal is to create a classifier that generalizes to new examples we haven’t seen yet.
Overfitting and underfitting are both counterproductive to that goal. So we’re always seeking a compromise:
we want decision boundaries that make fine distinctions without being downright superstitious.]
Kaggle.com:
– Runs ML competitions, including our HWs
– We may use 2 test sets:
public set: test scores available during competition
private set: test scores available after competition
[The private test set prevents you from “cheating” by throwing lots of models at the public test set
until you find a lucky one.]
Linear Classifiers, the Centroid Method, and Perceptrons 7
2 Linear Classifiers, the Centroid Method, and Perceptrons
CLASSIFIERS
You are given sample ofnobservations [aka examples ], each with dfeatures [aka predictors ].
Some observations belong to class C; some do not.
Example: Observations are ice cream lovers
Features are height & age ( d=2)
Some are in class “chocolate,” some prefer vanilla
Goal: Predict preferred flavor based on their height & age.
Represent each observation as a point in d-dimensional space,
called a sample point /a feature vector /independent variables .
C
CVV
V
V
VVV
VV
CC
CCC
C
CVV
VV
C
CC VVV
CCC
CC
VC V
VV
VV
V
VCCC
CC
C
overfitting
ageheight height
ageheight
age
[Draw this by hand; decision boundaries last. classify3.pdf ]
[We draw these lines /curves separating C’s from V’s. Then we use these curves to predict which future
borrowers will default. In the last example, though, we’re probably overfitting, which could hurt our predic-
tions.]
decision boundary : the boundary chosen by our classifier to separate items in the class from those not.
overfitting : When decision boundary fits spurious detail so well that it doesn’t classify future points well.
[A reminder that underlined phrases are definitions, worth memorizing.]
Some (not all) classifiers work by computing a
decision function : A function f(x) that maps a point xto a scalar such that
f(x)>0 if x∈class C;
f(x)≤0 if x<class C.
Aka predictor function or discriminant function .
For these classifiers, the decision boundary is {x∈Rd:f(x)=0}
[That is, the set of all points where the decision function is zero.]
Usually, this set is a ( d−1)-dimensional surface in Rd.
{x:f(x)=0}is also called an isosurface offfor the isovalue 0.
fhas other isosurfaces for other isovalues, e.g., {x:f(x)=1}.
8 Jonathan Richard Shewchuk
-2-1
01
23
4444
5555
-6 -4 -2 0 2 4 6-6-4-20246
radiusplot.pdf, radiusiso.pdf [3D plot and isocontour plot of the cone] f(x,y)=p
x2+y2−3.
[Imagine a decision function in Rd, and imagine its ( d−1)-dimensional isosurfaces.]
radiusiso3d.pdf
[One of these spheres could be the decision boundary.]
linear classifier : The decision boundary is a line /plane.
Usually uses a linear decision function.
Linear Classifiers, the Centroid Method, and Perceptrons 9
Linear Classifier Math
[I will write vectors in matrix notation.]
Vectors: x=x1
x2
x3
x4
x5=[x1x2x3x4x5]⊤
Think of xas a point in 5-dimensional space.
Conventions (often, but not always):
uppercase roman =matrix, random variable, set X
lowercase roman =vector x
Greek =real scalar α
Some integers: n=# of sample points
d=# of features (per point)
=dimension of sample points
i j k=indices
function (often scalar) f( ),s( ), . . .
Euclidean inner product (aka dot product ):x·y=x1y1+x2y2+...+xdyd
also written x⊤y
Clearly, f(x)=w·x+αis a linear function inx.
Euclidean norm :∥x∥=√x·x=q
x2
1+x2
2+...+x2
d
∥x∥is the length (aka Euclidean length ) of a vector x.
Given a vector x,0,x
∥x∥is a unit vector (length 1).
“Normalize a vector x”: replace xwithx
∥x∥.
Use dot products to compute angles:
θxy
cosθ=x·y
∥x∥∥y∥=x
∥x∥|{z}
length 1·y
∥y∥|{z}
length 1
obtuse
x·y>0 x·y=0acute right
(orthogonal)
x·y<0
Given a linear decision function f(x)=w·x+α, the decision boundary is
H={x:w·x=−α}.
The set His called a hyperplane . (A line in 2D, a plane in 3D.)
[A hyperplane is what you get when you generalize the idea of a plane to higher dimensions. The three most
important things to understand about a hyperplane is (1) it has dimension d−1 and it cuts the d-dimensional
space into two halves; (2) it’s flat; and (3) it’s infinite.]
10 Jonathan Richard Shewchuk
Theorem: Let x,ybe 2 points that lie on H. Then w·(y−x)=0.
Proof: w·(y−x)=−α−(−α)=0. [Therefore, wis orthogonal to any line segment that lies on H.]
wis called the normal vector ofH,
because (as the theorem shows) wis normal (perpendicular) to H.
[I.e., wis perpendicular to every line on H.]
w
w·x=−2w·x=1
w·x=0[Draw black part first, then red parts. hyperplane.pdf ]
Ifwis a unit vector, then f(x)=w·x+αis the signed distance from xtoH. [See Discussion 1.]
I.e., positive on w’s side of H; negative on other side.
Moreover, the distance from Hto the origin is α. [How do we know that?]
Henceα=0 if and only if Hpasses through origin.
[wdoes not have to be a unit vector for the classifier to work.
Ifwis not a unit vector, w·x+αis the signed distance times some real.
If you want to fix that, you can rescale the equation by computing ∥w∥and dividing both wandαby∥w∥.]
The coe fficients in w, plusα, are called weights (or parameters or regression coe fficients ).
[That’s why we call the vector w; “w” stands for “weights.”]
The training points are linearly separable if there exists a hyperplane that correctly classifies all the training
points.
[At the beginning of this lecture, I showed you one plot that’s linearly separable and two that are not.]
[We will investigate some linear classifiers that only work for linearly separable data and some that do a
decent job with non-separable data. Obviously, if your data are not linearly separable, a linear classifier
cannot do a perfect job. But we’re still happy if we can find a classifier that usually predicts correctly.]
A Simple Classifier
Centroid method : compute mean µCof all training points in class C and mean µXof all points NOT in C.
We use the decision function
f(x)=(µC−µX)|     {z     }
normal vector·x−(µC−µX)·µC+µX
2|    {z    }
midpoint between µC,µX
so the decision boundary is the hyperplane that bisects line segment w /endpointsµC,µX.
Linear Classifiers, the Centroid Method, and Perceptrons 11
XXX
C CCC
CX
CX
X
[Draw data, then µC,µX, then line & normal. centroid.pdf ]
[In this example, there’s clearly a linear classifier that classifies every training point correctly, and the cen-
troid method isn’t it.
Note that this is hardly the worst example I could have given.
If you’re in the mood for an easy puzzle, pull out a sheet of paper and think of an example, with lots of
training points, where the centroid method misclassifies every training point but two.]
[Nevertheless, there are circumstances where this method works well, like when all your positive examples
come from one Gaussian distribution, and all your negative examples come from another.]
[We can sometimes improve this classifier by adjusting the scalar term αto minimize the number of mis-
classified points. Then the hyperplane has the same normal vector, but a di fferent position.]
Perceptron Algorithm (Frank Rosenblatt, 1957)
Slow, but correct for linearly separable points.
Uses a numerical optimization algorithm, namely, gradient descent .
[Poll:
How many of you know what gradient descent is?
How many of you know what the backpropagation algorithm is?
How many of you know what a linear program is?
How many of you know what a quadratic program is?
We’re going to learn what these things are. As machine learning people, we will be heavy users of optimiza-
tion methods. Unfortunately, I won’t have time to teach you algorithms for many optimization problems,
but we’ll learn a few. To learn more, take EECS 127.]
Consider nsample points X1,X2, ...,Xn.
[The reason I’m using capital Xhere is because we typically store these vectors in a matrix X.]
For each sample point, the label yi=(1 if Xi∈class C, and
−1 if Xi<C.
For simplicity, consider only decision boundaries that pass through the origin . (We’ll fix this later.)
12 Jonathan Richard Shewchuk
Goal: find weights wsuch that
Xi·w≥0 if yi=1, and
Xi·w≤0 if yi=−1. [remember, Xi·wis the signed distance]
Equivalently: yiXi·w≥0.←inequality called a constraint .
Idea: We define a risk function Rthat is positive if some constraints are violated. Then we use optimization
to choose wthat minimizes R. [That’s how we train a perceptron classifier.]
Define the loss function
L(ˆy,yi)=(0 if yiˆy≥0, and
−yiˆyotherwise.
[Here, ˆ yis the classifier’s prediction, and yiis the correct answer, called the label .]
If ˆyhas the same sign as yi, the loss function is zero (happiness).
But if ˆ yhas the wrong sign, the loss function is positive.
[For each training point, you want to get the loss function down to zero, or as close to zero as possible. It’s
called the “loss function” because the bigger it is, the bigger a loser your classifier is.]
Define risk function (aka objective function or cost function )
R(w)=1
nnX
i=1L(Xi·w,yi)
=1
nX
i∈V−yiXi·w where Vis the set of indices ifor which yiXi·w<0.
Ifwclassifies all X1,..., Xncorrectly, then R(w)=0.
Otherwise, R(w) is positive, and we want to find a better w.
Goal: Solve this optimization problem :
Find wthat minimizes R(w).
riskplot.pdf [Plot of risk R(w). Every point in the dark green flat spot is a minimum. We’ll
look at this more next lecture.]
Perceptron Learning; Maximum Margin Classifiers 13
3 Perceptron Learning; Maximum Margin Classifiers
Perceptron Algorithm (cont’d)
Recall:
– linear decision fn f(x)=w·x (for simplicity, no α)
– decision boundary {x:f(x)=0} (a hyperplane through the origin)
– sample points X1,X2,..., Xn∈Rd; class labels y1,..., yn=±1
– goal: find weights wsuch that yiXi·w≥0
– goal, revised: find wthat minimizes R(w)=X
i∈V−yiXi·w [risk function]
where V={i:yiXi·w<0}.
[Our original problem was to find a separating hyperplane in one space, which I’ll call x-space. But we’ve
transformed this into a problem of finding an optimal point in a di fferent space, which I’ll call w-space. It’s
important to understand transformations like this, where a geometric structure in one space becomes a point
in another space.]
Objects in x-space transform to objects in w-space:
x-space w-space
hyperplane:{z:w·z=0}point: w
point: x hyperplane:{z:x·z=0}
Point xlies on hyperplane {z:w·z=0}⇔w·x=0⇔point wlies on hyperplane {z:x·z=0}inw-space.
[So a hyperplane transforms to a point that represents its normal vector. And a sample point transforms to
the hyperplane whose normal vector is the sample point.]
[In this algorithm, the transformations happen to be symmetric: a hyperplane in x-space transforms to a
point in w-space the same way that a hyperplane in w-space transforms to a point in x-space. That won’t
always be true for the decision boundaries we use this semester.]
If we want to enforce inequality x·w≥0, that means
– in x-space, xshould be on the same side of {z:w·z=0}asw
– in w-space, w” ” ” ” ” ” ” {z:x·z=0}asx
CXX
x-space w-space
w w[Draw this by hand. xwspace.pdf ]
[Observe that the x-space sample
points are the normal vectors for the
w-space lines. We can choose wto be
anywhere in the shaded region.]
[For a sample point xin class C, wandxmust be on the same side of the hyperplane that xtransforms into.
For a point xnot in class C (marked by an X), wandxmust be on opposite sides of the hyperplane that x
transforms into. These rules determine the shaded region above, in which wmust lie.]
[Again, what have we accomplished? We have switched from the problem of finding a hyperplane in x-
space to the problem of finding a point in w-space. That’s a better fit to how we think about optimization
algorithms.]
14 Jonathan Richard Shewchuk
[Let’s take a look at the risk function these three sample points create.]
-4 -2 0 2 4-4-2024
riskplot.pdf, riskiso.pdf [Plot & isocontours of risk R(w). Note how R’s creases match the
lines in the w-space drawn above.]
[In this plot, we can choose wto be any point in the bottom pizza slice; all those points minimize R.]
[We have an optimization problem; we need an optimization algorithm to solve it.]
An optimization algorithm : gradient descent onR.
[Draw the typical steps of gradient descent on the plot of R.]
Given a starting point w, find gradient of Rwith respect to w; this is the direction of steepest ascent.
Take a step in the opposite direction. Recall [from your vector calculus class]
∇R(w)=∂R
∂w1∂R
∂w2...
∂R
∂wdand∇w(z·w)=z1
z2
...
zd=z
∇R(w)=∇X
i∈V−yiXi·w=−X
i∈VyiXi
At any point w, we walk downhill in direction of steepest descent, −∇R(w).
w←arbitrary nonzero starting point (good choice is any yiXi)
while R(w)>0
V←set of indices ifor which yiXi·w<0
w←w+ϵX
i∈VyiXi
return w
ϵ >0 is the step size aka learning rate , chosen empirically. [Best choice depends on input problem!]
Problem: Slow! Each step takes O(nd) time. [Can we improve this?]
Perceptron Learning; Maximum Margin Classifiers 15
Optimization algorithm 2: stochastic gradient descent
Idea: each step, pick onemisclassified Xi;
do gradient descent on loss fn L(Xi·w,yi).
Called the perceptron algorithm . Each step takes O(d) time.
[Not counting the time to search for a misclassified Xi.]
while some yiXi·w<0
w←w+ϵyiXi
return w
[Stochastic gradient descent is quite popular and we’ll see it several times more this semester, especially
for neural networks. However, stochastic gradient descent does not work for every problem that gradient
descent works for. The perceptron risk function happens to have special properties that guarantee that
stochastic gradient descent will always succeed.]
What if separating hyperplane doesn’t pass through origin?
Add a fictitious dimension. Decision fn is
f(x)=w·x+α=[w1w2α]·x1
x2
1
Now we have sample points in Rd+1, all lying on hyperplane xd+1=1.
Run perceptron algorithm in ( d+1)-dimensional space. [We are simulating a general hyperplane in
ddimensions by using a hyperplane through the origin in d+1 dimensions.]
[The perceptron algorithm was invented in 1957 by Frank Rosenblatt at the Cornell Aeronautical Laboratory.
It was originally designed not to be a program, but to be implemented in hardware for image recognition on
a 20×20 pixel image. Rosenblatt built a Mark I Perceptron Machine that ran the algorithm, complete with
electric motors to do weight updates.]
frankrosenblatt.jpg, perceptron.jpg [Frank Rosenblatt (from Cornell Chronicle ) and his
Mark I Perceptron Machine. This is what it took to process a 20 ×20 image in 1957.]
16 Jonathan Richard Shewchuk
[Then he held a press conference where he predicted that perceptrons would be “the embryo of an electronic
computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of
its existence.” We’re still waiting on that.]
[Perceptron Convergence Theorem: If data is linearly separable, perceptron algorithm will find a linear
classifier that classifies all data correctly in at most O(r2/γ2) iterations, where r=max∥Xi∥is “radius of
data” andγis the “maximum margin.”]
[I’ll define “maximum margin” shortly.]
[We’re not going to prove this, because perceptrons are obsolete.]
[Although the step size /learning rate ϵdoesn’t appear in that big-O expression, it does have an e ffect on the
running time, but the e ffect is hard to characterize. The algorithm gets slower if ϵis too small because it has
to take lots of steps to get down the hill. But it also gets slower if ϵis too big for a di fferent reason: it jumps
right over the region with zero risk and oscillates back and forth for a long time.]
[Although stochastic gradient descent is faster for this problem than gradient descent, the perceptron algo-
rithm is still slow. There’s no reliable way to choose a good step size ϵ. Fortunately, optimization algorithms
have improved a lot since 1957. You can get rid of the step size by using a decent modern “line search” al-
gorithm. Better yet, you can find a better decision boundary much more quickly by quadratic programming,
which is what we’ll talk about next.]
MAXIMUM MARGIN CLASSIFIERS
The margin of a linear classifier is the distance from the decision boundary to the nearest training point.
What if we make the margin as wide as possible?
C XX
XX
XXC
CC C
C
w·x+α=1
w·x+α=0 w·x+α=−1[Draw this by hand. maxmargin.pdf ]
We enforce the constraints
yi(w·Xi+α)≥1 for i∈[1,n]
[Notice that the right-hand side is a 1, rather than a 0 as it was for the perceptron algorithm. It’s not obvious,
but this a better way to formulate the problem, partly because it makes it impossible for the weight vector w
to get set to zero.]
Perceptron Learning; Maximum Margin Classifiers 17
Recall: if∥w∥=1, signed distance from hyperplane to Xiisw·Xi+α.
Otherwise, it’sw
∥w∥·Xi+α
∥w∥. [We’ve normalized the expression to get a unit weight vector.]
Hence the margin is min i1
∥w∥|w·Xi+α||       {z       }
≥1≥1
∥w∥. [We get the inequality by substituting the constraints.]
To maximize the margin, minimize ∥w∥. Optimization problem:
Find wandαthat minimize∥w∥2
subject to yi(Xi·w+α)≥1 for all i∈[1,n]
Called a quadratic program ind+1 dimensions and nconstraints.
It has one unique solution! [If the points are linearly separable; otherwise, it has no solution.]
[A reason we use∥w∥2as an objective function, instead of ∥w∥, is that the length function ∥w∥is not smooth
atw=0, whereas∥w∥2is smooth everywhere. This makes optimization easier.]
The solution gives us a maximum margin classifier , aka a hard-margin support vector machine (SVM).
[Technically, this isn’t really a support vector machine yet; it doesn’t fully deserve that name until we add
features and kernels.]
At the optimal solution, the margin is exactly1
∥w∥. [Because at least one constraint holds with equality.]
There is a slab of width2
∥w∥containing no sample points [with the hyperplane running along its middle].
[Let’s see what these constraints look like in weight space.]
-1.0 -0.8 -0.6 -0.4 -0.2w2
-1.0-0.50.51.0alpha
weight3d.pdf, weightcross.pdf [This is an example of what the linear constraints look like
in the 3D weight space ( w1,w2,α) for the SVM we’ve been studying with three training
points. The SVM is looking for the point nearest the α-axis that lies above the blue plane
(representing an in-class training point) but below the red and pink planes (representing
out-of-class training points). In this example, that optimal point lies where the three planes
intersect. At right we see a 2D cross section w1=1/17 of the 3D space, because the
optimal solution lies in this cross section. The constraints say that the solution must lie
in the leftmost pizza slice, while being as close to the origin as possible, so the optimal
solution is where the three lines meet.]
[Like the perceptron algorithm, a hard-margin SVM works only with linearly separable point sets. We’ll fix
that in the next lecture.]
18 Jonathan Richard Shewchuk
4 Soft-Margin Support Vector Machines; Features
SOFT-MARGIN SUPPORT VECTOR MACHINES (SVMs)
Solves 2 problems:
– Hard-margin SVMs fail if data not linearly separable.
– ” ” ” sensitive to outliers.
9.2 Support Vector Classiﬁers 345
−10 1 2 3−10 1 2 3−10 1 2 3−10 1 2 3X1X1X2X2
FIGURE 9.5.Left:Two classes of observations are shown in blue and inpurple, along with the maximal margin hyperplane.Right:An additional blueobservation has been added, leading to a dramatic shift in the maximal marginhyperplane shown as a solid line. The dashed line indicates the maximal marginhyperplane that was obtained in the absence of this additional point.•Greater robustness to individual observations, and•Better classiﬁcation ofmostof the training observations.That is, it could be worthwhile to misclassify a few training observationsin order to do a better job in classifying the remaining observations.Thesupport vector classiﬁer,s o m e t i m e sc a l l e dasoft margin classiﬁer,supportvectorclassiﬁersoft marginclassiﬁerdoes exactly this. Rather than seeking the largest possible margin so thatevery observation is not only on the correct side of the hyperplane butalso on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side ofthe hyperplane. (The margin issoftbecause it can be violated by someof the training observations.) An example is shown in the left-hand panelof Figure 9.6. Most of the observations are on the correct side of the margin.However, a small subset of the observations are on the wrong side of themargin.An observation can be not only on the wrong side of the margin, but alsoon the wrong side of the hyperplane. In fact, when there is no separatinghyperplane, such a situation is inevitable. Observations on the wrong side ofthe hyperplane correspond to training observations that are misclassiﬁed bythe support vector classiﬁer. The right-hand panel of Figure 9.6 illustratessuch a scenario.9.2.2 Details of the Support Vector ClassiﬁerThe support vector classiﬁer classiﬁes a test observation depending onwhich side of a hyperplane it lies. The hyperplane is chosen to correctly
sensitive.pdf (ISL, Figure 9.5) [Example where one outlier moves the hard-margin SVM
decision boundary a lot.]
Idea: Allow some points to violate the margin, with slack variables .
Modified constraint for point i:
yi(Xi·w+α)≥1−ξi
[Observe that the only di fference between these constraints and the hard-margin constraints we saw last
lecture is the extra slack term ξi.]
[We also impose new constraints, that the slack variables are never negative.]
ξi≥0
[This inequality ensures that all sample points that don’t violate the margin are treated the same; they all
haveξi=0. Point ihas nonzero ξiif and only if it violates the margin.]
CC
CC
XCC
X
XXC
CCCC
C
XX
XXXX
X
XXX
XXC
X
ξ4/∥w∥ξ5/∥w∥
ξ3/∥w∥
1/∥w∥
1/∥w∥(margin)ξ2/∥w∥ξ1/∥w∥w·x+α=0
slacker +.pdf [A margin where some points have slack.]
Re-define “margin” to be 1 /∥w∥. [For soft-margin SVMs, the margin is no longer the distance from the
decision boundary to the nearest training point; instead, it’s 1 /∥w∥.]
Soft-Margin Support Vector Machines; Features 19
To prevent abuse of slack, we add a loss term to objective fn.
Optimization problem:
Find w,α, andξithat minimize∥w∥2+CPn
i=1ξi
subject to yi(Xi·w+α)≥1−ξifor all i∈[1,n]
ξi≥0 for all i∈[1,n]
. . . a quadratic program in d+n+1 dimensions and 2 nconstraints.
[It’s a quadratic program because its objective function is quadratic and its constraints are linear inequalities.]
C>0 is a scalar regularization hyperparameter that trades o ff:
small C big C
desire maximize margin 1 /∥w∥keep most slack variables zero or small
danger underfitting overfitting
(misclassifies much (awesome training, awful test)
training data)
outliers less sensitive very sensitive
boundary more “flat” more sinuous
[The last row only applies to nonlinear decision boundaries, which we’ll discuss next. Obviously, a linear
decision boundary can’t be “sinuous”—though it can overfit.]
Use validation to choose C.
348 9. Support Vector Machines
−10 1 2−3−2−10 1 2 3
−10 1 2−3−2−10 1 2 3
−10 1 2−3−2−10 1 2 3
−10 1 2−3−2−10 1 2 3X1X1X1X1X2X2
X2X2
FIGURE 9.7.As u p p o r tv e c t o rc l a s s i ﬁ e rw a sﬁ tu s i n gf o u rd iﬀerent values of thetuning parameterCin (9.12)–(9.15). The largest value ofCwas used in the topleft panel, and smaller values were used in the top right, bottom left, and bottomright panels. WhenCis large, then there is a high tolerance for observations beingon the wrong side of the margin, and so the margin will be large. AsCdecreases,the tolerance for observations being on the wrong side of the margin decreases,and the margin narrows.but potentially high bias. In contrast, ifCis small, then there will be fewersupport vectors and hence the resulting classiﬁer will have low bias buthigh variance. The bottom right panel in Figure 9.7 illustrates this setting,with only eight support vectors.The fact that the support vector classiﬁer’s decision rule is based onlyon a potentially small subset of the training observations (the support vec-tors) means that it is quite robust to the behavior of observations thatare far away from the hyperplane. This property is distinct from some ofthe other classiﬁcation methods that we have seen in preceding chapters,such as linear discriminant analysis. Recall that the LDA classiﬁcation rule
svmC.pdf (ISL, Figure 9.7) [Examples of how the slab varies with C. Smallest Cat upper
left; largest Cat lower right.]
[One way to think about slack is to pretend that slack is money we can spend to buy permission for a sample
point to violate the margin. The further a point penetrates the margin, the bigger the fine you have to pay.
We want to make the margin as wide as possible, but we also want to spend as little money as possible. If
the regularization parameter Cis small, it means we’re willing to spend lots of money on violations so we
can get a wider margin. If Cis big, it means we’re cheap and we won’t pay much for violations, even though
we’ll su ffer a narrower margin. If Cis infinite, we’re back to a hard-margin SVM.]
20 Jonathan Richard Shewchuk
FEATURES
Q: How to do nonlinear decision boundaries?
A: Make nonlinear features (aka basis functions ) that lift points into a higher-dimensional space.
High- dlinear classifier→low-dnonlinear classifier.
[Added features work with all classifiers—not only linear classifiers like perceptrons and SVMs, but also
classifiers that are not linear.]
Example 1: The parabolic lifting map
Φ:Rd→Rd+1
Φ(x)="x
∥x∥2#
←liftsxonto paraboloid xd+1=∥x∥2
[We’ve added one new feature, ∥x∥2. Even though the new feature is just a function of other input features,
it gives our linear classifier more power. Now an SVM can have spheres as decision boundaries.]
Find a linear classifier in Φ-space.
It induces a sphere classifier in x-space.
X
XX
XXXX
C
CC
C CCX X
XX
X XX
XX
CCCCC
CXXX
XX
∥x∥2
x1x2
x1
[Draw this by hand. circledec.pdf ]
Theorem: Φ(X1),...,Φ(Xn) are linearly separable i ffX1,...,Xnare separable by a hypersphere.
(Possibly an∞-radius hypersphere =hyperplane.)
Proof: Consider hypersphere in Rdw/center c& radiusρ.xis inside i ff
∥x−c∥2<ρ2
∥x∥2−2c·x+∥c∥2<ρ2
[−2c⊤1]|     {z     }
normal vector in Rd+1"x
∥x∥2#
|   {z   }
Φ(x)<ρ2−∥c∥2
Hence points inside sphere ↔lifted points underneath hyperplane in Φ-space.
[The implication works in both directions.]
[Hyperspheres include hyperplanes as a special, degenerate case. A hyperplane is essentially a hypersphere
with infinite radius. So hypersphere decision boundaries can do everything hyperplane decision boundaries
can do, plus a lot more. With the parabolic lifting map, if you pick a hyperplane in Φ-space that is vertical,
you get a hyperplane in x-space.]
Soft-Margin Support Vector Machines; Features 21
Example 2: Ellipsoid /hyperboloid /paraboloid decision boundaries
[Draw 2D examples of ellipse & hyperbola.]
In 3D, these have the formula
Ax2
1+Bx2
2+Cx2
3+Dx1x2+Ex2x3+Fx3x1+Gx1+Hx2+Ix3+α=0
[Here, the capital letters are scalars, not matrices.]
quadrics.png (courtesy Rahul Narain) [Quadrics in 3D.]
[If we add all the quadratic monomials as features, our decision boundaries can be arbitrary ellipsoids,
hyperboloids, paraboloids, and more.]
Φ(x)=[x2
1x2
2x2
3x1x2x2x3x3x1x1x2x3]⊤ [For perceptron or regression, add a
1 at end. For SVM, the 1 is built-in.]
Decision function is [ A B C D E F G H I ]|                                          {z                                          }
w⊤·Φ(x)+α
[Now, our decision function can be any degree-2 polynomial. Each component of Φis also called a
basis function , as it is a function of xand the decision function is a linear combination of basis functions.]
Isosurface defined by this equation is called a quadric .
A linear decision boundary in Φ-space imposes a quadric decision boundary in x-space.
[The word quadric just means an isosurface of a degree-2 polynomial. In the special case of two dimen-
sions, it’s also known as a conic section . Our decision boundary can be an arbitrary ellipsoid, hyperboloid,
paraboloid, cylinder, etc.]
[When dis large, there are order- d2cross-terms in Φ-space! So we are adding a lot of new features. This
will impose a serious computational cost on a classifier like a support vector machine. But it might be worth
it to find good classifiers for data that aren’t linearly separable.]
Φ(x) :Rd→R(d2+3d)/2[For perceptron or regression, add 1 for the fictitious dimension.]
[If all these extra features make the classifier overfit or make it too slow, you can leave out the cross-terms
and include only quadratic terms like x2
1,x2
2, etc. Then the number of added features is linear in d, not
quadratic in d. If you do that, your decision boundaries can be axis-aligned ellipsoids and axis-aligned
hyperboloids, but they can’t be rotated in arbitrary ways.]
22 Jonathan Richard Shewchuk
Example 3: Decision fn is degree- ppolynomial
E.g., a cubic in R2:
Φ(x)=[x3
1x2
1x2x1x2
2x3
2x2
1x1x2x2
2x1x2]⊤
Φ(x) :Rd→RO(dp)
Figure 6: The e ↵ect of the degree of a polynomial kernel. The polynomial kernel of degree
1 leads to a linear separation (A). Higher degree polynomial kernels allow a more ﬂexible
decision boundary (B-C). The style follows that of Figure 5.
features. The dimensionality of the feature-space associated with the above
example is quadratic in the number of dimensions of the input space. If we
were to use monomials of degree drather than degree 2 monomials as above,
the dimensionality would be exponential in d, resulting in a substantial
increase in memory usage and the time required to compute the discriminant
function. If our data are high-dimensional to begin with, such as in the case
of gene expression data, this is not acceptable. Kernel methods avoid this
complexity by avoiding the step of explicitly mapping the data to a high
dimensional feature-space.
We have seen above (Equation (5)) that the weight vector of a large
margin separating hyperplane can be expressed as a linear combination of
the training points, i.e. w=Pn
i=1yi↵ixi. The same holds true for a large
class of linear algorithms, as shown by the representer theorem (see [2]).
Our discriminant function then becomes
f(x)=nX
i=1yi↵ih (xi), (x)i+b. (7)
The representation in terms of the variables ↵iis known as the dual repre-
sentation (cf. Section “Classiﬁcation with Large Margin”). We observe that
the dual representation of the discriminant function depends on the data
only through dot products in feature-space. The same observation holds for
the dual optimization problem (Equation (4)) when replace xiwith  (xi)
(analogously for xj).
If the kernel function k(x,x0) deﬁned as
k(x,x0)=⌦
 (x), (x0)↵
(8)
10
degree5.pdf [Hard-margin SVMs with degree 1 /2/5 decision functions. Observe that the
margin tends to get wider as the degree increases.]
[Increasing the degree like this accomplishes two things.
– First, the data might become linearly separable when you lift them to a high enough degree, even if
the original data are not linearly separable.
– Second, raising the degree can widen the margin, so you might get a more robust decision boundary
that generalizes better to test data.]
degree 1→
←degree 10
d1.pdf, d2.pdf, . . . , d10.pdf [Decision boundaries found by linear regression with polyno-
mial features of maximum degrees from 1 through 10. Circles are training points; X’s are
validation points. (Implementation courtesy Josh Levine.)]
Soft-Margin Support Vector Machines; Features 23
degree12
103
1 1
1misclassified
2 3 4 5 6 7 8 92 26
359
misclass.pdf [Misclassified validation points for the linear regressions depicted above,
with polynomial features of degrees 1 through 10.]
[Here we see a U-shaped curve, as we do with the regularization parameter Cin soft-margin SVMs. This
example obtains best results with degree 2 or 3 polynomials. A linear classifier underfits, whereas classifiers
of degree 4 or greater overfit; generalization gets worse as the decision boundary becomes too flexible. The
degree is an example of a hyperparameter that can be optimized by validation.]
[If you use polynomial features with a soft-margin SVM, now you have two hyperparameters: the degree
and the regularization hyperparameter C. Generally, the optimal Cwill be di fferent for every polynomial
degree, so when you change the degree, you should run validation again to find the best Cfor that degree.]
[With polynomials, we’re really blowing up the number of features! If you have, say, 100 features per
sample point and you want to use degree-4 decision functions, then each lifted feature vector has a length of
roughly 4 million, and your learning algorithm will take approximately forever to run.]
[However, there is an extremely clever trick that allows us to work with these huge feature vectors very
quickly, without ever computing them. It’s called “kernelization” or “the kernel trick.” So even though it
appears now that working with degree-4 polynomials is computationally infeasible, it can actually be done
quickly.]
24 Jonathan Richard Shewchuk
[So far I’ve talked only about polynomial features. But features can get much more complicated than
polynomials, and they can be tailored to fit a specific problem. Let’s consider a type of feature you might
use if you wanted to implement a handwriting recognition algorithm.]
Example 5: Edge detection
Edge detector : algorithm for approximating grayscale /color gradients in image, e.g.,
– tap filter
– Sobel filter
– oriented Gaussian derivative filter
[images are discrete, not continuous fields, so approximation of gradients is necessary.]
[See “Image Derivative” on Wikipedia.]
Collect line orientations in local histograms (each having 12 orientation bins per region); use histograms as
features ( instead of raw pixels).
orientgrad.png [Image histograms.]
Paper: Maji & Malik, 2009.
[If you want to, optionally, use these features in future homeworks and try to win the Kaggle competition,
this paper is a good online resource.]
[When they use a linear SVM on the raw pixels, Maji & Malik get an error rate of 15.38% on the test set.
When they use a linear SVM on the histogram features, the error rate goes down to 2.64%.]
[Many applications can be improved by designing application-specific features. There’s no limit but your
own creativity and ability to discern the structure hidden in your application.]
Machine Learning Abstractions and Numerical Optimization 25
5 Machine Learning Abstractions and Numerical Optimization
ML ABSTRACTIONS [some meta comments on machine learning]
[When you write a large computer program, you break it down into subroutines and modules. Many of you
know from experience that you need to have the discipline to impose strong abstraction barriers between
different modules, or your program will become so complex you can no longer manage nor maintain it.]
[When you learn a new subject, it helps to have mental abstraction barriers, too, so you know when you can
replace one approach with a di fferent approach. I want to give you four levels of abstraction that can help
you think about machine learning. It’s important to make mental distinctions between these four things, and
the code you write should have modules that reflect these distinctions as well.]
APPLICATION /DATA
data labeled or not?
yes: labels categorical (classification) or quantitative (regression)?
no: similarity (clustering) or positioning (dimensionality reduction)?
MODEL [what kinds of hypotheses are permitted?]
e.g.:
– decision fns: linear, polynomial, logistic, neural net, . . .
– nearest neighbors, decision trees
– features
– low vs. high capacity (a ffects overfitting, underfitting, inference)
OPTIMIZATION PROBLEM
– variables, objective fn, constraints
e.g., unconstrained, convex program, least squares, PCA
OPTIMIZATION ALGORITHM
e.g., gradient descent, simplex, SVD
[In this course, we focus primarily on the middle two levels. As a data scientist, you might be given an
application, and your challenge is to turn it into an optimization problem that we know how to solve. We
will talk about optimization algorithms, but usually data analysts use optimization codes that are faster and
more robust than what they would write themselves.]
[The second level, the model, has a huge e ffect on the success of your learning algorithm. Sometimes you
get a big improvement by tailoring the model or its features to fit the structure of your specific data. The
model also has a big e ffect on whether you overfit or underfit. And if you want a model that you can interpret
so you can do inference , the model has to have a simple structure. Lastly, you have to pick a model that
leads to an optimization problem that can be solved. Some optimization problems are just too hard.]
[It’s important to understand that when you change something in one level of this diagram, you probably
have to change all the levels underneath it. If you switch your model from a linear classifier to a neural net,
your optimization problem changes, and your optimization algorithm changes too.]
26 Jonathan Richard Shewchuk
[Not all machine learning methods fit this four-level decomposition. Nevertheless, for everything you learn
in this class, think about where it fits in this hierarchy. If you don’t distinguish which math is part of the
model and which math is part of the optimization algorithm, this course will be very confusing for you.]
OPTIMIZATION PROBLEMS
[I want to familiarize you with some types of optimization problems that can be solved reliably and e ffi-
ciently, and the names of some of the optimization algorithms used to solve them. An important skill for
you to develop is to be able to go from an application to a well-defined optimization problem. That skill
depends on your ability to recognize well-studied types of optimization problems.]
Unconstrained
Goal: Find wthat minimizes (or maximizes) a continuous objective fn f(w).
fis smooth if its gradient is continuous too.
A global minimum offis a value wsuch that f(w)≤f(v) for every v.
A local minimum ” ” ” ” ” ” ” ” ” ”
for every vin a tiny ball centered at w.
[In other words, you cannot walk downhill from w.]
global minimumlocal minima
[Draw this by hand. minima.pdf ]
Usually, finding a local minimum is easy;
finding the global minimum is hard. [or impossible]
Exception: A function is convex if for every x,y∈Rd,
the line segment connecting ( x,f(x)) to ( y,f(y)) does not go below f(·).
y x[Draw this by hand. convex.pdf ]
Formally: for every x,y∈Rdandβ∈[0,1],f(x+β(y−x))≤f(x)+β(f(y)−f(x)).
E.g., perceptron risk fn is convex and nonsmooth.
Machine Learning Abstractions and Numerical Optimization 27
[When you sum together convex functions, you always get a convex function. The perceptron risk function
is a sum of convex loss functions, so it is convex.]
A [continuous] convex function [on a closed, convex domain] has either
– no minimum (goes to −∞), or
– just one local minimum, or
– a connected set of local minima that are all global minima with equal f.
[The perceptron risk function is in the last category.]
[In the last two cases, if you walk downhill, you eventually reach a global minimum.]
Gradient descent: repeat w←w−ϵ∇f(w)
learningrates20.gif (Gajanan Bhat, gbhat.com) [Gradient descent with di fferent learning
ratesϵ. Top left: painfully small. Top right: reasonable, but still smaller than ideal. Bottom
left: reasonable, but larger than ideal. Bottom right: too large; diverges. This is an animated
GIF; see https: //gbhat.com /machine learning /gradient descent learning rates.html .]
– Fails /diverges ifϵtoo large.
– Slow ifϵtoo small.
–ϵoften optimized by trial & error [for slow learners like neural networks].
[The best value of ϵis hard to guess. One common technique for dealing with divergence is to check whether
a step of gradient descent increases the function value rather than decreasing it; if so, reduce the step size.]
[That’s a simple example of what’s called an adaptive learning rate or a learning rate schedule . These
adaptations become even more important when you do stochastic gradient descent or when you optimize
non-convex, very twisty objective functions. We’ll revisit the idea when we learn neural networks.]
28 Jonathan Richard Shewchuk
[One interesting aspect of gradient descent that these figures illustrate is that it usually never reaches the
exact local minimum. Instead, it gets closer and closer forever, but never exactly reaches the true minimum.
We call this behavior “convergence.” The last question of Homework 2 will give you some understanding
of why convergence happens under the right conditions.]
[When we have a feature space with more than one dimension, another problem arises, which is that the
learning rate that’s good for one direction might be terrible in another direction. Consider the three examples
of gradient descent below.]
-4 -2 2 4w1
-2246w2
-4 -2 2 4w1
-2246w2
-4 -2 2 4w1
-2246w2
goodcondition.pdf, illcondition105.pdf, illcondition055.pdf [Left: 20 iterations of gradi-
ent descent on a well-conditioned quadratic function, f(w)=2w2
1+w2
2, with a modest step
sizeϵ=0.105. Center: 20 iterations on an ill-conditioned function, f(w)=10w2
1+w2
2; the
same step size is now too large. Right: after reducing the step size to ϵ=0.055, we have
convergence again but we aren’t approaching the minimum nearly as quickly.]
[The step size that works for the left example is too large for the center example; it diverges in the w1-
direction. At right, we reduce the step size and obtain convergence. But now convergence is slow in the
w2-direction.]
High ellipticity of the contours, a.k.a. ill-conditioning of the Hessian, means no learning rate is good in all
directions.
[The Hessian matrix is said to be ill-conditioned if its largest eigenvalue is much larger than its small-
est eigenvalue. Ill-conditioning can be a problem even for simple methods like linear regression, making
it harder to solve the problem. In response to these observations, there are adaptive learning rate algo-
rithms that explicitly choose di fferent learning rates for di fferent weights. Famous examples are Adam and
RMSprop.]
[There are many applications where you don’t have a convex objective function. Then gradient descent
usually can find a local minimum, but not necessarily a global minimum. And often there is no guarantee
that the local minimum you find will be nearly as good as the global minimum. Nevertheless, gradient
descent is used for a lot of nonconvex machine learning problems too. For example, neural networks try
to optimize an objective function that has lotsof local minima. But stochastic gradient descent is still the
algorithm of choice for training neural nets. We’ll talk more later in the semester about why.]
Machine Learning Abstractions and Numerical Optimization 29
Linear Program
Linear objective fn +linear inequality constraints.
Goal: Find wthat maximizes (or minimizes) c·w
subject to Aw≤b
where Aisn×dmatrix, b∈Rn, expressing nlinear constraints :
Ai·w≤bi, i∈[1,n]
inw-space:optimumc
active constraintactive constraint
feasible
region
[Draw this by hand. linprog.pdf ]
The set of points wthat satisfy all constraints is a convex polytope called the feasible region F[shaded].
The optimum is the point in Fthat is furthest in the direction c. [What does convex mean?]
A point set Pis convex if for every p,q∈P, the line segment with endpoints p,qlies entirely in P.
[What is a polytope? Just a polyhedron, generalized to higher dimensions.]
The optimum achieves equality for some constraints (but not most), called the active constraints of the
optimum. [In the figure above, there are two active constraints. In an SVM, active constraints correspond to
the training points that touch or violate the slab, and these points are also known as support vectors .]
[Sometimes, there is more than one optimal point. For example, in the figure above, if cpointed straight up,
every point on the top horizontal edge would be optimal. The set of optimal points is always convex.]
Example: EVERY feasible point (w,α) gives a linear classifier:
Find w,αthat satisfies yi(w·Xi+α)≥1 for all i∈[1,n]
[This is the problem of finding a feasible point. This problem can be cast as a slightly di fferent linear
program that uses an objective function to make all the inequalities be satisfied strictly if that’s possible.]
IMPORTANT: The data are linearly separable i ffthe feasible region is not the empty set.
→Also true for maximum margin classifier (quadratic program)
[The most famous algorithm for linear programming is the simplex algorithm, invented by George Dantzig
in 1947. The simplex algorithm is indisputably one of the most important and useful algorithms of the
20th century. It walks along edges of the feasible region, traveling from vertex to vertex until it finds an
optimum.]
[Linear programming is very di fferent from unconstrained optimization; it has a much more combinatorial
flavor. If you knew which constraints would be the active constraints once you found the solution, it would
be easy; the hard part is figuring out which constraints should be the active ones. There are exponentially
many possibilities, so you can’t a fford to try them all. So linear programming algorithms tend to have a
very discrete, computer science feeling to them, like graph algorithms, whereas unconstrained optimization
algorithms tend to have a continuous, numerical mathematics feeling.]
30 Jonathan Richard Shewchuk
[Linear programs crop up everywhere in engineering and science, but they’re usually in disguise. An ex-
tremely useful talent you should develop is to recognize when a problem is a linear program.]
[A linear program solver can find a linear classifier, but it can’t find the maximum margin classifier. We
need something more powerful.]
Quadratic Program
Quadratic, convex objective fn +linear inequality constraints.
Goal: Find wthat minimizes f(w)=w⊤Qw+c⊤w
subject to Aw≤b
where Qis a symmetric, positive semidefinite matrix.
[A matrix is positive semidefinite ifw⊤Qw≥0 for all w.]
Example: Find maximum margin classifier.
1
23
456
78
91010 10
1011
1111 11
1212 12
12 131313 13
14
1414 14
15
1515 15
16161616
17171717
-3 -2 -1 0 1 2 3-3-2-10123
quadratic.pdf, quadratic3D.pdf [Left: A hard-margin SVM minimizes the objective func-
tionw2
1+w2
2. Right: There is also an α-axis, so the isosurfaces of the objective function
are really cylinders. On the left isocontours, draw two polygons—one with one active
constraint, and one with two—and show the constrained minimum for each polygon. “In a
hard-margin SVM, we are looking for the point in this polygon that’s closest to the α-axis.”]
[IfQis positive definite, a quadratic program has just one unique local minimum, which is therefore the
global minimum. But in a support vector machine, Qis not definite; it is only positive semidefinite, because
the bias term αis a weight but it does not influence the objective function. Sometimes positive semidefinite
quadratic programs have multiple solutions, but SVMs are a special case where there is only one unique
minimum. By the way, if Qis indefinite, then fis not convex, the minimum is not always unique, and
quadratic programming is NP-hard. But we won’t need that kind of quadratic program in this class.]
Algs for quadratic programming:
– Simplex-like [commonly used for general-purpose quadratic programs, but not as good for SVMs as
the following two algorithms that specifically exploit properties of SVMs]
– Sequential minimal optimization (SMO, used in LIBSVM, “SVC” in scikit)
– Coordinate descent (used in LIBLINEAR, “LinearSVC” in scikit)
Numerical optimization @ Berkeley: EECS 127 /227AT /227BT /227C.
Decision Theory; Generative and Discriminative Models 31
6 Decision Theory; Generative and Discriminative Models
DECISION THEORY aka Risk Minimization
[Today I’m going to talk about a style of classifier very di fferent from SVMs. The classifiers we’ll cover in
the next few weeks are based on probability.]
[One aspect of probabilistic data is that sometimes a point in feature space doesn’t have just one class.
Suppose your data is adult men and women with just one feature: their height. You want to train a classifier
that takes in an adult’s height and returns a classification, man or woman. Suppose you are asked to predict
the sex of a 5’5” adult. Well, your training set includes some 5’5” women and some 5’5” men. What should
you do?]
[In your feature space, you have two training points at the same location with di fferent classes. More
generally, the height distributions of men and women overlap. Obviously, in that case, you can’t draw a
decision boundary that classifies all points with 100% accuracy.]
Multiple sample points with di fferent classes could lie at same point: we want a probabilistic classifier.
Suppose 10% of population has cancer, 90% doesn’t.
Probability distributions for occupation conditioned on cancer, P(X|Y):
job ( X) miner farmer other
cancer ( Y=1) 20% 50% 30%
no cancer ( Y=−1) 1% 10% 89%[caps here mean random variables, not matrices.]
[I made these numbers up. Please don’t take them as medical advice.]
Recall: P(X)=P(X|Y=1)P(Y=1)+P(X|Y=−1)P(Y=−1)
P(X=farmer) =0.5×0.1+0.1×0.9=0.14 [. . . so 14% of random people are farmers]
You meet a farmer. Guess whether he has cancer?
[If you’re in a hurry, you might see that 50% of people with cancer are farmers, but only 10% of people with
no cancer are farmers, and conclude that a typical farmer probably has cancer. But that would be wrong,
because that reasoning fails to take the prior probabilities into account.]
Bayes’ Theorem:
↓posterior probability ↓prior prob.↓ifX=farmer
P(Y=1|X)=P(X|Y=1)P(Y=1)
P(X)=0.05
0.14
P(Y=−1|X)=P(X|Y=−1)P(Y=−1)
P(X)=0.09
0.14[These two probs always sum to 1.]
P(cancer|farmer) =5/14≈36%.
[So we probably shouldn’t diagnose cancer.]
[BUT . . . we’re assuming that we want to maximize the chance of a correct prediction. But that’s not always
the right assumption. If you’re developing a cheap screening test for cancer, you’d rather have more false
positives and fewer false negatives. A false negative might mean somebody misses an early diagnosis and
dies of a cancer that could have been treated if caught early. A false positive just means that you spend more
money on more accurate tests. When there’s an asymmetry between the awfulness of false positives and
false negatives, we can quantify that with a loss function.]
32 Jonathan Richard Shewchuk
A loss function L(ˆy,y) specifies badness if classifier predicts ˆ y, true class is y.
E.g., L(ˆy,y)=1 if ˆ y=1,y=−1,false positive is bad
5 if ˆ y=−1,y=1,false negative is BAAAAAD
0 if ˆ y=y. [loss should always be zero for a perfectly correct prediction!]
A 36% probability of loss 5 is worse than a 64% prob. of loss 1,
so we recommend further cancer screening.
The loss fn above is asymmetrical .
[A loss is symmetrical if it is the same for false positives and false negatives. For example . . . ]
The 0-1 loss function isL(ˆy,y)=(1 if ˆ y,y,[always 1 for a wrong prediction]
0 if ˆ y=y.[always 0 for a correct prediction]
[Another application where you want a very asymmetrical loss function, besides medical diagnosis, is spam
detection. Putting a good email in the spam folder is much worse than putting spam in your inbox.]
Letr:Rd→± 1 be a decision rule , aka classifier :
a fn that maps a feature vector xto 1 (“in class”) or −1 (“not in class”).
The risk forris the expected loss over all values of x,y.[Memorize this definition!]
R(r)=E[L(r(X),Y)]
=X
xL(r(x),1)P(Y=1|X=x)+L(r(x),−1)P(Y=−1|X=x)P(X=x)
=P(Y=1)X
xL(r(x),1)P(X=x|Y=1)+P(Y=−1)X
xL(r(x),−1)P(X=x|Y=−1).
The Bayes decision rule aka Bayes classifier is the fn r∗that minimizes functional R(r).
Assuming L(1,1)=L(−1,−1)=0,
r∗(x)=(1 if L(−1,1)P(Y=1|X=x)>L(1,−1)P(Y=−1|X=x),
−1 otherwise.
When Lis symmetrical, [the big, key principle you should memorize is]
pick the class with the biggest posterior probability.
[But if the loss function is asymmetrical, then you must weight the posteriors with the losses.]
In cancer example, r∗(miner) =1,r∗(farmer) =1, and r∗(other) =−1.
The Bayes risk , aka optimal risk , is the risk of the Bayes classifier.
[In our cancer example, the last expression for risk Rgives:]
R(r∗)=0.1(5×0.3)+0.9(1×0.01+1×0.1)=0.249. No decision rule gives a lower risk.
[It is interesting that, if we really know all these probabilities, we really can construct an ideal probabilistic
classifier. But in real applications, we rarely know these probabilities; the best we can do is use statistical
methods to estimate them.]
Deriving /using r∗is called risk minimization .
[Did you memorize the two boldfaced lines above yet?]
Decision Theory; Generative and Discriminative Models 33
Continuous Distributions
Suppose Xhas a continuous probability density fn (PDF).
Review: [Go back to your CS 70 or stats notes if you don’t remember this.]
x2 x1 xf(x)[Draw this by hand. integrate.pdf ]
prob. that random variable X∈[x1,x2]=Zx2
x1f(x) dx[shaded area]
area under whole curve =1=Z∞
−∞f(x) dx expected value of g(X) : E[ g(X)]=Z∞
−∞g(x)f(x) dx
meanµ=E[X]=Z∞
−∞x f(x) dx varianceσ2=E[(X−µ)2]=E[X2]−µ2
[Perhaps our cancer statistics look like this.]
xfX|Y=1(x)
fX|Y=−1(x)
Draw this figure by hand (cancerconditional.png) [The area under each curve is 1.]
[Let’s use the 0-1 loss function. In other words, suppose you want a classifier that maximizes the chance of
a correct prediction. The wrong answer would be to look where these two curves cross and make that be the
decision boundary. As before, it’s wrong because it doesn’t take into account the prior probabilities.]
Suppose P(Y=1)=1/3,P(Y=−1)=2/3, 0-1 loss.
x Bayes optimal decision boundaryfX|Y=1(x)P(Y=1) fX|Y=−1(x)P(Y=−1)
Draw this figure by hand (cancerposterior.png)
[To maximize the chance you’ll predict correctly whether somebody has cancer, the Bayes decision rule
looks up xon this chart and picks the curve with the highest probability. In this example, that means you
pick cancer when xis left of the optimal decision boundary, and no cancer when xis to the right.]
34 Jonathan Richard Shewchuk
Define risk as before, replacing summations with integrals.
R(r)=E[L(r(X),Y)]
=P(Y=1)Z
L(r(x),1)fX|Y=1(x) dx+
P(Y=−1)Z
L(r(x),−1)fX|Y=−1(x) dx.
For Bayes decision rule, Bayes risk is the area under minimum of functions above. [Shade it.]
Assuming L(1,1)=L(−1,−1)=0,
R(r∗)=Z
min
y=±1L(−y,y)fX|Y=y(x)P(Y=y) dx.
[If you want to use an asymmetrical loss function, just scale the curves vertically in the figure above.]
IfLis 0-1 loss, [then the risk has a particularly nice interpretation:]
R(r)=P(r(x) is wrong) [which makes sense, because Ris the expected loss.]
and the Bayes optimal decision boundary is{x:P(Y=1|X=x)|             {z             }
decision fn= 0.5|{z}
isovalue}
fX|Y=1(x)P(Y=1)
Bayes optimal decision boundaryfX|Y=−1(x)P(Y=−1)
qda3d.pdf, qdacontour.pdf [Two di fferent views of the same 2D Gaussians.]
[Notice that the accuracy of the probabilities is most important near the decision boundary. Far away from
the decision boundary, a bit of error in the probabilities probably wouldn’t change the classification.]
[You can also have multi-class classifiers, choosing among three or more classes. The Bayesian approach is
a particularly convenient way to generate multi-class classifiers, because you can simply choose whichever
class has the greatest posterior probability. Then the decision boundary lies wherever two or more classes
are tied for the highest probability.]
Decision Theory; Generative and Discriminative Models 35
3 WAYS TO BUILD CLASSIFIERS
(1) Generative models (e.g., LDA) [We’ll learn about LDA next lecture.]
– Assume sample points come from probability distributions, di fferent for each class.
– Guess form of distributions
– For each class C, fit distribution parameters to class C points, giving fX|Y=C(x)
– For each C, estimate P(Y=C)
– Bayes’ Theorem gives P(Y|X)
– If 0-1 loss, pick class C that maximizes P(Y=C|X=x) [posterior probability]
equivalently, maximizes fX|Y=C(x)P(Y=C)
(2) Discriminative models (e.g., logistic regression)
[We’ll learn about logistic regression in a few weeks.]
– Model P(Y|X) directly
(3) Find decision boundary (e.g., SVM)
– Model r(x) directly (no posterior)
Advantage of (1 & 2): P(Y|X) tells you probability your guess is wrong
[This is something SVMs don’t do.]
Advantages of (1): you can diagnose outliers: f(x) is very small;
stabler for outliers or few training points.
Disadvantages of (1): often hard to estimate distributions accurately;
real distributions rarely match standard ones.
[What I’ve written here doesn’t actually define the phrases “generative model” or “discriminative model.”
The proper definitions accord with the way statisticians think about models. A generative model is a full
probabilistic model of all variables, whereas a discriminative model provides a model only for the target
variables that we want to predict.]
[It’s important to remember that we rarely know precisely the value of any of these probabilities. There is
usually error in all of these probabilities. In practice, generative models are most popular when you have
phenomena that are well approximated by the normal distribution or another “nice” distribution. Generative
methods also tend to be more stable than other methods when the number of training points is small or when
there are a lot of outliers.]
36 Jonathan Richard Shewchuk
7 Gaussian Discriminant Analysis; Maximum Likelihood Estimation
GAUSSIAN DISCRIMINANT ANALYSIS
Fundamental assumption: each class has a normal distribution [a Gaussian].
X∼N(µ,σ2) :f(x)=1
(√
2πσ)dexp 
−∥x−µ∥2
2σ2!
. [µ&x=vectors;σ=scalar; d=dimension]
For each class C, suppose we know mean µCand variance σ2
C, yielding PDF fX|Y=C(x),
and priorπC=P(Y=C).
QC(x)
Bayes optimal decision boundaryfX|Y=C(x)πC
fX|Y=D(x)πDQD(x)
qda3d.pdf, qdacontour.pdf, Q.pdf [Probability density functions for two classes. The
Bayes optimal decision boundary is an ellipse.]
[This PDF is a simplified version of the multivariate normal distribution. It is multivariate: xandµcan be
vectors, and this plot shows a 2D feature space. But the variance σ2is just a scalar; for simplicity, we will
avoid the covariance matrix until next lecture. That’s why the isocontours are circles, not ellipses. I call this
theisotropic normal distribution , because the variance is the same in every direction. Next lecture, we’ll
use the usual multivariate normal distribution, where the isosurfaces are ellipsoids.]
Given x, Bayes decision rule r∗(x) predicts class C that maximizes fX|Y=C(x)πC.
[Remember our last lecture’s main principle: pick the class with the biggest posterior probability! ]
lnωis monotonically increasing for ω> 0, so it is equivalent to maximize
QC(x)=ln
(√
2π)dfX|Y=C(x)πC
=−∥x−µC∥2
2σ2
C−dlnσC+lnπC. [QCis quadratic in x]
[In a 2-class problem, you can also incorporate an asymmetrical loss function by adding ln L(not C,C)
toQC(x). In a multi-class problem, asymmetric loss may be more di fficult to account for, because the
penalty for guessing wrong might depend on both the wrong prediction and the true class.]
Quadratic Discriminant Analysis (QDA)
Suppose only 2 classes C, D. Then the Bayes classifier is
r∗(x)=(C if QC(x)−QD(x)>0,
D otherwise. [Picks the class with the biggest posterior probability]
Decision fn is QC(x)−QD(x) (quadratic); Bayes decision boundary is {x:QC(x)−QD(x)=0}.
Gaussian Discriminant Analysis; Maximum Likelihood Estimation 37
– In 1D, B.d.b. may have 1 or 2 points. [Solutions to a quadratic equation]
– In d-D, B.d.b. is a quadric. [In 2D, that’s a conic section; see figure above]
[You might not be satisfied with just predicting how each point is classified. One of the great things about
QDA is that you can also estimate the probability that your prediction is correct. Let’s work that out.]
To recover posterior probabilities in 2-class case, use Bayes.
P(Y=C|X)=fX|Y=CπC
fX|Y=CπC+fX|Y=DπD
recall eQC(x)=(√
2π)dfX|Y=C(x)πC[by definition of QC]
P(Y=C|X=x)=eQC(x)
eQC(x)+eQD(x)=1
1+eQD(x)−QC(x)
=s(QC(x)−QD(x)),where
s(γ)=1
1+e−γ⇐logistic fn aka sigmoid fn [recall QC−QDis the decision fn]
-4 -2 0 2 4x0.20.40.60.81.0s(x)
logistic.pdf [The logistic function. Write
beside it:] s(0)=1
2,s(∞)→1,s(−∞)→0,
monotonically increasing.
[We interpret s(0)=1
2as saying that on
the decision boundary, there’s a 50% chance
of class C and a 50% chance of class D.]
Multi-class QDA: [QDA works very naturally with more than 2 classes.]
multiplicative.pdf [Multi-class QDA partitions the feature space into regions. In two or
more dimensions, you typically wind up with multiple decision boundaries that adjoin each
other at joints. It looks like a sort of V oronoi diagram. In fact, it’s a special kind of V oronoi
diagram called a multiplicatively, additively weighted V oronoi diagram .]
38 Jonathan Richard Shewchuk
Linear Discriminant Analysis (LDA)
[LDA is a variant of QDA with linear decision boundaries. It’s less likely to overfit than QDA.]
Fundamental assumption: all the Gaussians have same variance σ2.
[The equations simplify nicely in this case.]
QC(x)−QD(x)=(µC−µD)·x
σ2|          {z          }
w·x−∥µC∥2−∥µD∥2
2σ2+lnπC−lnπD.
|                                     {z                                     }
+α
[The quadratic terms in QCandQDcanceled each other out!]
Now it’s a linear classifier!
– decision boundary is w·x+α=0
– posterior is P(Y=C|X=x)=s(w·x+α)
[The e ffect of “ s(w·x+α)” is to scale and translate the logistic fn in x-space.]
-3 -2 -1 1 2 3x0.20.40.60.81.0P(x)
lda1d.pdf, lda2d.pdf [Two Gaussians (red) and the logistic function (black). The logistic
function is the right Gaussian divided by the sum of the Gaussians. Observe that even when
the Gaussians are 2D, the logistic function still looks 1D.]
Special case: if πC=πD=1
2⇒ (µC−µD)·x−(µC−µD)·µC+µD
2
=0.
This is the centroid method!
Multi-class LDA: choose C that maximizes linear discriminant fnµC·x
σ2−∥µC∥2
2σ2+lnπC.
voronoi.pdf [When you have many classes, their LDA decision boundaries form a classical
V oronoi diagram if the priors πCare equal. All the Gaussians have the same width.]
Gaussian Discriminant Analysis; Maximum Likelihood Estimation 39
MAXIMUM LIKELIHOOD ESTIMATION OF PARAMETERS (Ronald Fisher, circa 1912)
[To use Gaussian discriminant analysis, we must first fit Gaussians to the sample points and estimate the
class prior probabilities. We’ll do priors first—they’re easier, because they involve a discrete distribution.
Then we’ll fit the Gaussians—they’re less intuitive, because they’re continuous distributions.]
Let’s flip biased coins! Heads with probability p; tails w /prob. 1−p. [But we don’t know p.]
10 flips, 8 heads, 2 tails. [Let me ask you a weird question.] What is the most likely value of p?
# of heads is X∼B(n,p), binomial distribution:
P(X=x)= n
x!
px(1−p)n−x[this is the probability of getting exactly xheads in ncoin flips]
Prob. of x=8 heads in n=10 flips is
P(X=8)=45p8(1−p)2 def=L(p)
Written as a fn of distribution parameter p, this prob. is the likelihood fn L(p).
Maximum likelihood estimation (MLE): A method of estimating the parameters of a statistical model by
picking the params that maximize [the likelihood function] L.
. . . is one method of density estimation : estimating a PDF [probability density function] from data.
[Let’s phrase it as an optimization problem.]
Find pthat maximizesL(p).
0.2 0.4 0.6 0.8 1.00.050.100.150.200.250.30
binomlikelihood.pdf [Graph ofL(p) for this example.]
Solve by finding critical point of L:
dL
dp=360p7(1−p)2−90p8(1−p)=0
⇒ 4(1−p)−p=0⇒ p=0.8
[It shouldn’t seem surprising that a coin that is biased so it comes up heads 80% of the time is the coin most
likely to produce 8 heads in 10 flips.]
[Note:d2L
dp2−18.9<0 atp=0.8, confirming it’s a maximum.]
[Here’s how this applies to prior probabilities.]
Suppose our training set is npoints, with xin class C. Then our estimated prior for class C is ˆ πC=x/n.
40 Jonathan Richard Shewchuk
Likelihood of a Gaussian
Given sample points X1,X2,..., Xn, find best-fit Gaussian.
[Now we want to fit a normal distribution to data, instead of a binomial distribution. If you draw a random
point from a normal distribution, what is the probability that it will be exactly at X1?]
[Zero. So it might seem like we have a problem here. With a continuous distribution, the probability of
generating any particular point is zero. But we’re just going to ignore that and do “likelihood” anyway.]
Likelihood of drawing these points [in the specified order] is
L(µ,σ;X1,..., Xn)=f(X1)f(X2)···f(Xn). [How do we maximize this?]
The log likelihood ℓ(·) is the ln of the likelihood L(·).
Maximizing likelihood ⇔maximizing log likelihood.
ℓ(µ,σ;X1,...,Xn)=lnf(X1)+lnf(X2)+...+lnf(Xn)
=nX
i=1 
−∥Xi−µ∥2
2σ2−dln√
2π−dlnσ!
|                                      {z                                      }
ln of normal PDF
Set∇µℓ=0,∂ℓ
∂σ=0 [Find the critical point of ℓ]
∇µℓ=nX
i=1Xi−µ
σ2=0⇒ ˆµ=1
nnX
i=1Xi [The hats ˆ mean “estimated”]
∂ℓ
∂σ=nX
i=1∥Xi−µ∥2−dσ2
σ3=0⇒ ˆσ2=1
dnnX
i=1∥Xi−µ∥2
We don’t know µexactly, so substitute ˆ µforµto compute ˆ σ.
Takeaway: use sample mean & variance of pts in class C to estimate mean & variance of Gaussian for
class C.
For QDA: estimate conditional mean ˆµC& conditional variance ˆσ2
Cofeach class C separately [as above]
& estimate the priors:
ˆπC=nCP
DnD⇐ total sample points in all classes[ˆπCis the coin flip parameter]
For LDA: same means & priors; one variance for all classes:
ˆσ2=1
dnX
CX
{i:yi=C}∥Xi−ˆµC∥2⇐pooled within-class variance
[Notice that although LDA is computing one variance for all the data, each sample point contributes with
respect to its own class’s mean . This gives a very di fferent result than if you simply use the global mean!
It’s usually smaller than the global variance. We say “within-class” because we use each point’s distance
from its class’s mean, but “pooled” because we then pool all the classes together.]
Eigenvectors and the (Anisotropic) Multivariate Normal Distribution 41
8 Eigenvectors and the (Anisotropic) Multivariate Normal Distribution
EIGENVECTORS
[I don’t know if you were properly taught about eigenvectors here at Berkeley, but I sure don’t like the way
they’re taught in most linear algebra books. So I’ll start with a review. You all know the definition of an
eigenvector:]
Given square matrix A, ifAv=λvfor some vector v,0, scalarλ, then
vis an eigenvector ofAandλis the eigenvalue ofAassociated w /v.
[But what does that mean? It means that vis a magical vector that, after being multiplied by A, still points
in the same direction , or in exactly the opposite direction .]
A3v
vA2v
AvEigenvalue 2:
wEigenvalue−1
2:
Aw
A2wA3w
Draw this figure by hand (eigenvectors.pdf)
[For most matrices, most vectors don’t have this property. So the ones that do are special, and we call them
eigenvectors.]
[Clearly, when you scale an eigenvector, it’s still an eigenvector. Only the direction matters, not the length.
Let’s look at a few consequences.]
Theorem: if vis eigenvector of Aw/eigenvalueλ,
then vis eigenvector of Akw/eigenvalueλk[kis a+ve integer; we will use Theorem later]
Proof: A2v=A(λv)=λAv=λ2v, etc.
Theorem: moreover, if Ais invertible,
then vis eigenvector of A−1w/eigenvalue 1 /λ
Proof: A−1v=A−1(1
λAv)=1
λv [look at the figures above, but go from right to left.]
[Stated simply: When you invert a matrix, the eigenvectors don’t change, but the eigenvalues get inverted.
When you square a matrix, the eigenvectors don’t change, but the eigenvalues get squared.]
[Those theorems are pretty obvious. The next theorem is not obvious at all.]
42 Jonathan Richard Shewchuk
Spectral Theorem : every real, symmetric n×nmatrix has real eigenvalues and
neigenvectors that are mutually orthogonal, i.e., v⊤
ivj=0 for all i,j
[This takes about a page of math to prove. One detail is that a matrix can have more than neigenvector
directions. If two eigenvectors happen to have the same eigenvalue, then every linear combination of those
eigenvectors is also an eigenvector. Then you have infinitely many eigenvector directions, but they all span
the same plane. So you just arbitrarily pick two vectors in that plane that are orthogonal to each other. By
contrast, the set of eigenvalues is always uniquely determined by a matrix, including the multiplicity of the
eigenvalues.]
We can use them as a basis for Rn.
Building a Matrix with Specified Eigenvectors
[There are a lot of applications where you’re given a matrix, and you want to extract the eigenvectors and
eigenvalues. But when you’re learning the math, I think it’s more intuitive to go in the opposite direction.
Suppose you know what eigenvectors and eigenvalues you want, and you want to create the matrix that has
those eigenvectors and eigenvalues.]
Choose nmutually orthogonal unit n-vectors v1,..., vn[so they specify an orthonormal coordinate system]
LetV=[v1v2... vn]⇐n×nmatrix
Observe: V⊤V=I [off-diagonal 0’s because the vectors are orthogonal]
[diagonal 1’s because they’re unit vectors]
⇒ V⊤=V−1⇒ VV⊤=I
Vis orthonormal matrix : acts like rotation (or reflection)
Choose some eigenvalues λi:
LetΛ =λ10... 0
0λ2 0
.........
0 0... λ n[diagonal matrix of eigenvalues]
Defn. of eigenvector: AV=VΛ
[This is the same definition of eigenvector I gave you at the start of the lecture— Av=λv—but this version
covers all neigenvectors in one statement. How do we find the Athat satisfies this equation?]
⇒AVV⊤=VΛV⊤[which proves . . . ]
Theorem: A=VΛV⊤=Pn
i=1λiviv⊤
i|{z}
outer product: n×nmatrix, rank 1has chosen eigenvectors /values
This is a matrix factorization called the eigendecomposition . [every real, symmetric matrix has one]
Example: [Using the eigenvectors and eigenvalues from the start of the lecture]
A="1/√
2 1/√
2
1/√
2−1/√
2#"2 0
0−1/2#"1/√
2 1/√
2
1/√
2−1/√
2#
="3/4 5/4
5/4 3/4#
.
[This completes our task of finding a symmetric matrix with specified orthonormal eigenvectors and eigen-
values. Again, it is more common in practice that you are given a symmetric matrix, such as a sample
covariance matrix, and you need to compute its eigenvectors and eigenvalues. That’s harder. But I think that
going from eigenvectors to the matrix helps to build intuition.]
Eigenvectors and the (Anisotropic) Multivariate Normal Distribution 43
Observe: A2=VΛV⊤VΛV⊤=VΛ2V⊤A−1=(VΛV⊤)−1=(V⊤)−1Λ−1V−1=VΛ−1V⊤
[This is another way to see that squaring a matrix squares its eigenvalues without changing its eigenvectors.
It also suggests a way to define a matrix square root.]
Given a symmetric PSD matrix Σ, we can find a symmetric square root A= Σ1/2:
compute eigenvectors /values of Σ
take square roots of Σ’s eigenvalues
reassemble matrix A[with the same eigenvectors as Σbut changed eigenvalues]
[Again, the first step of this algorithm—computing the eigenvectors and eigenvalues of a matrix—is much
harder than the remaining two steps.]
Visualizing Quadratic Forms
[My favorite way to visualize a symmetric matrix is to graph something called the quadratic form , which
shows how applying the matrix a ffects the length of a vector.]
The quadratic form ofMisx⊤Mx.
Suppose you want a matrix whose quadratic form has the isocontours at right below, which are circles
transformed by A. [The same matrix AI’ve been using, which stretches along the direction with eigenvalue 2
and shrinks along the direction with eigenvalue −1/2.]
123
45 55
5
6 66
6
77
7
7
-2 -1 0 1 2-2-1012
isocontours
transformed by A
−→
z-space x-space
qs(z)=∥z∥2qe(x)=???
1.2.
2.3.3.
4.4.
5.5.6.
6.7.
7.
8.8.
9.9.
10.10.
11.11.
12.12.
13.13.
14.14.
15.15.
16.16.
17.17.
18.18.
19.19.
-2-1012-2-1012
∥z∥2
 ???
circles.pdf, ellipses.pdf, circlebowl.pdf, ellipsebowl.pdf
[Both figures at left are plots of ∥z∥2, and both figures at right are plots of x⊤A−2x.
(Draw the stretch direction (1 ,1) with eigenvalue 2 and the shrink direction (1 ,−1) with
eigenvalue−1
2on the ellipses at right.)]
44 Jonathan Richard Shewchuk
That is, we want qe(Az)=qs(z).
Answer: set x=Az.
Then qe(x)=qs(z)=qs(A−1x)=∥A−1x∥2=x⊤A−2x.
The isocontours of the quadratic form x⊤A−2xare ellipsoids determined by the eigenvectors /values of A.
{x:x⊤A−2x=1}is an ellipsoid with axes v1,v2,..., vnand
radiiλ1,λ2,...,λ n
because if vihas length 1 ( vilies on unit circle), x=Avihas lengthλi(Avilies on the ellipsoid).
Therefore, isocontours of x⊤Mxare ellipsoids determined by eigenvectors /values of M−1/2.
[The eigenvalues of M−1/2are the inverse square roots of the eigenvalues of M.]
Special case: A(orM) is diagonal⇔eigenvectors are coordinate axes
⇔ellipsoids are axis-aligned
[Draw axis-aligned isocontours for a diagonal metric.]
A symmetric matrix Mis
positive definite ifw⊤Mw>0 for all w,0⇔all eigenvalues positive
positive semidefinite ifw⊤Mw≥0 for all w⇔all eigenvalues nonnegative
indefinite if+ve eigenvalue &−ve eigenvalue
invertible if no zero eigenvalue
pos definite pos semidefinite indefinite
posdef.pdf, possemi.pdf, indef.pdf
[Examples of quadratic forms for positive definite, positive semidefinite, and indefinite ma-
trices. Positive eigenvalues correspond to axes where the curvature goes up; negative eigen-
values correspond to axes where the curvature goes down. (Draw the eigenvector directions,
and draw the flat trough in the positive semidefinite bowl.)]
Every squared matrix is pos semidef, including A−2. [Eigenvalues of A−2are squared, cannot be negative.]
IfA−2exists, it is pos def. [An invertible matrix has no zero eigenvalues.]
What about the isosurfaces of x⊤Mxfor a +ve semidef, singular M ?
[IfMis only positive semidefinite, but not positive definite, the isosurfaces are cylinders instead of ellipsoids.
These cylinders have ellipsoidal cross sections spanning the directions with nonzero eigenvalues, but they
run in straight lines along the directions with zero eigenvalues.]
Eigenvectors and the (Anisotropic) Multivariate Normal Distribution 45
ANISOTROPIC GAUSSIANS
[Let’s revisit the multivariate Gaussian distribution, with di fferent variances along di fferent directions.]
X∼N(µ,Σ) [ Xandµared-vectors. Xis a random variable with mean µ.]
f(x)=1p
(2π)d|Σ|exp 
−1
2(x−µ)⊤Σ−1(x−µ)!
↑determinant of Σ
Σis the d×dSPD covariance matrix .
Σ−1is the d×dSPD precision matrix .
Write f(x)=n(q(x)), where q(x)=(x−µ)⊤Σ−1(x−µ)
↑ ↑
R→R, exponential Rd→R, quadratic
[Now q(x) is a function we understand—it’s just a quadratic bowl centered at µ, the quadratic form of the
precision matrix Σ−1. The other function n(·) is a simple, monotonic, convex function, an exponential of the
negation of half its argument. This mapping n(·) does not change the isosurfaces.]
Principle: given monotonic n:R→R, isosurfaces of n(q(x)) are same as q(x) (different isovalues).
1.2.
2.3.3.
4.4.
5.5.6.
6.7.
7.
8.8.
9.9.
10.10.
11.11.
12.12.
13.13.
14.14.
15.15.
16.16.
17.17.
18.18.
19.19.
-2-1012-2-1012
q(x)→
01234x0.050.100.15n(x)
n(·)→
0.0360.036
0.0720.0720.108
0.1080.1440.144
0.180.216
0.2520.2880.3240.36
-2-1012-2-1012
f(x)=n(q(x))
ellipsebowl.pdf, ellipses.pdf, exp.pdf, gauss3d.pdf, gausscontour.pdf
[(Show this figure on a separate “whiteboard” for easy reuse next lecture.) A paraboloid
(left) becomes a bivariate Gaussian (right) after you compose it with a suitable scalar func-
tion (center).]
46 Jonathan Richard Shewchuk
[One of the main ideas is that if you understand the isosurfaces of a quadratic function, then you understand
the isosurfaces of a Gaussian, because they’re the same. The di fferences are in the isovalues—in particular,
the Gaussian achieves its maximum at the mean, and decreases to zero as you move infinitely far away from
the mean.]
The isocontours of ( x−µ)⊤Σ−1(x−µ) are determined by eigenvectors /values of Σ1/2.
123
45 55
5
6 66
6
77
7
7
-2 -1 0 1 2-2-1012
isocontours transformed
byΣ1/2
−→
1.2.
2.3.3.
4.4.
5.5.6.
6.7.
7.
8.8.
9.9.
10.10.
11.11.
12.12.
13.13.
14.14.
15.15.
16.16.
17.17.
18.18.
19.19.
-2-1012-2-1012
Aside: q(x) is the squared distance from Σ−1/2xtoΣ−1/2µ. Consider the metric
d(x,µ)=Σ−1/2x−Σ−1/2µ=q
(x−µ)⊤Σ−1(x−µ)=p
q(x).
[So we think of the precision matrix as a “metric tensor” which defines a metric, a sort of warped distance
from xto the mean µ.]
Covariance
LetR,Sbe random variables—column vectors or scalars
Cov( R,S)=E[(R−E[R]) (S−E[S])⊤]=E[RS⊤]−µRµ⊤
S
Var(R)=Cov( R,R)
IfRis a vector, covariance matrix forRis
Var(R)=Var(R1) Cov( R1,R2)... Cov( R1,Rd)
Cov( R2,R1) Var( R2) Cov( R2,Rd)
.........
Cov( Rd,R1) Cov( Rd,R2)... Var(Rd)[symmetric; each Riis scalar]
For a Gaussian R∼N(µ,Σ), one can show Var( R)= Σ. [. . . as you did in Homework 2.]
[An important point is that statisticians didn’t just arbitrarily decide to call Σa covariance matrix. Rather,
statisticians discovered that if you find the covariance of the normal distribution by integration, it turns out
that the covariance is Σ. This is a happy fact; it’s rather elegant.]
Ri,Rjindependent⇒ Cov( Ri,Rj)=0 [the reverse implication is not generally true, but . . . ]
Cov( Ri,Rj)=0 AND multivariate normal dist. ⇒ Ri,Rjindependent
all features pairwise independent ⇒ Var(R) is diagonal [the reverse is not generally true, but . . . ]
Var(R) is diagonal AND multi normal
⇔ f(x)|{z}
multivariate=f(x1)f(x2)···f(xd)|                   {z                   }
univariate Gaussians
⇒ ellipsoids are axis-aligned, with squared radii on diagonal of Σ =Var(R)
[So when the features are independent, you can write the multivariate Gaussian PDF as a product of uni-
variate Gaussian PDFs. When they aren’t, you can do a change of coordinates to the eigenvector coordinate
system, and write it as a product of univariate Gaussian PDFs in eigenvector coordinates. You did something
very similar in Q7.2 of Homework 2.]
Anisotropic Gaussians: MLE, QDA, and LDA Revisited 47
9 Anisotropic Gaussians: MLE, QDA, and LDA Revisited
GDA WITH ANISOTROPIC GAUSSIANS
[Recall from our last lecture the probability density function of the multivariate normal distribution in its
full generality. xandµared-vectors.]
Normal PDF: f(x)=n(q(x)), n(q)=1p
(2π)d|Σ|e−q/2, q(x)=(x−µ)⊤Σ−1(x−µ).
↑ ↑ ↑
R→R, exponential determinant of ΣRd→R, quadratic
[The covariance matrix Σand its symmetric square root and its inverse all play roles in our intuition about
the multivariate normal distribution. Consider their eigendecompositions.]
Σ =VΓV⊤covariance matrix
↑eigenvalues of Σare variances along the eigenvectors, Γii=σ2
i
Σ1/2=VΓ1/2V⊤maps spheres to ellipsoids [recall end of last lecture]
↑eigenvalues of Σ1/2are Gaussian widths /ellipsoid radii /standard deviations,√Γii=σi
123
45 55
5
6 66
6
77
7
7
-2 -1 0 1 2-2-1012
Σ1/2
−→
1.2.
2.3.3.
4.4.
5.5.6.
6.7.
7.
8.8.
9.9.
10.10.
11.11.
12.12.
13.13.
14.14.
15.15.
16.16.
17.17.
18.18.
19.19.
-2-1012-2-1012←−quadratic form
q(x)=(x−µ)⊤Σ−1(x−µ)
Σ−1=VΓ−1V⊤precision matrix (metric tensor) [ ↑quadratic form of Σ−1defines contours]
[Recall from last lecture
that the isocontours of the
multivariate normal
distribution are the same as
the isocontours of the
quadratic form of the
precision matrix Σ−1.]
1.2.
2.3.3.
4.4.
5.5.6.
6.7.
7.
8.8.
9.9.
10.10.
11.11.
12.12.
13.13.
14.14.
15.15.
16.16.
17.17.
18.18.
19.19.
-2-1012-2-1012
q(x)→
01234x0.050.100.15n(x)
n(·)→
0.0360.036
0.0720.0720.108
0.1080.1440.144
0.180.216
0.2520.2880.3240.36
-2-1012-2-1012
f(x)=n(q(x))
48 Jonathan Richard Shewchuk
Maximum Likelihood Estimation for Anisotropic Gaussians
-4 -2 0 2 4 6 8 10-4-20246810
MLE40pts.pdf, MLE40pts3D.pdf [Maximum likelihood estimation takes these 40 points
as input and outputs this Gaussian. Note that the points do not actually come from a normal
distribution; they come from a uniform distribution over a tilted rectangle. Nevertheless,
the Gaussian is a decent approximation of that.]
Given training points X1,..., Xnand classes y1,..., yn, find best-fit Gaussians.
LetnC=# of training pts in class C.
[Once again, we want to choose the Gaussian parameters that maximize the likelihood of generating the
training points in a specified class. This time I won’t derive the maximum-likelihood Gaussian; I’ll just tell
you the answer.]
For QDA:
ˆΣC=1
nCX
i:yi=C(Xi−ˆµC) (Xi−ˆµC)⊤
|                    {z                    }
outer product matrix, d×d⇐conditional covariance for pts in class C
Prior ˆπC, mean ˆµC: same as Lecture 7.
[ˆπCis number of training points in class C ÷total training points; ˆ µCis mean of training points in class C.]
ˆΣCis positive semidefinite, but not always definite!
[If there are some zero eigenvalues, the standard version of QDA just doesn’t work. We can try to fix it by
eliminating the zero-variance dimensions (eigenvectors). Homework 3 suggests a way to do that.]
For LDA:
ˆΣ =1
nX
CX
i:yi=C(Xi−ˆµC) (Xi−ˆµC)⊤⇐pooled within-class covariance matrix
Anisotropic Gaussians: MLE, QDA, and LDA Revisited 49
[Let’s revisit QDA and LDA and see what has changed now that we use the multivariate normal distribution
in its full, anisotropic generality. The short answer is “not much has changed, but the graphs look cooler.”
Conflicting notation warning: capital Xrepresents a random variable, but later it will represent a matrix.]
QDA
Choosing C that maximizes P(Y=C|X=x) is equivalent to maximizing the quadratic discriminant fn
QC(x)=ln
(√
2π)dfX|Y=C(x)πC
=−1
2(x−µC)⊤Σ−1
C(x−µC)−1
2ln|ΣC|+lnπC.
[This works for any number of classes. In a multi-class problem, you just pick the class with the greatest
quadratic discriminant for x.]
2 classes: Decision fn QC(x)−QD(x) is quadratic, but may be indefinite.
⇒Decision boundary is a quadric.
Posterior is P(Y=C|X=x)=s(QC(x)−QD(x)) where s(·) is logistic fn.
fX|Y=C(x) &fX|Y=D(x)
-3 -2 -1 0 1 2 3-3-2-10123
 QC−QD
-3 -2 -1 0 1 2 3-3-2-10123→
-4 -2 0 2 4x0.20.40.60.81.0s(x)
s(·)→
s(QC−QD)
0.10.1
0.20.2
0.30.3
0.4 0.4
0.50.50.6
0.60.70.7
0.8
0.80.9
0.9
-3 -2 -1 0 1 2 3-3-2-10123
qdaaniso3d.pdf, qdaanisocontour.pdf, qdaanisodi ff3d.pdf, qdaanisodi ffcontour.pdf,
logistic.pdf, qdaanisoposterior3d.pdf, qdaanisoposteriorcontour.pdf
[(Show this figure on a separate “whiteboard.”) An example where the decision boundary
is a hyperbola—which is not possible with isotropic Gaussians. At left, two anisotropic
Gaussians. Center left, the di fference QC−QD. After applying the logistic function to this
difference we obtain the posterior probabilities at right, which tells us the probability that x
is in class C. Observe that we can see the decision boundary in all three contour plots: it is
QC−QD=0 and s(QC−QD)=0.5. We don’t need to apply the logistic function to find
the decision boundary, but we do need to compute it if we want the posterior probabilities.]
50 Jonathan Richard Shewchuk
[This procedure has two interpretations. If we actually know the exact, true parameters πC,µC, and ΣC, this
procedure gives us the Bayes classifier and the Bayes optimal decision boundary. By contrast, when we
estimate ˆπC, ˆµC, and ˆΣCfrom data, this procedure is the QDA algorithm. We hope the QDA classifier will
approximate the Bayes classifier. Sometimes in our textbooks, you will see examples where they plot both
the Bayes optimal decision boundary and the decision boundary computed by a learning algorithm. (See
the figure two pages forward.) When you see that, the authors know the exact, true probability distributions
because they have chosen them and written a program that produces synthetic data from those distributions.
With data from the real world, you cannot know the Bayes optimal decision boundary.]
Multi-class QDA:
aniso.pdf [When you have many classes, their QDA decision boundaries form an
anisotropic V oronoi diagram . Interestingly, a cell of this diagram might not be connected.]
LDA
OneΣfor all classes. Decision fn is
[Once again, the quadratic terms cancel each other out so the decision function is linear and the decision
boundary is a hyperplane.]
QC(x)−QD(x)=(µC−µD)⊤Σ−1x|               {z               }
w⊤x−µ⊤
CΣ−1µC−µ⊤
DΣ−1µD
2+lnπC−lnπD
|                                               {z                                               }
+α.
Decision boundary is w⊤x+α=0.
Posterior is P(Y=C|X=x)=s(w⊤x+α).
Multi-class LDA: choose class C that maximizes the linear discriminant fn
µ⊤
CΣ−1x−µ⊤
CΣ−1µC
2+lnπC. [works for any # of classes]
[Note that we use a linear solver to e fficiently compute µ⊤
CΣ−1just once, so the classifier can evaluate test
points quickly.]
Anisotropic Gaussians: MLE, QDA, and LDA Revisited 51
fX|Y=C(x) &fX|Y=D(x)
-3 -2 -1 0 1 2 3-3-2-10123
 QC−QD
-3 -2 -1 0 1 2 3-3-2-10123→
-4 -2 0 2 4x0.20.40.60.81.0s(x)
s(·)→
s(QC−QD)
0.1
0.2
0.30.4
0.50.6
0.7
0.8
0.9
-3 -2 -1 0 1 2 3-3-2-10123
ldaaniso3d.pdf, ldaanisocontour.pdf, ldaanisodi ff3d.pdf, ldaanisodi ffcontour.pdf,
logistic.pdf, ldaanisoposterior3d.pdf, ldaanisoposteriorcontour.pdf
[(Show this figure on a separate “whiteboard.”) In LDA, the decision boundary is always a
hyperplane. Note that Mathematica messed up the top left plot a bit; there should be no red
in the left corner, nor blue in the right corner.]
oooo
oo o
oo
oo
o
oo o
oo
oooo
o
oo
ooooo
oo
ooo
o
ooo
oooo
oo
oooo
o
ooo
o
oo
ooo
o
o
o
ooo
o
oo
oo oo
oo
ooo
o
oo
oo
oo
o
oo
o
oooo
o
oo
oo
oo
oooo
ooo
o
o
o
o
oo
o
oo
oo
o
o
ooo
o
ooo
ooo
oo
o
ooo
oo
oooo
oo
oo
o
ooo
o
o
ooo
o
ooo
oo
oo
oooo
oo
oo
o
oo
o
ooo
o
oooo
oooo
oo
o o
ooo
o
o
o
oo
oooo
oo
oo
o
oo
ooo
o
oo
oo
oo
o oo
ooo
o
oo
oo
oooo
oo
oo
oo
o
oo
o
oo
oooo
o
ooo
oooooo
ooo
oo
oo
ooooo
ooo
o
ooo
o oo
oo
ooo
o
oooo
oo
o
ooo
o
ooo
ooooo
o
oo
oo
o oo
oo
oo
oo
oo
o
ooo
oo
o
o
oo
o
oo
oooo
o
o
oooo
o
oo
oo
o
oo
oo
o
oo
ooo
oooo
oo
oo
ooo
oo
oo
o
oo
oo
ooo
o
oo
oo
o
oooo
oooo
o
oo
oo
o
ooo
ooooo
ooo
oo
oo
oo
oo
o
o
o
o
o
oo
oo
o
oo
ooo
o
o o
oo
o o 
oo
o
oo
o
ooo
oo
ooo
o
oo
o
oo o
ooo
oo
oo
o
oo
oo
oo
oo
ooo
ooo
o
oo
o
oo
oo
oo
oo
o
oo
oo
oo
oo
ooo
o
oo
ooo
oo
oo
o
oo
o
o
o••••••••••••••
••
••
••••
LDAdata.pdf (ESL, Figure 4.11) [An example of LDA with messy data. The real-world
distributions almost surely aren’t Gaussians, but LDA still works reasonably well.]
52 Jonathan Richard Shewchuk
Notes on QDA /LDA
– For 2 classes,
– LDA has d+1 parameters ( w,α);
[. . . in the decision function. We estimated more statistical parameters than that, but only the
degrees of freedom of the decision function matter for diagnosing underfitting or overfitting.]
– QDA hasd(d+3)
2+1 params;
– LDA more likely to underfit;
– QDA more likely to overfit. [The danger is much bigger when the dimension dis large.]
−4 −2 0 2 4 −4 −3 −2 −1 0 1 2 
−4 −2 0 2 4 −4 −3 −2 −1 0 1 2 X2
ldaqda.pdf (ISL, Figure 4.9) [In these examples, the Bayes optimal decision boundary is
purple (and dashed), the QDA decision boundary is green, the LDA decision boundary is
black (and dotted). When the Bayes optimal boundary is linear, as at left, LDA gives a
more stable fit whereas QDA may overfit. When the Bayes optimal boundary is curved, as
at right, QDA often gives you a better fit.]
– QDA on data doesn’t find true optimum Bayes classifier.
– estimate distributions from finite data.
– real-world data not perfectly Gaussian.
– Changing priors or loss =adding constants to discriminant fns.
[So it’s very easy. In the 2-class case, it’s equivalent to changing the isovalue . . . ]
– Posterior gives decision boundaries for 10% probability, 50%, 90%, etc.
choosing isovalue =probability pis equivalent to
– choosing πC=1−p,πD=p; OR
– choosing asymmetrical loss pfor false positive, 1 −pfor false negative.
– With added features, LDA can give nonlinear boundaries; QDA nonquadratic.
[LDA & QDA are the best method in practice for many applications. In the STATLOG project, either LDA
or QDA were among the top three classifiers for 10 out of 22 datasets. But it’s not because all those datasets
are Gaussian. LDA & QDA work well when the data can only support simple decision boundaries such as
linear or quadratic, because Gaussian models provide stable estimates. See ESL, Section 4.3.]
Anisotropic Gaussians: MLE, QDA, and LDA Revisited 53
Some Terms
LetXben×ddesign matrix of sample pts
Each row iofXis a sample pt X⊤
i.
[Now I’m using capital Xas a matrix instead of a random variable vector. I’m treating Xias a column vector
to match the standard convention for multivariate PDFs like the Gaussian, but X⊤
iis a row of X.]
centering X: subtracting ˆ µ⊤from each row of X.X→˙X
[ˆµ⊤is the mean of all the rows of X. Now the mean of all the rows of ˙Xis zero.]
LetRbe drawn from uniform distribution on sample pts. Sample covariance matrix is
Var(R)=1
n˙X⊤˙X.
[This is the simplest way to remember how to compute a covariance matrix for QDA. Imagine you have a
design matrix XCthat contains only the sample points of class C; then you have ˆΣC=1
nC˙X⊤
C˙XC.]
[When we have points from an anisotropic Gaussian distribution, sometimes it’s useful to perform a linear
transformation that maps them to an axis-aligned distribution, or maybe even to an isotropic distribution.]
decorrelating ˙X: applying rotation Z=˙XV, where Var( R)=VΛV⊤
[rotates the sample points to the eigenvector coordinate system]
Then Var( Z)= Λ. [ Zhas diagonal covariance. If Xi∼N(µ,Σ), then approximately, Zi∼N(0,Λ).]
[Proof: Var( Z)=1
nZ⊤Z=1
nV⊤˙X⊤˙XV=V⊤Var(R)V=V⊤VΛV⊤V= Λ.]
original.jpg, centered.jpg, decorrelated.jpg, whitened.jpg
sphering ˙X: applying transform W=˙XVar(R)−1/2[Recall that Σ−1/2maps ellipsoids to spheres.]
whitening X: centering +sphering, X→W
Then Whas covariance matrix I. [If Xi∼N(µ,Σ), then approximately, Wi∼N(0,I).]
[Whitening input data is often used with other machine learning algorithms, like SVMs and neural networks.
The idea is that some features may be much bigger than others—for instance, because they’re measured in
different units. SVMs penalize violations by large features more heavily than they penalize small features.
Whitening the data before you run an SVM puts the features on an equal basis.]
[One nice thing about discriminant analysis is that whitening is built in.]
[Incidentally, what we’ve done here—computing a sample covariance matrix and its eigenvectors /values—
is about 75% of an important unsupervised learning method called principal components analysis , or PCA,
which we’ll learn later in the semester.]
54 Jonathan Richard Shewchuk
10 Regression, including Least-Squares Linear and Logistic Regression
REGRESSION aka Fitting Curves to Data
Classification: given point x, predict class (often binary)
Regression: given point x, predict a numerical value
[Classification gives a discrete prediction, whereas regression gives us a quantitative prediction, usually on a
continuous scale. We’ve already seen an example of regression in Gaussian discriminant analysis. QDA and
LDA don’t just estimate a classifier; they also give us the probability that a particular prediction is correct.
So QDA and LDA do regression on probability values.]
– Choose form of regression fn h(x;w) with parameters w (h=hypothesis )
– like decision fn in classification [e.g., linear, quadratic, logistic in x]
– Choose a cost fn (objective fn) to optimize
– usually based on a loss fn; e.g., empirical risk =expected loss on data
Some regression fns:
(1) linear: h(x;w,α)=w·x+α
(2) polynomial [equivalent to linear regression with added polynomial features]
(3) logistic: h(x;w,α)=s(w·x+α) recall: logistic fn s(γ)=1
1+e−γ
[The last choice is interesting. You’ll recall that LDA produces a posterior probability function with this
expression. So the logistic function seems to be a natural form for modeling certain probabilities. If we want
to model posterior probabilities, sometimes we use LDA; but alternatively, we could skip fitting Gaussians
to points, and instead just try to directly fit a logistic function to a set of probabilities.]
Some loss fns: let ˆ ybe prediction h(x);ybe true label
(A) L(ˆy,y)=(ˆy−y)2squared error
(B) L(ˆy,y)=|ˆy−y| absolute error
(C) L(ˆy,y)=−yln ˆy−(1−y) ln(1−ˆy) logistic loss , aka cross-entropy :y∈[0,1], ˆy∈(0,1)
Some cost fns to minimize:
(a) J(h)=1
nPn
i=1L(h(Xi),yi) mean loss [you can leave out the “1
n”]
(b) J(h)=maxn
i=1L(h(Xi),yi) maximum loss
(c) J(h)=Pn
i=1ωiL(h(Xi),yi) weighted sum [some points are more important than others]
(d) J(h)=(a), (b), or (c) +λ∥w∥2ℓ2penalized /regularized
(e) J(h)=(a), (b), or (c) +λ∥w∥ℓ1ℓ1penalized /regularized
Some famous regression methods:
Least-squares linear regr.: (1) +(A)+(a)quadratic cost; minimize w /calculus Weighted least-squ. linear: (1) +(A)+(c)
Ridge regression: (1) +(A)+(a)+(d)
Lasso: (1) +(A)+(a)+(e) quadratic program
Logistic regr.: (3) +(C)+(a) convex cost; minimize w /gradient descent
Least absolute deviations: (1) +(B)+(a)
linear programChebyshev criterion: (1) +(B)+(b)
[I have given you several choices of regression function, several choices of loss function, and several choices
of objective function. You can snap one part out and replace it with a di fferent one. But the optimization
algorithm and its speed depend crucially on which parts you pick. Let’s consider some examples.]
Regression, including Least-Squares Linear and Logistic Regression 55
LEAST-SQUARES LINEAR REGRESSION (Gauss, 1801)
Linear regression fn (1) +squared loss fn (A) +cost fn (a).
Find w,αthat minimizesnX
i=1(Xi·w+α−yi)2.
•• •
••••
••••
•••
••
•••
•
••
••
•••
••••••
••••
•••
••
••
•
••
••
••••
••
••
•
•••
•
••••
••••
••
••
X1X2
linregress.pdf (ISL, Figure 3.4) [An example of linear regression.]
Convention: Xisn×ddesign matrix of sample pts
yisn-vector of scalar labels
X11X12... X1j... X1d
X21X22 X2j X2d
...
Xi1Xi2 Xi j Xid
...
Xn1Xn2 Xn j Xnd←point X⊤
iy1
y2
...
yn
↑ ↑
feature column X∗j y
Usually n>d. [But not always.]
Recall fictitious dimension trick [from Lecture 3]: rewrite h(x)=x·w+αas
[x1x21]w1
w2
α.
Now Xis an n×(d+1) matrix; wis a ( d+1)-vector. [We’ve added a column of all-1’s to the end of X.]
[We rewrite the optimization problem above:]
Find wthat minimizes∥Xw−y∥2=RSS( w), for residual sum of squares
56 Jonathan Richard Shewchuk
Optimize by calculus:
minimize RSS( w)=w⊤X⊤Xw−2y⊤Xw+y⊤y
∇RSS =2X⊤Xw−2X⊤y=0
⇒ X⊤X|{z}
(d+1)×(d+1)w=X⊤y|    {z    }
(d+1)−vectors⇐the normal equations [wunknown; X&yknown]
IfX⊤Xis singular, problem is underconstrained.
[. . . because the sample points all lie on a common subspace (through the origin).]
[Notice that X⊤Xis always positive semidefinite, but not always positive definite.]
We use a linear solver to find w=(X⊤X)−1X⊤
|        {z        }
X+,the pseudoinverse ofX,(d+1)×ny. [never actually invert the matrix!]
[We never compute X+directly, but we are interested in the fact that wis a linear transformation of y.]
[Xis usually not square, so Xcan’t have an inverse. However, every Xhas a pseudoinverse X+, and if X⊤X
is invertible, then X+is a “left inverse.”]
Observe: X+X=(X⊤X)−1X⊤X=I⇐(d+1)×(d+1) [which explains the name “left inverse”]
Observe: the predicted values of yiare ˆyi=w·Xi⇒ ˆy=Xw=XX+y=Hy
where H|{z}
n×n=XX+is called the hat matrix because it puts the hat on y.
[Ideally, Hwould be the identity matrix and we’d have a perfect fit, but if n>d+1, then His singular.]
Advantages:
– Easy to compute; just solve a linear system.
– Unique, stable solution. [. . . except when the problem is underconstrained.]
Disadvantages:
– Very sensitive to outliers, because errors are squared!
– Fails if X⊤Xis singular. [Which means the problem is underconstrained, has multiple solutions.]
[In discussion section 6, we’ll address how to handle the underconstrained case where X⊤Xis singular.]
[Apparently, least-squares linear regression was first posed and solved in 1801 by the great mathematician
Carl Friedrich Gauss, who used least-squares regression to predict the trajectory of the planetoid Ceres.
A paper he wrote on the topic is regarded as the birth of modern linear algebra.]
LOGISTIC REGRESSION (David Cox, 1958)
Logistic regression fn (3) +logistic loss fn (C) +cost fn (a).
Fits “probabilities” in range [0 ,1].
Usually used for classification. The input yi’scanbe probabilities,
but in most applications they’re all 0 or 1.
QDA, LDA: generative models
logistic regression: discriminative model
[We’ve learned from LDA that in classification, the posterior probabilities are often modeled well by a
logistic function. So why not just try to fit a logistic function directly to the data, skipping the Gaussians?]
Regression, including Least-Squares Linear and Logistic Regression 57
With Xandwincluding the fictitious dimension; αisw’s last component . . .
Find wthat minimizes
J=nX
i=1L(s(Xi·w),yi)=−nX
i=1yilns(Xi·w)+(1−yi) ln (1−s(Xi·w)).
L(ˆy,0)
0.0 0.2 0.4 0.6 0.8 1.0z1234L(z) L(ˆy,0.7)
0.0 0.2 0.4 0.6 0.8 1.0z1234L(z)
logloss0.pdf, loglosspt7.pdf [Plots of the loss L(ˆy,y) for y=0 (left) and y=0.7 (right). As
you might guess, the left function is minimized at ˆ y=0, and the right function is minimized
at ˆy=0.7. These loss functions are always convex.]
J(w) is convex! Solve by gradient descent.
[To do gradient descent, we’ll need to compute some derivatives.]
s′(γ)=d
dγ1
1+e−γ=e−γ
(1+e−γ)2
=s(γ) (1−s(γ))
-4 -2 0 2 4x0.20.40.60.81.0s(x)
-4 -2 0 2 4x0.050.100.150.200.25s(x)
logistic.pdf, dlogistic.pdf [Plots of s(γ) (left) and s′(γ) (right).]
Letsi=s(Xi·w)
∇wJ=−X yi
si∇si−1−yi
1−si∇si!
=−X yi
si−1−yi
1−si!
si(1−si)Xi
=−X
(yi−si)Xi
=−X⊤(y−s(Xw)) where s(Xw)=s1
s2
...
sn[applies scomponent-wise to Xw]
58 Jonathan Richard Shewchuk
Gradient descent rule: w←w+ϵX⊤(y−s(Xw))
Stochastic gradient descent: w←w+ϵ(yi−s(Xi·w))Xi
Works best if we shu ffle points in random order, process one by one.
For very large n, sometimes converges before we visit all points!
[This looks a lot like the perceptron learning rule. The only di fference is that the “ −si” part is new.]
Starting from w=0 works well in practice.
problogistic.png, by “mwascom” of Stack Overflow
http://stackoverflow.com /questions /28256058 /plotting-decision-boundary-of-logistic-regression
[An example of logistic regression.]
If sample pts are linearly separable and w·x=0 separates them (with decision boundary touching no pt),
scaling wto have infinite length causes s(Xi·w)→1 for a pt iin class C, s(Xi·w)→0 for a pt not in class C,
andJ(w)→0 [in the limit as∥w∥→∞ ].
[Moreover, making wgrow extremely large is the only way to get the cost function Jto approach zero.]
Therefore, logistic regression always separates linearly separable pts!
[In this case, the cost function J(w) has no finite local minimum, but gradient descent will “converge” to a
solution, in the sense that the cost Jwill get arbitrarily close to zero, though of course the weight vector w
will never become infinitely long. Mathematically speaking, wdoesn’t converge at all—it diverges—though
J(w) does converge to zero.]
[A 2018 paper by Soudry, Ho ffer, Nacson, Gunasekar, and Srebro shows that gradient descent applied to
logistic regression eventually converges to the maximum margin classifier, but the convergence is very, very
slow. A practical logistic regression solver should use a di fferent optimization algorithm.]
Polynomial and Weighted Regression; Newton’s Method; ROC Curves 59
11 Polynomial and Weighted Regression; Newton’s Method; ROC Curves
LEAST-SQUARES POLYNOMIAL REGRESSION
Replace each Xiwith feature vector Φ(Xi) with all terms of degree 0 ...p
e.g.,Φ(Xi)=[X2
i1Xi1Xi2X2
i2Xi1Xi21]⊤
[Notice that we’ve added the fictitious dimension “1” here, so we don’t need to add it again to do linear or
logistic regression. This basis covers all polynomials quadratic in Xi1andXi2.]
Otherwise just like linear or logistic regression.
Log. reg. +quadratic features =same form of posteriors as QDA.
Very easy to overfit!
overunder.png, degree20.png, UScensusquartic.png
[Here are some examples of polynomial overfitting, to show the importance of choosing the polynomial
degree very carefully. At left, we have sampled points from a degree-3 curve (black) with added noise. We
show best-fit polynomials of degrees 2, 4, 6, and 8 found by regression of the black points. The degree-4
curve (green) fits the true curve (black) well, whereas the degree-2 curve (red) underfits and the degree-6
and 8 curves (blue, yellow) overfit the noise and oscillate. The oscillations in the yellow degree-8 curve are
a characteristic problem of polynomial interpolation.]
[At upper right, a degree-20 curve shows just how insane high-degree polynomial oscillations can get. It
takes a great deal of densely spaced data to tame the oscillations in a high degree curve, and there isn’t
nearly enough data here.]
[At lower right, somebody has regressed a degree-4 curve to U.S. census population numbers. The curve
doesn’t oscillate, but can you nevertheless see a flaw? This shows the di fficulty of extrapolation outside the
range of the data. As a general rule, extrapolation is much harder than interpolation. The k-nearest neighbor
classifier is one of the few that does extrapolation decently without occasionally returning crazy values.]
60 Jonathan Richard Shewchuk
order10extrap.pdf [From Mehta, Wang, Day, Richardson, Bukov, Fisher, and Schwab, “A
High-Bias, Low-Variance Introduction to Machine Learning for Physicists.”]
[This example shows that a fitted degree-10 polynomial (green) can be tamed by using a very large amount
of training data (left), even if the training data is noisy. The training data was generated from a di fferent
degree-10 polynomial, with noise added. On the right, the same curves are plotted, but the blue diamonds
are test points, some of which go outside the range of the training data. We see that the degree-10 regression
does decent extrapolation for a short distance, albeit only because the original data was also from a degree-10
polynomial.]
WEIGHTED LEAST-SQUARES REGRESSION
Linear regression fn (1) +squared loss fn (A) +cost fn (c).
[The idea of weighted least-squares is that some sample points might be more trusted than others, or there
might be certain points you want to fit particularly well. So you assign those more trusted points a higher
weight. If you suspect some points of being outliers, you can assign them a lower weight.]
Assign each sample pt a weight ωi; collectωi’s in n×ndiagonal matrix Ω.
Greaterωi→work harder to minimize (ˆ yi−yi)2recall: ˆ y=Xw [ˆyiis predicted label for Xi]
Find wthat minimizes ( Xw−y)⊤Ω(Xw−y)=nX
i=1ωi(Xi·w−yi)2.
[As with ordinary least-squares regression, we find the minimum by setting the gradient to zero, which leads
us to the normal equations.]
Solve for win normal equations: X⊤ΩXw=X⊤Ωy
NEWTON’S METHOD
Iterative optimization method for smooth fn J(w).
Often much faster than gradient descent. [We’ll use Newton’s method for logistic regression.]
Idea: You’re at point v. Approximate J(w) near vby quadratic fn.
Jump to its unique critical pt. Repeat until bored.
Polynomial and Weighted Regression; Newton’s Method; ROC Curves 61
-2 2 4
-20-101020304050
-2 2 4
-20-101020304050
-2 2 4
-20-101020304050
newton1.pdf, newton2.pdf, newton3.pdf [Three iterations of Newton’s method in one-
dimensional space. We seek the minimum of the blue curve, J. Each brown curve is a
local quadratic approximation to J. Each iteration, we jump to the bottom of the brown
parabola.]
newton2D.png [Steps taken by Newton’s method in two-dimensional space.]
Taylor series about v:
∇J(w)=∇J(v)+(∇2J(v)) (w−v)+O(∥w−v∥2)
where∇2J(v) is the Hessian matrix ofJatv.
Approximate critical pt wby setting∇J(w)=0:
w≈v−(∇2J(v))−1∇J(v)
[This is an iterative update rule you can repeat until it converges to a solution. As usual, we probably don’t
want to compute a matrix inverse directly. It is faster to solve a linear system of equations, typically by
Cholesky factorization or the conjugate gradient method.]
Newton’s method:
pick starting point w
repeat until convergence
e←solution to linear system ( ∇2J(w))e=−∇J(w)
w←w+e
Warning: Doesn’t know di fference between minima, maxima, saddle pts.
Starting pt must be “close enough” to desired critical pt.
62 Jonathan Richard Shewchuk
[If the objective function Jis actually quadratic, Newton’s method needs only one step to find the exact
solution. The closer Jis to quadratic, the faster Newton’s method tends to converge.]
[Newton’s method is superior to gradient descent with a fixed step size for some optimization problems for
at least two reasons. First, it tries to find the right step length to reach the minimum, rather than just walking
an arbitrary distance downhill. Second, rather than follow the direction of steepest descent, it tries to choose
a better descent direction.]
[Nevertheless, it has some major disadvantages. The biggest one is that computing the Hessian can be quite
expensive, and it has to be recomputed every iteration. It can work well for low-dimensional weight spaces,
but you would never use it for a neural network, because there are too many weights. Newton’s method
also doesn’t work for most nonsmooth functions. It particularly fails for the perceptron risk function, whose
Hessian is zero, except where the Hessian is not even defined.]
LOGISTIC REGRESSION (continued)
[Let’s use Newton’s method to solve logistic regression faster.]
Recall: s′(γ)=s(γ) (1−s(γ)), si=s(Xi·w), s=s1
s2
...
sn,
∇wJ(w)=−nX
i=1(yi−si)Xi=−X⊤(y−s)
[Now let’s derive the Hessian too, so we can use Newton’s method.]
∇2
wJ(w)=nX
i=1si(1−si)XiX⊤
i=X⊤ΩX where Ω =s1(1−s1) 0 ... 0
0 s2(1−s2) 0
.........
0 0 ... sn(1−sn)
Ωis+ve definite∀w⇒ X⊤ΩXis+ve semidefinite∀w⇒ Jis convex.
[The logistic regression cost function is convex, so Newton’s method finds a globally optimal point if it
converges at all.]
Newton’s method:
w←0
repeat until convergence
e←solution to normal equations ( X⊤ΩX)e=X⊤(y−s) Recall: Ω,sare fns of w
w←w+e
[Notice that this looks a lot like weighted least squares, but the weight matrix Ωand the right-hand-side
vector y−schange every iteration. So we call it . . . ]
An example of iteratively reweighted least squares .
[We need to be very careful with the analogy, though. The weights don’t have the same meaning they
had when we learned weighted least-squares regression, because there is no Ωon the right-hand side of
(X⊤ΩX)e=X⊤(y−s). Contrary to what you’d expect, a small weight in Ωcauses the Newton iteration to
putmore emphasis on a point when it computes e.]
Polynomial and Weighted Regression; Newton’s Method; ROC Curves 63
[Misclassified points far from the decision boundary have the most influence on the step e, and correctly
classified points far from the decision boundary have the least (because yi−siis small for such a point).
Points near the decision boundary have medium influence. But if there are no misclassified points far from
the decision boundary, then points near the decision boundary have most of the influence.]
[Here’s one more idea for speeding up logistic regression.]
Idea: If nvery large, save time by using a random subsample of the pts per iteration. Increase sample size
as you go.
[The principle is that the first iteration isn’t going to take you all the way to the optimal point, so why waste
time looking at allthe sample points? Whereas the last iteration should be the most accurate one.]
LDA vs. Logistic Regression
Advantages of LDA:
– For well-separated classes, LDA stable; log. reg. surprisingly unstable
–>2 classes easy & elegant; log. reg. needs modifying (softmax regression ) [see Discussion 6]
– LDA slightly more accurate when classes nearly normal, especially if nis small
Advantages of log. reg.:
– More emphasis on decision boundary; always separates linearly separable pts
[Correctly classified points far from the decision boundary have a small e ffect on logistic regression—
albeit a bigger e ffect than they have on SVMs—whereas misclassified points far from the decision
boundary have the biggest e ffect. By contrast, LDA gives all the sample points equal weight when
fitting Gaussians to them. Weighting points according to how badly they’re misclassified is good for
reducing training error, but it can also be bad if you want stability or insensitivity to bad data.]
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0
x10.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0
x1x2
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0
x1x2
logregvsLDAuni.pdf [Logistic regression vs. LDA for a linearly separable data set with
a very narrow margin. Logistic regression (center) always succeeds in separating linearly
separable classes, because the cost function approaches zero for a maximum margin classi-
fier. In this example, LDA (right) misclassifies some of the training points.]
– More robust on some non-Gaussian distributions (e.g., dists. w /large skew)
– Naturally fits labels between 0 and 1 [usually probabilities]
[When you use logistic regression with added quadratic features, you get a quadric decision boundary, just
as you do with QDA. Based on what I’ve said here, do you think logistic regression with quadratic features
gives you exactly the same classifier as QDA?]
64 Jonathan Richard Shewchuk
ROC CURVES (for test sets)
ROC Curve
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
ROC Curve
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
rate=% of
+ve classified
as+ve, aka
sensitivitypositiveclassifyalways
false positive rate =% of−ve classified as +vefalse negative rate
random classifiersfalse positive rate
always classify negativesensitivity true positivespecificity =true negative rate
ROC.pdf
[This is a ROC curve . That stands for receiver operating characteristics , which is an awful name but we’re
stuck with it for historical reasons.
A ROC curve is a way to evaluate your classifier after it is trained.
It is made by running a classifier on the test set or validation set.
It shows the rate of false positives vs. true positives for a range of settings.
We assume there is a knob we can turn to trade o fffalse positives against false negatives. For our purposes,
that knob is the posterior probability threshold for Gaussian discriminant analysis or logistic regression.
However, neither axis of this plot is that knob.]
x-axis: “false positive rate =% of−ve classified as +ve”
y-axis: “true positive rate =% of +ve classified as +ve aka sensitivity ”
“false negative rate”: vertical distance from curve to top [1 −sensitivity]
“specificity ”: horizontal distance from curve to right [1 −false positive rate; “true negative rate”]
[You generate this curve by trying every probability threshold; for each threshold, measure the false positive
& true positive rates and plot a point.]
upper right corner: “always classify +ve (Pr≥0)”
lower left corner: “always classify −ve (Pr>1)”
diagonal: “random classifiers”
[A rough measure of a classifier’s e ffectiveness is the area under
the curve. For a classifier that is always correct, the area under
the curve is one. For the random classifier, the area under the
curve is 1/2, so you’d better do better than that.]
IMPORTANT: In practice, the trade-o ffbetween false negatives
and false positives is usually negotiated by choosing a point on
this plot, based on real test data, and NOT by taking the choice
of threshold that’s best in theory.
[Close up, ROC curves are made of horizontal and vertical line segments (see the figure at right), as the test
data is finite and there are only finitely many thresholds where some test point’s classification changes.]
Statistical Justifications; the Bias-Variance Decomposition 65
12 Statistical Justifications; the Bias-Variance Decomposition
STATISTICAL JUSTIFICATIONS FOR REGRESSION
[So far, I’ve talked about regression as a way to fit curves to points. Recall that early in the semester I divided
machine learning into 4 levels: the application, the model, the optimization problem, and the optimization
algorithm. My last two lectures about regression were at the bottom two levels: optimization. But why did
we pick these cost functions? Today, let’s take a step up to the second level, the model. I will describe
some models, how they lead to those optimization problems, and how they contribute to underfitting or
overfitting.]
Typical model of reality:
– sample points come from unknown prob. distribution: Xi∼D.
–y-values are sum of unknown, non-random fn +random noise:
∀Xi,yi=g(Xi)+ϵi, ϵ i∼D′,D′has mean zero. [ g=“ground truth.”]
[We are positing that reality is described by a “ground truth” function g. We don’t know g, but gis not
random; it represents a consistent relationship between Xandythat we can estimate. We add to ga ran-
dom variable ϵ, which represents measurement errors and all the other sources of statistical error when we
measure real-world phenomena. Notice that the noise is independent of X. That’s a pretty questionable
assumption, and often it does not apply in practice, but that’s all we’ll have time to deal with this semester.
Also notice that this model leaves out systematic errors, like when your measuring device adds one to every
measurement, because we usually can’t diagnose systematic errors from data alone.]
Goal of regression: find hthat estimates g.
Ideal approach: choose h(x)=EY[Y|X=x]|         {z         }=g(x)+E[ϵ]=g(x).
[If this expectation exists at all, it partly justifies our model of reality. We can retroactively define gto be
this expectation.]
Least-Squares Cost Function from Maximum Likelihood
Supposeϵi∼N(0,σ2); then yi∼N(g(Xi),σ2).
Recall that log of normal PDF is
lnf(yi)=−(yi−µi)2
2σ2−constant ⇐µi=g(Xi)
& log likelihood is
ℓ(g;X,y)=ln (f(y1)f(y2)···f(yn))=lnf(y1)+...+lnf(yn)=−1
2σ2X
(yi−g(Xi))2−constant.
Takeaway: Max likelihood on “parameter” g⇒choose a gthat minimizesP(yi−g(Xi))2.
[We treat gas a “distribution parameter.” If the noise is normally distributed, maximum likelihood tells us
to estimate gby least-squares regression.]
[However, I’ve told you in previous lectures that least-squares is very sensitive to outliers. If the error is
truly normally distributed, that’s not a big deal, especially when you have a lot of sample points. But in
the real world, the distribution of outliers often isn’t normal. Outliers might come from wrongly measured
measurements, data entry errors, anomalous events, or just not having a normal distribution. When you have
a heavy-tailed distribution of noise, for example, least-squares isn’t a great choice.]
66 Jonathan Richard Shewchuk
Empirical Risk
The risk for hypothesis his expected loss R(h)=E[L] over all ( X,Y) in some joint distribution.
Discriminative model: we don’t know X’s dist. D. How can we minimize risk?
[If we have a generative model, we can estimate the joint probability distribution for XandYand derive the
expected loss. That’s what we did for Gaussian discriminant analysis. But today I’m assuming we don’t
have a generative model, so we don’t know those probabilities. Instead, we approximate the distribution in
a very crude way: we pretend that the sample points arethe distribution.]
Empirical distribution : the discrete uniform distribution over the sample pts
Empirical risk : expected loss under empirical distribution
ˆR(h)=1
nnX
i=1L(h(Xi),yi)
[The hat on the Rindicates that ˆRis only a cheap approximation of the true, unknown statistical risk R
we really want to minimize. Often, this is the best we can do. For many but not all distributions, the
empirical risk converges to the true risk in the limit as n→∞ . Choosing hthat minimizes ˆRis called
empirical risk minimization .]
Takeaway: this is why we [usually] minimize the average of the loss fns.
Logistic Loss from Maximum Likelihood
What cost fn should we use for probabilities?
Actual probability pt Xiis in class C is yi; predicted prob. is h(Xi).
Imagineβduplicate copies of Xi, with yiβin class C, and (1−yi)βnot.
[The size of βisn’t very important, but imagine that yiβand (1−yi)βare both integers for all i. Ifyiis
irrational, approximate it with a very close rational number.]
[Let’s use maximum likelihood estimation to choose the hypothesis most likely to generate these labels for
these sample points. The following likelihood is the probability of generating these labels in a particular
fixed order.]
Likelihood isL(h;X,y)=nY
i=1h(Xi)yiβ(1−h(Xi))(1−yi)β
Log likelihood is ℓ(h)=lnL(h)
=βX
iyilnh(Xi)+(1−yi) ln(1−h(Xi))
=−βX
logistic loss fn L(h(Xi),yi).
Takeaway: Max likelihood ⇒minimizePlogistic losses.
[So the principle of maximum likelihood explains where the weird logistic loss function comes from.]
Statistical Justifications; the Bias-Variance Decomposition 67
THE BIAS-V ARIANCE DECOMPOSITION
There are 2 sources of error in a hypothesis h:
bias: error due to inability of hypothesis hto fit gperfectly
e.g., fitting quadratic gwith a linear h
variance : error due to fitting random noise in data
e.g., we fit linear gwith a linear h, yet h,g.
g
hg
x xh
biasvar.pdf [Draw this figure by hand. At left, the error due to bias: a linear hypothesis h
just can’t fit a degree-2 ground truth gwell. At right, the error due to variance: although
hcould fit gperfectly, the noise in the data misleads it.]
Model: Xi∼D,ϵi∼D′,yi=g(Xi)+ϵi [remember that D′has mean zero]
fit hypothesis htoX,y
Now his a random variable; i.e., its weights are random
Consider arbitrary pt z∈Rd(not necessarily a sample pt!) & γ=g(z)+ϵ,ϵ∼D′
[Sozisarbitrary , whereasγisrandom .]
Note: E[γ]=g(z); Var(γ)=Var(ϵ) [the mean comes from g, and the variance comes from ϵ]
Risk fn when loss =squared error:
R(h)=E[L(h(z),γ)]
↑take expectation over possible training sets X,y& values of γ
[Stop and take a close look at this expectation. Remember that the hypothesis his a random variable. We are
taking a mean over the probability distribution of hypotheses. That seems pretty weird if you’ve never seen
it before. But remember, the training data Xandycome from a joint probability distribution. We use the
training data to choose weights, so the weights that define halso come from some probability distribution.
It might be hard to work out what that distribution is, but it exists. This “E[ ·]” is integrating the loss over all
possible values of the weights.]
=E[(h(z)−γ)2]
=E[h(z)2]+E[γ2]−2 E[γh(z)] [Observe that γandh(z) are independent]
=Var(h(z))+E[h(z)]2+Var(γ)+E[γ]2−2E[γ] E[h(z)]
=(E[h(z)]−E[γ])2+Var(h(z))+Var(γ)
=(E[h(z)]−g(z))2
|              {z              }
bias2of method+ Var(h(z))|    {z    }
variance of method+ Var(ϵ)|{z}
irreducible error
[This is called the bias-variance decomposition of the risk function. Let’s look at an intuitive interpretation
of these three parts.]
68 Jonathan Richard Shewchuk
!"
Bias, Variance, Noise
!"#$
%#&"#'()
*+"$)
,
-./0"1$/23./)4#567)$/)#(89
bvn.pdf [In this example, we’re trying to fit a sine wave with lines, which obviously aren’t
going to be accurate. At left, we have generated 50 di fferent hypotheses (lines). Each line
was generated from 20 random training points by least-squares linear regression. At up-
per right, the black curve is the ground truth function g, and the red line is the expected
hypothesis —an average over infinitely many hypotheses. We see that at most points in fea-
ture space, the bias (di fference between the black and red curves) is large, because lines
don’t fit sine waves well. However, the bias is small at some points—where the sine wave
crosses the red line. At center right, the variance is the expected squared di fference between
a random hypothesis (black line) and the expected hypothesis (red line) at an arbitrary point
in feature space. At lower right, the irreducible error is the expected squared di fference be-
tween a random test point’s noisy label and the sine wave. The irreducible error is optional;
it only makes sense to talk about it if we can make real-world measurements at test points.]
This is pointwise version [of the bias-variance decomposition.]
Mean version: let z∼Dbe random variable; take mean over Dof bias2, variance.
[So you can decompose one test point’s error into these three numbers, or you can decompose the error of
the hypothesis over its entire range into three numbers, which tells you roughly how big they’ll be on a large
test set.]
Statistical Justifications; the Bias-Variance Decomposition 69
[Now I will write down a list of consequences of what we’ve just learned.]
– Underfitting =too much bias
– Most overfitting caused by too much variance
– Training error reflects bias but not much variance; test error reflects both
[which is why low training error can fool you when you’ve overfitted]
– For many distributions, variance →0 asn→∞
– If hcan fit gexactly, for many distributions bias →0 asn→∞
– If hcannot fit gwell, bias is large at “most” points
– Adding a good feature reduces bias; adding a bad feature rarely increases it
– Adding a feature usually increases variance [don’t add a feature unless it reduces bias more]
– Can’t reduce irreducible error [hence its name]
– Noise in test set a ffects only Var( ϵ);
noise in training set a ffects only bias & Var( h)
– We can’t precisely measure bias or variance of real-world data
[because we cannot know gexactly and our noise model might be wrong]
– But we can test learning algs by choosing g& making synthetic data
0 20 40 60 80 1002 4 6 8 10 12
2 5 10 200.0 0.5 1.0 1.5 2.0 2.5Mean Squared Error
2 5 10 200.0 0.5 1.0 1.5 2.0 2.5
splinefit.pdf, biasvarspline.pdf (ISL, Figures 2.9 and 2.12) [At left, a data set is fit with
splines having various degrees of freedom. The synthetic data is taken from the black curve
with added noise. At center, we plot training error (gray) and test error (red) as a function
of the number of degrees of freedom. At right, we plot the squared test error as a sum of
squared bias (blue) and variance (orange). As the number of degrees of freedom increases,
the training and test errors both decrease up to 6 degrees because the bias decreases, but for
more degrees of freedom the test error increases because the variance increases.]
[The bias-variance decomposition is sometimes called the “bias-variance trade-o ff,” because sometimes you
see a U-shaped curve like this, for some hyperparameter like polynomial degree or Cin SVMs. But that’s
misleading; it’s not really a trade-o ff. Sometimes both bias and variance are very high; sometimes both are
very low. If you try to fit 30 periods of a sine wave with a degree-10 polynomial fit to 15 points sampled
from the sine wave, both your bias and variance will be very high. Then you’re underfitting and overfitting at
the same time! They’re not opposites. There are neural networks for image classification with 99% accuracy
on test sets, so bias and variance are both admirably low. Always we seek models that can fit the ground
truth well, but aren’t easily perturbed by noise in the data.]
70 Jonathan Richard Shewchuk
Example: Least-Squares Linear Reg.
For simplicity, no fictitious dimension.
[This implies that our linear regression function has to be zero at the origin.]
Model: g(z)=v⊤z (ground truth is linear)
[So we could fit gperfectly with a linear hif not for the noise in the training set.]
Letebe noise n-vector, ei∼N(0,σ2)
Training labels: y=Xv+e
[X&yare the inputs to linear regression. We don’t know vore.]
Lin. reg. computes weights
w=X+y=X+(Xv+e)=v+X+e.|{z}
noise in weights. [We want w=v, but the noise in ybecomes noise in w.]
BIAS is|E[h(z)]−g(z)|=|E[w⊤z]−v⊤z|=|z⊤E[w−v]|=|z⊤E[X+e]|=0.
[E[X+e] is zero because X+andeare independent, and e’s Gaussian PDF is symmetric around zero.]
Warning: This does not mean h(z)−g(z) is always 0!
Sometimes +ve, sometimes−ve, mean over training sets is 0.
[Those deviations from the mean are captured in the variance.]
[When the bias is zero, a perfect fit is possible. But when a perfect fit is possible, not all learning methods
give you a bias of zero; here it’s a benefit of the squared error loss function. With a di fferent noise or a
different loss function, we might have a nonzero bias even fitting a linear hto a linear g.]
V ARIANCE is Var( h(z))=Var(w⊤z)=Var(v⊤z+(X+e)⊤z)=Var(z⊤X+e)
[This is the dot product of a vector z⊤X+with an isotropic, normally distributed vector e. The dot product
reduces it to a one-dimensional Gaussian along the direction z⊤X+, so this variance is just the variance of
the 1D Gaussian times the squared length of the vector z⊤X+.]
=σ2z⊤X+2=σ2z⊤(X⊤X)−1X⊤X(X⊤X)−1z
=σ2z⊤(X⊤X)−1z.
If we choose coordinate system so Dhas mean zero, then X⊤X→nVar(D) asn→∞ , so for z∼D,
Var(h(z))≈σ2d
n.
[Where dis the dimension—the number of features per sample point.]
[Why? With the eigendecomposition Var( D)=VΛV⊤, we have E[ z⊤Var(D)−1z]=E[∥Λ−1/2V⊤z∥2]=Pd
i=1E[(vi·z)2]/λi. But as z∼D, E[(vi·z)2]=Var[vi·z]=λi, so E[ z⊤Var(D)−1z]=d. Approximating the
covariance Var( D) with the sample covariance matrix gives E[ z⊤(X⊤X)−1z]≈d/n.]
Takeaways: Bias can be zero when hypothesis function can fit the real one!
[This is a nice property of the squared error loss function.]
Variance portion of RSS (overfitting) decreases as 1 /n(sample points),
increases as d(features)
orO(dp) if you use degree- ppolynomials.
Shrinkage: Ridge Regression, Subset Selection, and Lasso 71
13 Shrinkage: Ridge Regression, Subset Selection, and Lasso
RIDGE REGRESSION aka Tikhonov Regularization
Least-squares linear regression +ℓ2penalized mean loss. (1) +(A)+(a)+(d).
Find wthat minimizes∥Xw−y∥2+λ∥w′∥2=J(w)
where w′iswwith component αreplaced by 0.
Xhas fictitious dimension but we DON’T penalize α.
Adds a regularization term, aka a penalty term, for shrinkage : to encourage small ∥w′∥. Why?
(1) Guarantees positive definite normal eq’ns; always unique solution.
[Standard least-squares linear regression yields singular normal equations when the sample points lie on a
common hyperplane in feature space—for example, when d>n.]
-4 -2 0 2 4-4-2024
-4 -2 0 2 4-4-2024
lslrcontour.pdf, lslr.pdf, ridge.pdf, ridgecontour.pdf [The cost function J(w) without and
with regularization. This plot ignores the dimension of the bias term α.]
[At left, we see a cost function for least-squares regression, a positive semidefinite quadratic form. This cost
function has many minima, and the regression problem is said to be ill-posed . By adding a small penalty
term, we obtain a positive definite quadratic form (right), with one unique minimum. “Regularization”
implies that we are turning an ill-posed problem into a well-posed problem.]
[That was the original motivation, but the next has become more important in machine learning . . . ]
(2) Reduces overfitting by reducing variance. Why?
Example: Input X1=(0,0) with label 0; X2=(1,1) with label 0; X3=(0.51,0.49) with label 1.
Linear regr. gives 50 x1−50x2. [This linear function fits all three points exactly.]
Big weights!
0.2 0.4 0.6 0.8 1.0x10.20.40.60.81.0x2
[Weights this big would be justified if there were big di fferences between
labels, or if there were small distances between points, but neither is true.
Large weights imply that tiny changes in xcan cause huge changes in y.
Consider that the labels don’t di ffer by more than 1 and the points are
separated by distances greater than 0 .7. So these disproportionately large
weights are a sure sign of overfitting.]
So we penalize large weights.
[This use of regularization is closely related to the first one. When you have
large variance and a lot of overfitting, it implies that your problem is close
tobeing ill-posed, even though technically it might be well-posed.]
72 Jonathan Richard Shewchuk
ˆw
isocontours of∥Xw−y∥2
isocontours of∥w∥2least-squares solution
ridge solution for
several values of λw2
w1
ridgeterms2.pdf (redrawing of ISL, Figure 6.7) [In this plot of weight space, for simplic-
ity, we’re not using a bias term α(we set it to zero). ˆ wis the least-squares solution. The red
ellipses are isocontours of ∥Xw−y∥2. The blue circles are isocontours of ∥w∥2, centered at
the origin. The ridge regression solution lies where a red isocontour just touches a blue iso-
contour tangentially. As λincreases, the solution will occur at a more outer red isocontour
and a more inner blue isocontour. This shrinks wand helps to reduce overfitting.]
Setting∇J=0 gives normal eq’ns
(X⊤X+λI′)w=X⊤y
where I′is identity matrix w /bottom right set to zero. [Don’t penalize the bias term α.]
[Don’t worry; X⊤X+λI′is always positive definite for λ>0, assuming Xends with a column of 1’s.]
Algorithm: Solve for w. Return h(z)=w⊤z.
Increasingλ⇒more regularization ; smaller∥w′∥
Recall [from the previous lecture] our data model y=Xv+e, where eis noise.
Variance of ridge regr. at test pt zis Var( z⊤(X⊤X+λI′)−1X⊤e).
Asλ→∞ , variance→0, but bias increases.
Mean Squared Error
1e−01 1e+01 1e+030 10 20 30 40 50 60
λ
h→0 asλ→∞test error
variance bias2=(E[h]−g)2
ridgebiasvar.pdf (ISL, Figure 6.5) [Plot of bias2& variance as λincreases.]
[The test error as a function of λis a U-shaped curve. We find the bottom by validation. Regularization is
intended to reduce the variance, but this method of regularization also increases the bias.]
λis a hyperparameter; tune by (cross-)validation.
Ideally, features should be “normalized” to have same variance.
Alternative: use asymmetric penalty by replacing I′w/other diagonal matrix. [For example, if you use
polynomial features, you could use di fferent penalties for monomials of di fferent degrees.]
Shrinkage: Ridge Regression, Subset Selection, and Lasso 73
Bayesian Justification for Ridge Reg.
Assign a prior probability on w′:w′∼N(0,ς2), with PDF f(w′)∝e−∥w′∥2/(2ς2)
[This prior probability says that we think weights close to zero are more likely to be correct.]
Apply MLE to maximize the posterior prob.
Bayes’ Theorem: posterior fW|X,Y(w)=fY|X,W(y)f(w′)
fY|X(y)
Maximize log posterior =lnfY|X,W(y)+lnf(w′)−const
=−const∥Xw−y∥2−const∥w′∥2−const
⇒ Minimize∥Xw−y∥2+λ∥w′∥2
[We are treating wandyas random variables, but Xas a fixed constant—it’s not random.]
This method (using MLE, but maximizing posterior) is called maximum a posteriori (MAP).
[A prior probability on the weights is another way to understand regularizing ill-posed problems.]
FEATURE SUBSET SELECTION
[Some of you may have noticed as early as Homework 1 that you can sometimes get better performance on
a spam classifier simply by dropping some useless features.]
All features increase variance, but not all features reduce bias.
Idea: Identify poorly predictive features, ignore them (weight zero).
Less overfitting, smaller test errors.
2nd motivation: Inference. Simpler models convey interpretable wisdom.
Useful in all classification & regression methods.
Sometimes it’s hard: Di fferent features can partly encode same information.
Combinatorially hard to choose best feature subset.
Alg: Best subset selection. Try all 2d−1 nonempty subsets of features. [Train one classifier per subset.]
Choose best classifier by (cross-)validation. Slow.
[Obviously, best subset selection isn’t feasible if we have a lot of features. But it gives us an “ideal”
algorithm to compare practical algorithms with. If dis large, there is no algorithm that’s guaranteed to find
the best subset and that runs in acceptable time. But heuristics often work well.]
Heuristic 1: Forward stepwise selection.
Start with null model (0 features); repeatedly add best feature until validation errors start increasing (due to
overfitting) instead of decreasing. At each outer iteration, inner loop tries every feature & chooses the best
by validation. Requires training O(d2) models instead of O(2d).
Not perfect: e.g., won’t find the best 2-feature model if neither of those
features yields the best 1-feature model. [That’s why it’s a heuristic.]
Heuristic 2: Backward stepwise selection.
Start with all dfeatures; repeatedly remove feature whose removal gives best reduction in validation error.
Also trains O(d2) models.
[Forward stepwise is a better choice when you suspect only a few features will be good predictors; e.g.,
spam. Backward stepwise is better when most features are important. If you’re lucky, you’ll stop early.]
74 Jonathan Richard Shewchuk
LASSO (Robert Tibshirani, 1996)
Least-squares linear regression +ℓ1penalized mean loss. (1) +(A)+(a)+(e).
“Least absolute shrinkage and selection operator.”
[This is a regularized regression method similar to ridge regression, but it has the advantage that it often
naturally sets some of the weights to zero.]
Find wthat minimizes∥Xw−y∥2+λ∥w′∥1where∥w′∥1=Pd
i=1|wi|. (Don’t penalize α.)
Recall ridge regr.: isosurfaces of ∥w′∥2are hyperspheres.
The isosurfaces of ∥w′∥1are cross-polytopes .
The unit cross-polytope is the convex hull of all the positive & negative unit coordinate vectors.
∥w∥1=1
[Draw this figure by hand crosspolys.pdf ]
[You get larger and smaller cross-polytope isosurfaces by scaling these.]
isocontours of∥Xw−y∥2
isocontours of∥w∥2ˆw
isocontours of∥w∥1w2
w1w2
w1isocontours of∥Xw−y∥2ˆw
lassoridge2.pdf [Isocontours of the terms of the objective function for the Lasso appear at
left. Compare with the ridge regression isocontours at right.]
[The red ellipses are the isocontours of ∥Xw−y∥2, and the least-squares solution lies at their center. The
isocontours of∥w′∥1are diamonds centered at the origin (blue). The solution lies where a red isocontour
just touches a blue diamond. What’s interesting is that for large values of λ, the red isocontour touches just
the tip of a diamond. Then the weight w1gets set to zero. That’s what we want to happen to features that
don’t have enough predictive power. For small values of λ, the red isosurface just barely touches a side of a
diamond instead of a tip of the diamond, and no weight gets set to zero.]
[When you go to higher dimensions, you might have several weights set to zero. For example, in 3D, if the
red isosurface just touches a sharp vertex of a cross-polytope, two of the three weights get set to zero. If it
just touches a sharp edge of a cross-polytope, one weight gets set to zero. If it just touches a flat side of a
cross-polytope, no weight is zero.]
Shrinkage: Ridge Regression, Subset Selection, and Lasso 75
Standardized Coefficients
20 50 100 200 500 2000 5000−200 0 100 200 300 400
λ
lassoweights.pdf (ISL, Figure 6.6) [Weights as a function of λ.]
[This shows the weights for a typical linear regression problem with about 10 variables. You can see that as
lambda increases, more and more of the weights become zero. Only four of the weights are really useful for
prediction; they’re in color. Statisticians used to choose λby looking at a chart like this and trying to eyeball
a spot where there aren’t too many predictors and the weights aren’t changing too fast. But nowadays they
prefer validation.]
Sometimes sets some weights to zero, especially for large λ.
Algs: subgradient descent, least-angle regression (LARS), forward stagewise
[Lasso can be reformulated as a quadratic program, but it’s a quadratic program with 2dconstraints, because
ad-dimensional cross-polytope has 2dfacets. In practice, special-purpose optimization methods have been
developed for Lasso. I’m not going to teach you one, but if you need one, look up the last two of these
algorithms. LARS is built into the R Programming Language for statistics.]
[As with ridge regression, you should probably normalize the features first before applying Lasso.]
76 Jonathan Richard Shewchuk
14 Decision Trees
DECISION TREES
Nonlinear method for classification and regression.
Uses tree with 2 node types:
– internal nodes test feature values (usually just one) & branch accordingly
– leaf nodes specify class h(x)
check
x3
x1100
75
25
050
overcastx2
sunny rainOutlook ( x1)
Humidity ( x2) Wind ( x3)overcast sunny rain
yes
yes yes no no>75% >20≤20 ≤75%no
yesyes
[Draw this by hand. dectree.pdf Deciding whether to go out for a picnic.]
– Cuts x-space into rectangular cells
– Works well with both categorical and quantitative features
– Interpretable result (inference)
– Decision boundary can be arbitrarily complicated
linear classifer
decision tree
treelinearcompare2.pdf (redrawing of ISL, Figure 8.7) [Comparison of linear classifiers
vs. decision trees on 2 examples.]
Decision Trees 77
Consider classification first. Greedy, top-down learning heuristic:
[This algorithm is more or less obvious, and has been rediscovered many times. It’s naturally recursive. I’ll
show how it works for classification first; later I’ll talk about how it works for regression.]
LetXben×ddesign matrix; y∈Rnbe labels.
LetS⊆{1,2,..., n}be set of sample point indices.
Top-level call: S={1,2,..., n}.
GrowTree( S)
if (yi=Cfor all i∈Sand some class C) then {
return new leaf( C) [We say the leaves are pure ]
}else{
choose best splitting feature jand splitting value β(*)
Sl={i∈S:Xi j<β} [Or you could use ≤and>]
Sr={i∈S:Xi j≥β}
return new node( j,β, GrowTree( Sl), GrowTree( Sr))
}
(*) How to choose best split?
– Try all splits. [All features, and all splits within a feature.]
– For a set S, letJ(S) be the cost ofS.
– Choose the split that minimizes J(Sl)+J(Sr); or,
the split that minimizes weighted average|Sl|J(Sl)+|Sr|J(Sr)
|Sl|+|Sr|.
[Here, I’m using the vertical bars |·|to denote set cardinality.]
How to choose cost J(S)?
[I’m going to start by suggesting a mediocre cost function, so you can see why it’s mediocre.]
Idea 1 (bad): Label Swith the class C that labels the most points in S.
J(S)←# of points in Snot in class C.
J(S)=10
5 D 5 D
J(Sl)=5 J(Sr)=5x2
10 D 0 D
J(Sr)=0 J(Sl)=10x120 C 10 D
10 C 10 C20 C 10 D
10 C 10 C
[Draw this by hand. badcost.pdf ]
Problem: J(Sl)+J(Sr)=10 for both splits, but left split is much better. Weighted avg prefers right split!
[There are many di fferent splits that all have the same total cost. We want a cost function that better distin-
guishes between them.]
78 Jonathan Richard Shewchuk
Idea 2 (good): Measure the entropy . [An idea from information theory.]
LetYbe a random class variable, and suppose P(Y=C)=pC.
The surprise ofYbeing class C is−log2pC. [Always nonnegative.]
– event w /prob. 1 gives us zero surprise.
– event w /prob. 0 gives us infinite surprise!
[In information theory, the surprise is equal to the expected number of bits of information we need to
transmit which events happened to a recipient who knows the probabilities of the events. Often this means
using fractional bits, which may sound crazy, but it makes sense when you’re compiling lots of events into
a single message; e.g., a sequence of biased coin flips.]
The entropy of an index set Sis the average surprise [when you draw a point at random from S],
H(S)=−X
CpClog2pC,where pC=|{i∈S:yi=C}|
|S|.[The proportion of points in S
that are in class C.]
If all points in Sbelong to same class? H(S)=−1 log21=0.
Half class C, half class D? H(S)=−0.5 log20.5−0.5 log20.5=1.
npoints, all di fferent classes? H(S)=−log21
n=log2n.
[The entropy is the expected number of bits of information we need to transmit to identify the class of a
sample point in Schosen uniformly at random. It makes sense that it takes 1 bit to specify C or D when
each class is equally likely. And it makes sense that it takes log2nbits to specify one of nclasses when each
class is equally likely.]
0.0 0.2 0.4 0.6 0.8 1.0pC0.20.40.60.81.0H
entropy.pdf [Left: plot of the entropy H(pC) when there are only two classes. The proba-
bility of the second class is pD=1−pC, so we can plot the entropy with just one dependent
variable. Right: plot of the entropy H(pC,pD) when there are three classes. The probability
of the third class is pE=1−pC−pD. Observe that the entropy is strictly concave.]
Decision Trees 79
Weighted avg entropy after split is Hafter=|Sl|H(Sl)+|Sr|H(Sr)
|Sl|+|Sr|.
Choose split that maximizes information gain H(S)−Hafter. [Which is just the same as minimizing Hafter.]
10 C 1 DH(S)=−20
30lg20
30−10
30lg10
300.918
H(Sl)=−10
19lg10
19−9
19lg9
190.998 H(Sr)=−10
11lg10
11−1
11lg1
110.439x320 C 10 D
10 C 9 D
Hafter=0.793 info gain =0.125
[Draw this by hand. infogain.pdf ]
Info gain always positive except it is zero when one child is empty or
for all C, P(yi=C|i∈Sl)=P(yi=C|i∈Sr). [Which is the case for the second split we considered.]
[Recall the graph of the entropy.]
}
0.4 0.2 0.6 0.800.51
1 1pC pC50%
0%H(Sr)entropy: strictly concave H(pC) J(pC)=% misclassified: concave, not strict
H(Sl)
J(Sr)J(Sl)H(S)
J(S)=Jafter
0 0.4 0.2 0.6 0.8Hafterinfo gain
0
[Draw this by hand on entropy.pdf. concave.pdf ]
[Suppose we pick two points on the entropy curve, then draw a line segment connecting them. Because the
entropy curve is strictly concave, the interior of the line segment is strictly below the curve. Any point on
that segment represents a weighted average of the two entropies for suitable weights. If you unite the two
sets into one parent set, the parent set’s value pCis the weighted average of the children’s pC’s. Therefore,
the point directly above that point on the curve represents the parent’s entropy. The information gain is
the vertical distance between them. So the information gain is positive unless the two child sets both have
exactly the same pCand lie at the same point on the curve.]
[On the other hand, for the graph on the right, plotting the % misclassified, if we draw a line segment
connecting two points on the curve, the segment might lie entirely on the curve. In that case, uniting the two
child sets into one, or splitting the parent set into two, changes neither the total misclassified sample points
nor the weighted average of the % misclassified. The bigger problem, though, is that many di fferent splits
will get the same weighted average cost; this test doesn’t distinguish the quality of di fferent splits well.]
80 Jonathan Richard Shewchuk
[By the way, the entropy is not the only function that works well. Many concave functions work fine,
including the simple polynomial p(1−p).]
More on choosing a split:
– For binary feature xi: children are xi=0 &xi=1.
– If xihas 3+discrete values: split depends on application.
[Sometimes it makes sense to use multiway splits; sometimes binary splits.]
– If xiis quantitative: sort xivalues in S; try splitting between each pair of unequal consecutive values.
[We can radix sort the points in linear time, and if nis huge we should.]
Clever bit: As you scan sorted list from left to right, you can update entropy in O(1) time per point!6
[This is important for obtaining a fast tree-building time.]
[Draw a row of C’s and X’s; show how we update the # of C’s and # of X’s in each of SlandSras we
scan from left to right.]
C X X C X
1X 2X 1X0C 2C
2X1C 1C
2X 1X 3X 0X1C 1C 1C 1C
xi
scan.pdf
Algs & running times:
– Classify test point: Walk down tree until leaf. Return its label.
Worst-case time is O(tree depth).
For binary features, that’s ≤d. [Quantitative features may go deeper.]
Usually (not always) ≤O(logn).
– Training: For binary features, try O(d) splits at each node.
For quantitative features, try O(n′d) splits; n′=points in node
Either way ⇒O(n′d) time at this node
[Training on quantitative features is asymptotically just as fast as training on binary features because
of our clever way of computing the entropy for each split.]
Each point participates in O(depth) nodes, costs O(d) time in each node.
[This is an amortized analysis: we are charging O(ddepth) time to each sample point.]
Running time≤O(nddepth).
[Asndis the size of the design matrix X, and the depth is often logarithmic, this is a surprisingly
reasonable running time.]
6LetCbe the number of class C sample points to the left of a potential split and cbe the number to the right of the split. Let
Dbe the number of class not-C points to the left of the split and dbe the number to the right of the split. Update C,c,D, and d
at each split (in O(1) time per split) as you move from left to right. At each potential split, calculate the entropy of the left set as
−C
C+Dlog2C
C+D−D
C+Dlog2D
C+Dand the entropy of the right set as −c
c+dlog2c
c+d−d
c+dlog2d
c+d. Note: log 0 is undefined, but this
formula works if we use the convention 0 log 0 =0.
It follows that the weighted average of the two entropies is −1
n′
Clog2C
C+D+Dlog2D
C+D+clog2c
c+d+dlog2d
c+d
, where n′=
C+D+c+dis the total number of sample points stored in this treenode. Choose the split that minimizes this weighted average.
More Decision Trees, Ensemble Learning, and Random Forests 81
15 More Decision Trees, Ensemble Learning, and Random Forests
DECISION TREE V ARIATIONS
[Last lecture, I taught you the vanilla algorithm for building decision trees and using them to classify test
points. There are many variations on this basic algorithm; I’ll discuss a few now.]
Decision Tree Regression
Creates a piecewise constant regression fn. [This seems too rudimentary to be true, but it’s true.]
x1x2
x1h
x2x2≥2x2<2x2≥2
x2<2x1≥3 x1<3
x1≥1x2<3x2≥3
x1<1
treeregresstree.pdf [Decision tree regression.]
Leaf stores label µS=1
|S|X
i∈Syi, the mean label for training pts i∈S.
Cost J(S)=Var({yi:i∈S})=1
|S|X
i∈S(yi−µS)2.
[So if all the points in a leaf have the same y-value, then the cost is zero.]
[We choose the split that minimizes the weighted average of the variances of the two children after the split.]
Stopping Early
[The basic version of the decision tree algorithm keeps subdividing treenodes until every leaf is pure. We
don’t have to do that; sometimes we prefer to stop subdividing treenodes earlier.]
Why?
– Limit tree depth (for speed)
– Limit tree size (big data sets)
– Pure tree may overfit
– Given noise or overlapping distributions, pure leaves tend to overfit; better to stop early and estimate
posterior probs
82 Jonathan Richard Shewchuk
[When you have strongly overlapping class distributions, refining the tree down to one training point per leaf
is absolutely guaranteed to overfit, giving you a classifier akin to the 1-nearest neighbor classifier. It’s better
to stop early, then classify each leaf node by taking a vote of its training points; this gives you a classifier
akin to a k-nearest neighbor classifier.]
treeoverfit.pdf [Overlapping distributions cause pure decision trees to overfit. Compare
this decision tree with the Bayes decision rule; the Bayes optimal decision boundary would
be just one point.]
[Alternatively, you can use the points to estimate a posterior probability for each leaf, and return that. If
there are many points in each leaf, the posterior probabilities might be reasonably accurate.]
0
106
10
2
102
10x2
x1p(Y|X)13
20
4
20 2
201
20
p(Y|X)
leaf2.pdf [In the decision tree at left, each leaf has multiple classes. Instead of returning
the majority class, each leaf could return a posterior probability histogram.]
Leaves with multiple points return
– a majority vote or class posterior probs (classification) or
– an average (regression).
How to stop? Select stopping condition(s):
– Next split doesn’t reduce entropy /error enough (dangerous; pruning is better)
– Most of node’s points (e.g., >90%) have same class [to deal with outliers & overlapping distribs]
– Node contains few training points (e.g., <10) [especially for big data]
– Box’s edges are all tiny [super-fine resolution may be pointless]
– Depth too great [risky if there are still many training points in the box]
– Use validation to compare
[The last is the slowest but most e ffective way to know when to stop: use validation to decide whether
splitting the node lowers your validation error. But if your goal is to avoid overfitting, it’s generally even
more e ffective to grow the tree a little too large and then use validation to prune it back . . . ]
More Decision Trees, Ensemble Learning, and Random Forests 83
Pruning
Grow tree too large; greedily remove each split whose removal improves validation performance.
[We have to do validation once for each split that we’re considering reversing.]
More reliable than stopping early.
[The reason why pruning often works better than stopping early is because often a split that doesn’t seem to
make much progress is followed by a split that makes a lot of progress. If you stop early, you’ll never find
out. Pruning is simple and highly recommended when you have enough time to build and prune the tree.]
Y earsHits
1117.5238
1 4 . 5 2 4 R1R3
R2
2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0
Tree SizeMean Squared ErrorTraining
Cross −Validation
Test
prunedhitters.pdf, prunehitters.pdf (ISL, Figures 8.5 & 8.2) [At left, a decision tree pre-
dicting the salaries of baseball players from years in Major League Baseball and hitting
average: R1 =$165,174, R2 =$402,834, R3 =$845,346. At right, a plot of decision tree
leaf nodes vs. mean squared error. The graph shows that the decision tree with the best
validation accuracy has three leaves, so that tree appears at left.]
[In this example, a 10-leaf decision tree was constructed to predict the salaries of baseball players, based
on their years in the league and average hits per season. Then the tree was pruned by validation. The best
decision tree on the validation data turned out to have just three leaves.]
[Observe that this tree is very interpretable . You could easily explain it to grandpa.]
[It might seem expensive to do validation once for each split we consider reversing. But you can do it pretty
cheaply. What you don’t do is reclassify every validation point from scratch. Instead, you first compute
which leaf each validation point winds up in, then for each leaf you make a list of its validation points.
When you are deciding whether to remove a split, you just look at the validation points in the two leaves
you’re thinking of removing, and see how they will be reclassified and how that will change the error rate.
You can compute this very quickly.]
X
X
X
XCX
XX
X
CC
CCC
C
CX
X
X
XX
X
XX
X
CC
CCC
CX
validation pointspruningprunecheck.pdf [After we determine
which leaf boxes each validation point
ends up in, we find that pruning these two
leaves improves the validation accuracy.
Box colors indicate the majority classes
of the training points (not shown). Local
validation accuracy improves from 7 /16
to 9/16.]
84 Jonathan Richard Shewchuk
Multivariate Splits
Find non-axis-aligned splits with other classification algs or by generating them randomly.
x2<7 x2<3
x1<10x2<8
x1<5x2≥7
x2≥5 x2<5x1≥13
x2≥8
x1≥10x2<6x2≥6
x1<8x1≥8
x2≥3x1<13
x1≥5
multivar.pdf [An example where an ordinary decision tree needs many splits to approxi-
mate a diagonal linear decision boundary, but a single multivariate split takes care of it.]
[Here you can use other classification algorithms such as SVMs, logistic regression, and Gaussian discrimi-
nant analysis. Decision trees permit these algorithms to find nonlinear decision boundaries by making them
hierarchical.]
May gain better classifier at cost of worse interpretability or speed.
[Standard decision trees are very fast because they check only one feature at each treenode. But if there are
hundreds of features, and you have to check all of them at every level of the tree to classify a point, it slows
down classification a lot.]
[A good compromise is to set a limit on the number of features you check at each treenode—say, three. You
can use forward stepwise selection at each treenode to choose the three features.]
[On exams, assume we check only one feature per treenode unless we say otherwise!]
ENSEMBLE LEARNING
Decision trees are fast, simple, interpretable, easy to explain,
invariant under scaling /translation, robust to irrelevant features.
But not the best at prediction. [Compared to previous methods we’ve seen.]
High variance. [Though we can achieve very low bias.]
[For example, suppose we take a training data set, split it into two halves, and train two decision trees, one
on each half of the data. It’s not uncommon for the two trees to turn out very di fferent. In particular, if the
two trees pick di fferent features for the very first split at the root of the tree, then it’s quite common for the
trees to be completely di fferent. So decision trees tend to have high variance.]
[So let’s think about how to fix this. As an analogy, imagine that you are generating random numbers
from some distribution. If you have just one random number, its variance might be high. But if you have
nrandom numbers and take their average, then the variance of that average is ntimes smaller. So you might
ask yourself, can we reduce the variance of decision trees by taking an average answer of a bunch of decision
trees? Yes we can.]
More Decision Trees, Ensemble Learning, and Random Forests 85
wisdom.jpg, penelope.jpg [James Surowiecki’s book “The Wisdom of Crowds” and Pene-
lope the cow. Surowiecki tells us this story . . . ]
[A 1906 county fair in Plymouth, England had a contest to guess the weight of an ox. A scientist named
Francis Galton was there, and he did an experiment. He calculated the median of everyone’s guesses. The
median guess was 1,207 pounds, and the true weight was 1,198 pounds, so the error was less than 1%. Even
the cattle experts present didn’t estimate it that accurately.]
[National Public Radio repeated the experiment in 2015 with a cow named Penelope whose photo they
published online. They got 17,000 guesses, and the average guess was 1,287 pounds. Penelope’s actual
weight was 1,355 pounds, so the crowd got it to within 5 percent.]
[The main idea is that sometimes the average opinion of a bunch of idiots is better than the opinion of one
expert. And so it is with learning algorithms. We call a learning algorithm a weak learner if it does better
than guessing randomly. And we combine a bunch of weak learners to get a strong one.]
We can take average of output of
– different learning algs
– same learning alg on many training sets [if we have tons of data]
– bagging : same learning alg on many random subsamples of one training set
– random forests : randomized decision trees on random subsamples
[These last two are the most common ways to use averaging, because usually we don’t have enough training
data to use fresh data for every learner.]
Metalearner takes test point, feeds it into all Tlearners, returns majority class or average output.
[Averaging is not specific to decision trees; it can work with many di fferent learning algorithms. But it
works particularly well with decision trees.]
Regression algs: take median or mean output [of all the weak learners]
Classification algs: take majority vote OR average posterior probs
[Apology to readers: I show some videos in this lecture, which cannot be included in this report.]
[Show averageaxis.mov] [Here’s a simple classifier that takes an average of “stumps,” trees of depth 1.
Observe how good the posterior probabilities look.]
[Show averageaxistree.mov] [Here’s a 4-class classifier with depth-2 trees.]
86 Jonathan Richard Shewchuk
[The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user
ratings for films, based on previous ratings. It ran for three years and ended in 2009 with a $1,000,000 prize.
The winning team, BellKor’s Pragmatic Chaos, used an extreme ensemble method that took an average of
many di fferent learning algorithms. A couple of top teams combined into one team so they could combine
their methods. They said, “Let’s average our models and split the money,” and that’s what happened.]
Use learners with low bias (e.g., deep decision trees).
High variance & some overfitting are okay. Averaging reduces the variance!
[Each learner may overfit, but each overfits in its own unique way.]
Averaging sometimes reduces bias & increases flexibility a bit, but not reliably.
e.g., creating nonlinear decision boundary from linear classifiers.
[Averaging rarely reduces bias as much as it reduces variance, so get the bias small before averaging.]
Hyperparameter settings usually di fferent than 1 learner. [Averaging reduces variance more than bias.]
[Sometimes the number of learners is said to be a hyperparameter, but extra learners improve the variance
without worsening the bias. The main limit on the number of learners is computation time. So you trade o ff
time for improved variance.]
Bagging =Bootstrap AGGregatING (Leo Breiman, 1994)
[Leo Breiman was a statistics professor right here at Berkeley. He did his best work after he retired in 1993.
The bagging algorithm was published the following year, and then he went on to co-invent random forests
as well. Unfortunately, he died in 2005.]
leobreiman3.png [Leo Breiman]
[Bagging is a randomized method for creating many di fferent learners from the same data set. It works well
with many di fferent learning algorithms. One exception seems to be k-nearest neighbors; bagging mildly
degrades it.]
Given n-point training sample, generate random subsample of size n′by sampling with replacement . Some
points chosen multiple times; some not chosen.
More Decision Trees, Ensemble Learning, and Random Forests 87
1 3 4 6 8 9
↙ ↘
6 3 6 1 1 9 8 8 4 9 1 8
Ifn′=n,∼63.2% are chosen. [On average; this fraction varies randomly.]
Build learner. Points chosen jtimes have greater weight:
[If a point is chosen jtimes, we want to treat it the same way we would treat jdifferent points all bunched
up infinitesimally close together.]
– Decision trees: j-time point has j×weight in entropy.
– SVMs: j-time point incurs j×penalty to violate margin.
– Regression: j-time point incurs j×loss.
Build Tlearners from Tsubsamples.
Random Forests
Bagging +trees isn’t random enough!
[With bagging, often the decision trees look very similar. Why is that?]
One really strong predictor →same feature split at top of every tree.
[For example, if you’re building decision trees to identify spam, the first split might always be “viagra.”
Random sampling might not change that. If the trees are too similar, then taking their average doesn’t
reduce the variance much.]
Let’s reduce the correlation between di fferent trees. [That makes averaging work better.]
Idea: At each treenode, take random sample of mfeatures (out of d).
Choose best split from mfeatures.
[We’re not allowed to split on the other d−mfeatures!]
Different random sample for each treenode.
m≈√
dworks well for classification; m≈d/3 for regression.
[So if you have a 100-dimensional feature space, you randomly choose 10 features and pick the one
of those 10 that gives the best split. But mis a hyperparameter, and you might get better results by
tuning it for your particular application. These values of mare good starting guesses.]
Smaller m→more randomness, less tree correlation, more bias
[One reason this works is if there’s a really strong predictor, only a fraction of the trees can choose that pre-
dictor as the first split. That fraction is m/d. So the split tends to “decorrelate” the trees. And that means
that when you take the average of the trees, your average will have less variance than a single tree.]
[You have to be careful, though, because you don’t want to dumb down the trees too much in your quest for
decorrelation. Averaging works best when you have very strong learners that are also diverse. But it’s hard
to create a lot of learners that are very di fferent yet all very smart. The Netflix Prize winners did it, but it
was a huge amount of work.]
Sometimes test error drops even at 100s or 1,000s of decision trees!
Disadvantages: slow; loses interpretability /inference.
[But the compensation is it’s a more accurate predictor than a single decision tree.]
[I will end by showing you examples of a very non-standard method for random forests that works magic in
certain di fficult circumstances.]
Idea: generate srandom multivariate splits (oblique lines, quadrics); choose best split.
[You have to be clever about how you generate random decision boundaries; I’m not going to discuss that.
I’ll just show lots of examples.]
88 Jonathan Richard Shewchuk
[Show treesidesdeep.mov] [Lots of good-enough conic random decision trees.]
[Show averageline.mov]
[Show averageconic.mov]
[Show square.mov] [Depth 2; look how good the posterior probabilities look.]
[Show squaresmall.mov] [Depth 2; see the uncertainty away from the center.]
[Show spiral2.mov] [Doesn’t look like a decision tree at all, does it?]
[Show overlapdepth14.mov] [Overlapping classes. This example overfits!]
[Show overlapdepth5.mov] [Better fit.]
500.pdf [Random forest classifiers for 4-class spiral data. Each forest takes the average of
400 trees. The top row uses trees of depth 4. The bottom row uses trees of depth 12. From
left to right, we have axis-aligned splits, splits with lines with arbitrary rotations, and splits
with conic sections. Each split is chosen to be the best of 500 random choices.]
randomness.pdf [Random forest classifiers for the same data. Each forest takes the average
of 400 trees. In these examples, allthe splits are axis-aligned. The top row uses trees of
depth 4. The bottom row uses trees of depth 12. From left to right, we choose each split from
1, 5, or 50 random choices. The more choices, the less bias and the better the classifier.]
Neural Networks 89
16 Neural Networks
NEURAL NETWORKS
Can do both classification & regression.
[They tie together many ideas from the course: perceptrons, linear regression, logistic regression, ensembles
of learners, and stochastic gradient descent. They also tie in the idea of lifting sample points to a higher-
dimensional feature space, but with a new twist: neural nets can learn features themselves.]
[I want to begin by reminding you of the story I told you at the beginning of the semester, about Frank
Rosenblatt’s invention of perceptrons in 1957. Remember that he held a press conference where he predicted
that perceptrons would be “the embryo of an electronic computer that [the Navy] expects will be able to
walk, talk, see, write, reproduce itself and be conscious of its existence.”]
[Perceptron research continued until something unfortunate happened in 1969. Marvin Minsky, one of the
founding fathers of AI, and Seymour Papert published a book called “Perceptrons.” Sounds promising?
Well, part of the book was devoted to things perceptrons can’t do. One of those things is XOR.]
0 1
0
01
1x2x1
XOR
0
1
[Think of the four outputs here as training points in two-dimensional space. Two of them are in class 1, and
two of them are in class 0. We want to find a linear classifier that separates the 1’s from the 0’s. Can we do
it? No.]
[The XOR problem is also called parity , especially when you have more features: the input is a bunch of
bits and you answer whether the number of 1’s is even or odd. It was known even then that you could solve
parity problems by adding extra layers of perceptrons, but Minsky and Papert gave technical proofs about
some circumstances where this can’t be done, and those limitations were misinterpreted. The book had a
devastating e ffect on the field. After its publication, almost no research was done on neural net-like ideas for
a decade, a time we now call the first “AI Winter.” Shortly after the book was published, Frank Rosenblatt
died in a boating accident.]
[There are several almost obvious ways to get around the XOR problem. Here’s the easiest.]
If you add one new quadratic feature, x1x2, XOR is linearly separable in 3D.
0 1
1
0 [Draw this by hand. xorcube.pdf ]
[Now we can find a plane that cuts through the cube obliquely and separates the 0’s from the 1’s.]
90 Jonathan Richard Shewchuk
[However, there’s an even more powerful way to do XOR. The idea is to design linear classifiers whose
output is the input to other linear classifiers. That way, you should be able to emulate arbitrary logic circuits.
Suppose I put together some linear decision functions like this.]
zlinear combo
linear combolinear combox1
x2 [Draw this by hand. lincombo.pdf ]
[Interpret the output as true if zis greater than one-half or false if zis less than one-half. Can I do XOR with
this?]
A linear combo of linear combos is a linear combo . . . only works for linearly separable points.
[We need one more idea to make neural nets. We need to add some sort of nonlinearity between the linear
combinations. Let’s call these boxes that compute linear combinations “neurons.” If a neuron sends the
linear combination it computes through some nonlinear function before sending it on to other neurons, then
the neurons can act somewhat like logic gates. The nonlinearity could be as simple as clamping the output
so it can’t go below zero. That’s what people usually use in practice these days.]
[However, the traditional choice was to use the logistic function. The logistic function can’t go below zero
or above one, which is nice because it can’t ever get huge and oversaturate the other neurons it’s sending
information to. The logistic function is also smooth, which means it has well-defined gradients and Hessians
we can use for optimization. And we know that the logistic is often a good model for posterior probabilities.]
[With logistic functions between the linear combinations, here’s a two-layer perceptron that computes the
XOR function.]
s(20a+20b−30)
ORANDx1
x2x1⊕x2s(30−20x1−20x2)
s(20x1+20x2−10)a
bNAND
[Draw this by hand. xorgates.pdf ]
[The big question is: can an algorithm learn a function like this?]
Neural Networks 91
Network with 1 Hidden Layer
Input layer: x1,..., xd;xd+1=1 [Index d+1 is the fictitious dimension.]
Hidden units: h1,..., hm;hm+1=1
Output layer: ˆ y1,..., ˆyk
Layer 1 weights: m×(d+1) matrix V V⊤
iis row i: weights into hi
Layer 2 weights: k×(m+1) matrix W W⊤
iis row i: weights into ˆ yi
1
1W24
V33x2h2
h3W12 ˆy1V21 x1V11
ˆy2h1
[Draw this by hand. neuralnetwork.pdf ]
Recall [logistic function] s(γ)=1
1+e−γ. Other nonlinear fns can be used, called the activation fns .
For vector u,s(u)=s(u1)
s(u2)
...,s1(u)=s(u1)
s(u2)
...
1[We apply sto a vector component-wise.]
h=s1(V x) . . . that is, hi=s(Vi·x)
ˆy=s(Wh)=s(Ws1(V x))
[Neural networks often have more than one output. This allows us to build multiple classifiers that share
hidden units. One of the interesting advantages of neural nets is that if you train multiple classifiers simul-
taneously, sometimes some of them come out better because they can take advantage of particularly useful
hidden units that first emerged to support one of the other classifiers.]
[We can add more hidden layers, and for image recognition tasks it’s common to have 6 to 200 hidden
layers. There are many variations you can experiment with—for instance, you can have connections that go
forward more than one layer.]
92 Jonathan Richard Shewchuk
Training
Usually stochastic or batch gradient descent.
Pick loss fn L(ˆy,y) e.g., L(ˆy,y)=∥ˆy−y∥2.
↑↑
predictions true labels (could be vectors)
Find VandWthat minimize the cost fn J(V,W)=1
nnX
i=1L(ˆy(Xi),Yi).
[I’m using a capital Yhere because Yis a matrix with one row for each training point and one column for
each output unit of the neural net. Each training point has a whole vector of labels Yi, stored as a row of Y.]
Usually there are many local minima!
[The cost function for a neural net is, generally, not even close to convex. Sometimes, it’s possible to wind
up in a bad minimum. Usually, you can avoid bad minima by having lots of units in each layer.]
[Now let me ask you this. Suppose we start by setting all the weights to zero, and then we do gradient
descent on the weights. What will go wrong?]
[This neural network has a symmetry: there’s really no di fference between one hidden unit and any other
hidden unit. The gradient descent algorithm has no way to break the symmetry between hidden units. You
can get stuck in a situation where all the weights out of an input unit have the same value, and all the weights
into an output unit have the same value, and they have no way to become di fferent from each other. To avoid
this problem, and in the hopes of finding a better local minimum, we start with random weights.]
Letwbe a vector containing all the weights in V&W. Batch gradient descent:
w←vector of random weights
repeat
w←w−ϵ∇J(w)
[We’ve just rewritten all the weights as a vector for notational convenience. When you actually write the
code, for the sake of speed, you should probably operate directly on the weight matrices VandW.]
[It’s important to make sure the random weights aren’t too big, because if a unit’s output gets too close to
zero or one, it can get “stuck,” meaning that a modest change in the input values causes barely any change
in the output value. Stuck units tend to stay stuck. I’ll say more about that next lecture.]
[Instead of batch gradient descent, we can use stochastic gradient descent, which means we use the gradient
of one training point’s loss function at each step. Typically, we shu ffle the points in a random order, or just
pick one randomly at each step. I’ll say more about that next week.]
[The hard part of this algorithm is computing the gradient. If you simply derive one derivative for each
weight, you’ll find that for a network with many layers of hidden units, it takes time linear in the number of
edges in the neural network to compute a derivative for one weight. Multiply that by the number of weights.
We’ll spend the rest of this lecture learning to improve the running time to linear in the number of edges.]
Naive gradient computation: O(edges2) time
Backpropagation: O(edges) time
Neural Networks 93
Computing Gradients for Arithmetic Expressions
[Let’s see what it takes to compute the gradient of an arithmetic expression. It turns into repeated applica-
tions of the chain rule from calculus.]
2ed
cf e ba
1
1
dc
∂f
∂z=∂f
∂n∂n
∂z∂f
∂a
∂f
∂f=1
=d∂f
∂e=∂f
∂e∂e
∂c=∂f
∂d∂d
∂a
=∂f
∂d
∂f
∂d
∂f
∂e=∂f
∂f∂f
∂e=c∂f
∂e
=∂f
∂d=∂f
∂d∂d
∂b
∂f
∂c∂f
∂b
∂e
∂c=d∂e
∂d=cf=e2
∂d
∂a=1∂d
∂b=1d=a+b e =cd∂f
∂e=2e=2e∂f
∂f=∂f
∂e∂e
∂d
computed during forward pass+
“backpropagation”where zis an input to n.Goal: compute∇f=∂f
∂a
∂f
∂b
∂f
∂c
e2
Each value zgives partial derivative of the form×
computed during backward pass after forward pass
[Draw this by hand. gradientsarith.pdf Draw the black diagram first. Then the goal (upper
right). Then the green and red expressions, from left to right, leaving out the green arrows.
Then the green arrows, starting at the right side of the page and moving left. Lastly, write
the text at the bottom. (Use the same procedure for the next two figures.)]
94 Jonathan Richard Shewchuk
[What if a unit’s output goes to more than one unit? Then we need to understand a more complicated version
of the chain rule. This is a standard rule of multivariate calculus:]
∂
∂αL(y1(α),y2(α))=∂L
∂y1∂y1
∂α+∂L
∂y2∂y2
∂α=∇yL·∂
∂αy
[With this rule, let’s compute gradients for an expression from least-squares linear regression.]
w1
w2
α+X21∂L
∂ˆy2
+X22∂L
∂ˆy2
=∂L
∂ˆy1+∂L
∂ˆy2∂L
∂α+∂L
∂ˆy2∂ˆy2
∂α=∂L
∂ˆy1∂ˆy1
∂α∂L
∂w1
∂L
∂w2∂L
∂ˆy1=2(ˆy1−y1)
∂L
∂ˆy2=2(ˆy2−y2)=∂L
∂ˆy1∂ˆy1
∂w1+∂L
∂ˆy2∂ˆy2
∂w1
=∂L
∂ˆy1∂ˆy1
∂w2+∂L
∂ˆy2∂ˆy2
∂w2=X11∂L
∂ˆy1
=X12∂L
∂ˆy1
X21w1+X22w2+αLossˆy1
ˆy2X11w1+X12w2+α
∥ˆy−y∥2
[Draw this by hand. gradientsmulti.pdf ]
[Observe that we’re doing dynamic programming here. We’re computing the solutions of subproblems, then
using each solution to compute the solutions of several bigger problems.]
[In one sense, all we’ve done here is to rederive the fact that the gradient of the least-squares regression
cost function is∇wL=2X⊤(ˆy−y), where ˆ y=Xw. But the way we’ve divided it into a forward pass and a
backward pass gives us a way to generalize it by adding more layers of computations.]
Neural Networks 95
The Backpropagation Alg.
[Backpropagation is a dynamic programming algorithm for computing the gradients we need to do neural
net stochastic gradient descent in time linear in the number of weights.]
Recall s′(γ)=s(γ) (1−s(γ)); V⊤
iis row iof weight matrix V[and likewise for rows of W]
hi=s(Vi·x), so∇Vihi=s′(Vi·x)x=hi(1−hi)x
ˆyj=s(Wj·h), so∇Wjˆyj=s′(Wj·h)h=ˆyj(1−ˆyj)h
∇hˆyj =ˆyj(1−ˆyj)Wj
[Here is the arithmetic expression for the same neural network I drew for you three illustrations ago. It looks
very di fferent when you depict it like this, but don’t be fooled; it’s exactly the same network I started with.
But now we treat the weights VandWas the inputs, rather than the point x.]
VW
h=∂L
∂ˆyj∇Wjˆyj
=∂L
∂hihi(1−hi)x=Pk
j=1∂L
∂ˆyj∇hˆyj
=P
j∂L
∂ˆyjˆyj(1−ˆyj)Wj=∂L
∂hi∇Vihi=∂L
∂ˆyjˆyj(1−ˆyj)h
Compute∇VL,∇WLone row at a time.ˆy
∇ˆyL=2(ˆy−y)
s(V x)∇WjL
∇ViL∥ˆy−y∥2
∇hLL
s(Wh)
[Draw this by hand. backalg.pdf ]
[Note that hand ˆyare computed during the forward pass, and ∇ˆyL,∇hL,∇WL, and∇VLare computed during
the backward pass. In particular, we can’t compute ∇VLuntil after we compute ∇hL, and we can’t compute
that until after we compute ∇ˆyL. The loss Ldoesn’t need to be explicitly computed at all! We can compute
all the gradients without it.]
96 Jonathan Richard Shewchuk
17 Vanishing Gradients; ReLUs; Output Units and Losses; Neurobiology
THE V ANISHING GRADIENT PROBLEM; ReLUs
[Last lecture, we put a logistic function at the output of every unit except the input units. These units are
called sigmoid units . But in practice, sigmoid units are usually a poor choice for hidden layers.]
Problem: When unit output sis close to 0 or 1 for most training points, s′=s(1−s)≈0, so gradient descent
changes svery slowly. Unit is “stuck.” Slow training.
!! !" # " !!#$"#$!#$%#$&'$#""!#
flat spot}flat spotmaximum curvature{
linear region
{
logistic.pdf [Draw flat spots, “linear” region, & maximum curvature points (at s(λ)0.21
ands(λ)0.79) of the sigmoid function. Ideally, we would stay away from the flat spots.]
[This is called the vanishing gradient problem . The more layers your network has, the more problematic
this problem becomes. Most of the early attempts to train deep, many-layered neural nets failed.]
Solution: Replace sigmoids with ReLUs: rectified linear units .
ramp fn :r(γ)=max{0,γ}.
r′(γ)=(1, γ≥0,
0, γ< 0.
γr(γ)
[The derivative is not defined at zero, but we just pretend it is for the sake of gradient descent.]
Most neural networks today use ReLUs for the hidden units.
[However, it is still common to use sigmoids for the output units in classification problems.]
[ReLUs are preferred over sigmoids as hidden units because in practice, they’re much less likely to get stuck.
But the derivative r′is sometimes zero, so you might wonder if ReLUs can get stuck too. Fortunately, it’s
rare for a ReLU’s gradient to be zero for allthe training data; it’s usually zero for just some training points.
But yes, ReLUs sometimes get stuck too; just not as often as sigmoids.]
[The output of a ReLU can be arbitrarily large; the fact that ReLUs don’t saturate like sigmoids do leaves
them vulnerable to a related problem called the “exploding gradient problem.” It is not a big problem in
shallow networks, but it becomes a big problem in deep or recurrent networks.]
[Even though ReLUs are linear in each half of their range, they’re still nonlinear enough to easily compute
functions like XOR. Of course, if you replace sigmoids with ReLUs, you have to change the derivation of
backprop to reflect the changes in the gradients. We’ll do that later in this lecture.]
Vanishing Gradients; ReLUs; Output Units and Losses; Neurobiology 97
OUTPUT UNITS
[Many neural networks use ReLUs for all or most of the hidden units, but ReLUs are rarely used as output
units. The output units are chosen to fit the application, and there are three common choices.]
Most output units are linear units (regression) or sigmoid /logistic or softmax units (classification).
[When you train a neural network with these output units by gradient descent, the last layer of edges of
the network is solving a problem in linear regression, logistic regression, or softmax regression by gradient
descent. Or maybe all three!]
(1) Linear units for regression.
Given vector hof unit values in last hidden layer, output layer computes ˆ y=Wh.
Activation fn is the identity fn. [You could say there is no activation function.]
Then the final layer of edges is doing linear regression (on values of h&y)!
Usually trained with squared-error loss. If so, it’s least-squares linear regression.
[When we train a neural network by gradient descent, each linear output unit finds the solution of a linear
regression problem by gradient descent. In principle we could find the weights entering that unit by solving
the normal equations. But we don’t, because the hidden unit values keep changing during training.]
(2) Sigmoid units [aka logistic units] for [two-class] classification.
Lety∈Rkbe vector of labels; yi∈[0,1].
Given hidden layer h, output layer computes pre-activation a=Wh∈Rkand applies sigmoid activations to
obtain prediction ˆ y=s(a).
[Here, sis the logistic function applied component-wise to the vector a. The labels yiare usually 0’s and 1’s,
but ˆyican never be exactly 0 or 1. So it might be better to choose target labels like yi=0.05 or yi=0.95,
because then a neural network with enough weights and su fficiently wide layers can achieve ˆ y=yexactly
for every training point! Unless there are co-located training points with di fferent labels.]
Loss fn: Use logistic loss instead of squared error. Fixes vanishing gradients at output!
[The logistic loss function prevents output units from su ffering the vanishing gradient problem, but it can’t
solve the vanishing gradient problem for hidden units. So we don’t use sigmoid hidden units.]
[When we train a neural network by gradient descent, each sigmoid output unit finds the solution of a logistic
regression problem by gradient descent.]
98 Jonathan Richard Shewchuk
(3) Softmax units fork-class classification.
[E.g., in the MNIST digit recognition problem, we would have k=10 softmax output units, one for each
digit class.]
Lety∈Rkbe vector of labels for training pt x[indicating x’s membership in the kclasses].
[It is easy to design a neural network to solve more than one multi-class classification problem simultane-
ously, but for ease of notation let’s suppose we’re solving just one, so there are only koutput units.]
Strongly recommended: choose training labels sokX
i=1yi=1.
We commonly use a one-hot encoding : one label is 1, the others are 0.
[But one-hot encoding has a disadvantage we’ve already discussed for sigmoids: each softmax prediction ˆ yi
can never be exactly 1 or 0. It might be better to choose target labels such as 0 .9, 0.05, and 0.05. Think of
the labels as posterior probabilities, so they should sum to 1.]
Given hidden layer h, output layer computes pre-activation a=Wh∈Rkand applies softmax activation to
obtain prediction ˆ y∈Rk.
Softmax output is ˆ yi(a)=eai
Pk
j=1eaj. Each ˆ yi∈(0,1);kX
i=1ˆyi=1.
[Interpret ˆ yias an estimate of the posterior probability that the input belongs to class i.]
Loss fn: Use cross-entropy . Fixes vanishing gradients at output.
Fork-class softmax output, cross-entropy is L(ˆy,y)=−kX
i=1yiln ˆyi.
↑true labels
↑prediction
k-vectors
[When we train a neural network by gradient descent, if there are softmax output units, those units find the
solution of a softmax regression problem by gradient descent.]
[Cross-entropy losses are only for softmax and sigmoid outputs. For linear or ReLU outputs, cross-entropy
makes no sense, but squared error loss makes sense.]
Vanishing Gradients; ReLUs; Output Units and Losses; Neurobiology 99
Backpropagation for Outputs
For backprop, we need ∇WLand∇hL, where his last hidden unit layer, Wis weights of last edge layer.
output +loss linear +squared error sigmoid +logistic loss softmax +cross-entropy
ˆy= Wh s(Wh) softmax( Wh)
L(ˆy,y)=∥ˆy−y∥2−P
i(yiln ˆyi+(1−yi) ln(1−ˆyi))−P
iyiln ˆyi
∇WL= 2(ˆy−y)h⊤(ˆy−y)h⊤(ˆy−y)h⊤
∇hL= 2W⊤(ˆy−y) W⊤(ˆy−y) W⊤(ˆy−y)
assumingPk
i=1yi=1
[It’s interesting that all three types of outputs produce essentially the same form of gradients for the final
layer of the network, except that the predictions ˆ yare di fferent in each case. This is true even though each
linear or sigmoid output unit is independent, but the softmax outputs units are coupled with each other.]
[Observe that even for sigmoid and softmax units, both ∇WLand∇hLare linear in the error ˆ y−y. This is a
nice outcome, when you consider the exponentials and logarithms we started with. It implies that sigmoid
units with logistic loss do not get stuck when the sigmoid derivatives are small. This is related to the fact
that the logistic loss goes to infinity as the predicted value ˆ yiapproaches zero or one. The vanishing gradient
of the sigmoid unit is compensated for by the exploding gradient of the logistic loss.]
Note: we don’t need to compute ∇ˆyL.
Instead, we eliminate ˆ yby substituting ˆ y(W,h) into L(ˆy,y).
[. . . before taking derivatives. This makes it easier both to derive the math and to write the code.]
[Now I will show you how to perform backpropagation for two hidden layers of ReLU units, a k-class
softmax output, the cross-entropy loss function, and ℓ2regularization—which may improve test accuracy.]
Uh VW
gˆy
∇gL=Pm
j=1∂L
∂hj∇ghj
=P
j∂L
∂hjr′(Vj·g)Vj−kX
i=1yiln ˆyi
+λ(∥U∥2
F+
∥V∥2
F+∥W∥2
F)
r(Vg)∇hL=W⊤(ˆy−y)L
ℓ2regularization (optional)
To add more hidden layers,
r(Ux)copy this.+2λVi∇WL=(ˆy−y)h⊤+2λW
+2λUi =∂L
∂gir′(Ui·x)x=∂L
∂gi∇Uigi=∂L
∂hir′(Vi·g)g=∂L
∂hi∇Vihi∇ViLˆyi=eWi·h
Pk
j=1eWj·h
∇UiL
[Draw this by hand. backpropsoft2.pdf ]
[Note that r(Ux) is the ramp function applied component-wise to the vector Ux. The derivative r′(Ui·x) is
always zero or one. Observe that we don’t need to compute the loss Lat all. We also don’t compute ∇ˆyL, as
we said above, but we do need to compute the value of ˆ yto compute gradients.]
100 Jonathan Richard Shewchuk
Derivations
[I won’t go over this page of derivations in lecture, but I include them here for completeness. Students who
want to understand neural networks deeply should spend some time going through these.]
Linear output units (ˆ y=Wh) with squared error loss:
L(ˆy,y)=∥ˆy−y∥2=∥Wh−y∥2=kX
i=1(Wi·h−yi)2,
∇WiL=2(Wi·h−yi)h=2(ˆyi−yi)h,
∇WL=2(ˆy−y)h⊤,
∇hL=2W⊤(Wh−y)=2W⊤(ˆy−y).
Sigmoid [logistic] output units (ˆ y=s(a)=s(Wh)) with logistic loss:
L(ˆy,y)=−kX
i=1(yiln ˆyi+(1−yi) ln(1−ˆyi))=−kX
i=1 
yiln1
1+e−ai+(1−yi) ln 
1−1
1+e−ai!!
=kX
i=1 
yiln(1+e−ai)−(1−yi) lne−ai
1+e−ai!
=kX
i=1 yiln(1+e−ai)−(1−yi)(−ai−ln(1+e−ai)=kX
i=1 (1−yi)ai+ln(1+e−ai),
∂L
∂ai=1−yi−e−ai
1+e−ai=1
1+e−ai−yi=ˆyi−yi,
ai=Wi·h,∇Wiai=h,∇hai=Wi,
∇WiL=∂L
∂ai∇Wiai=(ˆyi−yi)h,
∇WL=(ˆy−y)h⊤,
∇hL=kX
i=1∂L
∂ai∇hai=kX
i=1(ˆyi−yi)Wi=W⊤(ˆy−y).
Softmax output units (ˆ y=softmax( a)=softmax( Wh)) with cross-entropy loss:
[This derivation uses the assumption thatkX
i=1yi=1 for each training point’s labels.]
L(ˆy,y)=−kX
i=1yiln ˆyi=−kX
i=1yiai−lnkX
j=1eaj=−kX
i=1yiai+lnkX
j=1eaj,
∂L
∂ai=−yi+eai,kX
j=1eaj=ˆyi−yi.
From here, we repeat the last four lines of the sigmoid derivation.
Vanishing Gradients; ReLUs; Output Units and Losses; Neurobiology 101
NEUROBIOLOGY
[The field of artificial intelligence started with some wrong premises. The early AI researchers attacked
problems like chess and theorem proving, because they thought those exemplified the essence of intelligence.
They didn’t pay much attention at first to problems like vision and speech understanding. Any four-year-old
can do those things, and so researchers underestimated their di fficulty.]
[Today, we know better. Computers can beat world chess champions, but they still can’t play with toys well.
We’ve come to realize that rule-based symbol manipulation is not the primary defining mark of intelligence.
Even rats do computations that we’re hard pressed to match with our computers. We’ve also come to realize
that these are di fferent classes of problems that require very di fferent styles of computation. Brains and
computers have very di fferent strengths and weaknesses, which reflect their di fferent computing styles.]
[Neural networks are partly inspired by the workings of actual brains. Let’s take a look at a few things we
know about biological neurons, and contrast them with both neural nets and traditional computation.]
– CPUs: largely sequential, nanosecond gates, fragile if gate fails
superior for arithmetic, logical rules, perfect key-based memory
– Brains: very parallel, millisecond neurons, fault-tolerant
[Neurons are continually dying. You’ve probably lost a few since this lecture started. But you probably
didn’t notice. And that’s interesting, because it points out that our memories are stored in our brains
in a di ffuse representation. There is no one neuron whose death will make you forget that 2 +2=4.
Artificial neural nets often share that resilience. Brains and neural nets seem to superpose memories
on top of each other, all stored together in the same weights, sort of like a hologram.]
[In the 1920’s, the psychologist Karl Lashley conducted experiments to identify where in the brain
memories are stored. He trained rats to run a maze, and then made lesions in di fferent parts of the
cerebral cortex, trying to erase the memory trace. Lashley failed; his rats could still find their way
through the maze, no matter where he put lesions. He concluded that memories are not stored in
any one area of the brain, but are distributed throughout it. Neural networks, properly trained, can
duplicate this property.]
superior for vision, speech, associative memory
[By “associative memory,” I mean noticing connections between things. One thing our brains are very
good at is retrieving a pattern if we specify only a portion of the pattern.]
[It’s impressive that even though a neuron needs a few milliseconds to transmit information to the next
neurons downstream, we can perform very complex tasks like interpreting a visual scene in a tenth of a
second. This is possible because neurons run in parallel, but also because of their computation style.]
[Neural nets try to emulate the parallel, associative thinking style of brains, and they are the best techniques
we have for many fuzzy problems, including most problems in vision and speech. Not coincidentally, neural
nets are also inferior at many traditional computer tasks such as multiplying 10-digit numbers or compiling
source code.]
102 Jonathan Richard Shewchuk
18 Neurobiology; Faster Neural Network Training
NEUROBIOLOGY (cont’d)
neurons.pdf
– Neuron : A cell in brain /nervous system for thinking /communication
– Action potential or spike : An electrochemical impulse fired by a neuron to communicate w /other
neurons
– Axon : The limb(s) along which the action potential propagates; “output”
[Most axons branch out eventually, sometimes profusely near their ends.]
[It turns out that squids have a very large axon they use for fast propulsion by expelling jets of water.
The mathematics of action potentials was first characterized in these squid axons, and that work won
a Nobel Prize in Physiology in 1963.]
– Dendrite : Smaller limb by which neuron receives info; “input”
– Synapse : Connection from one neuron’s axon to another’s dendrite
[Some synapses connect axons to muscles or glands.]
– Neurotransmitter : Chemical released by axon terminal to stimulate dendrite
[When an action potential reaches an axon terminal, it causes tiny containers of neurotransmitter, called
vesicles , to empty their contents into the space where the axon terminal meets another neuron’s dendrite.
That space is called the synaptic cleft . The neurotransmitters bind to receptors on the dendrite and influence
the next neuron’s body voltage. This sounds incredibly slow, but it all happens in 1 to 5 milliseconds.]
You have about 1011neurons, each with about 104synapses.
Neurobiology; Faster Neural Network Training 103
Analogies: [between artificial neural networks and brains]
– Output of unit↔firing rate of neuron
[An action potential is “all or nothing”—all action potentials have the same shape and size. The output
of a neuron is not signified by voltage like the output of a transistor. The output of a neuron is the
frequency at which it fires. Some neurons can fire at nearly 1,000 times a second, which you might
think of as a strong “1” output. Conversely, some types of neurons can go for minutes without firing.
But some types of neurons never stop firing, and for those you might interpret a firing rate of 10 times
per second as a “0”.]
– Weight of connection ↔synapse strength
– Positive weight ↔excitatory neurotransmitter (e.g., glutamine)
– Negative weight ↔inhibitory neurotransmitter (e.g., GABA, glycine) [Gamma aminobutyric acid.]
[A typical neuron is either excitatory at all its axon terminals, or inhibitory at all its terminals. It can’t
switch from one to the other. Artificial neural nets have an advantage here.]
– Linear combo of inputs ↔summation
[A neuron fires when the sum of its inputs, integrated over time, reaches a high enough voltage.
However, the neuron body voltage also decays slowly with time, so if the action potentials are coming
in slowly enough, the neuron might not fire at all.]
– Logistic /sigmoid fn↔firing rate saturation
[A neuron can’t fire more than 1,000 times a second, nor less than zero times a second. This limits its
ability to overpower downstream neurons. We accomplish the same thing with the sigmoid function.]
– Weight change /learning↔synaptic plasticity
[Donald] Hebb’s rule (1949): “Cells that fire together, wire together.”
[This doesn’t mean that the cells have to fire at exactly the same time. But if one cell’s firing tends to
make another cell fire more often, their excitatory synaptic connection tends to grow stronger. There’s
a reverse rule for inhibitory connections. And there are ways for neurons that aren’t even connected
to grow connections.]
[There are simple computer learning algorithms based on Hebb’s rule. They can work, but they’re
generally not nearly as fast or e ffective as backpropagation.]
[Backpropagation is one part of artificial neural networks for which any analogy is doubtful. There have
been some proposals that the brain might do something vaguely like backpropagation,7but it seems tenuous.
Learning in brains is still not well understood.]
[As computer scientists, our primary motivation for studying neurology is to try to get clues about how we
can get computers to do tasks that humans are good at. But neurologists and psychologists have also been
part of the study of neural nets from the very beginning. Their motivations are scientific: they’re curious
how humans think, and how we can do the things we do.]
7See Lillicrap et al., “Backpropagation and the Brain,” Nature Reviews Neuroscience 21, pages 335-–346, April 2020.
104 Jonathan Richard Shewchuk
HEURISTICS FOR FASTER TRAINING
[A big disadvantage of neural nets is that they take a long, long time to train compared to other classifi-
cation methods we’ve studied. Here are some ways to speed them up. Unfortunately, you usually have to
experiment with techniques and hyperparameters to find which ones will help with your particular applica-
tion. I suggest you implement vanilla backpropagation first, usually in combination with stochastic gradient
descent and intelligent weight initialization, and experiment with fancy heuristics only after you get that
working.]
(1) ReLUs. [To fix the vanishing gradient problem, as described in the previous lecture.]
(2) Stochastic gradient descent (SGD): faster than batch on large, redundant data sets.
[Whereas batch gradient descent walks downhill on one cost function, stochastic descent takes a very short
step downhill on one point’s loss function and then another short step on another point’s loss function.
The cost function is the sum of the loss functions over all the sample points, so one batch step is akin
tonstochastic steps and does roughly the same amount of computation. But if you have many di fferent
examples of the digit “9”, they contain much redundant information, and stochastic gradient descent learns
the redundant information more quickly—often much more quickly. Conversely, if the data set is so small
that it encodes little redundant information, batch gradient descent is typically faster.]
102
0y
1
1.41.210.80.60.40.200.20.40.60.811.21.41.41.210.80.60.40.200.20.40.60.811.21.4
				

	




				

	




w2z
w1
x1 x2
batchvsstochmod.pdf (LeCun et al., “E fficient BackProp”) [Left: a perceptron with only
two weights trained to minimize the mean squared error cost function, and its 2D training
data. Center: batch gradient descent makes only a little progress each epoch. Epochs
alternate between red and blue. Right: stochastic descent decreases the error much faster
than batch descent. Again, epochs alternate between red and blue.]
One epoch presents every training point once. Training usually takes many epochs, but if sample is huge
[and carries lots of redundant information], SGD can take less than one epoch.
Neurobiology; Faster Neural Network Training 105
(3) SGD with minibatches .
Choose a minibatch size b; e.g. 256.
Repeatedly perform gradient descent on the sum of the loss functions of brandomly chosen points.
[Although we perform gradient descent on a minibatch of training points all at once, we don’t call it batch
gradient descent. We still call it stochastic gradient descent.]
Advantages [compared to SGD done just one point at a time]:
– Less “bouncy”; usually converges more quickly.
[SGD bounces around wildly. Minibatches reduce the variance of the steps by a factor of√
bwhile
maintaining the advantages of SGD.]
– Can use parallelism, vectorization, GPUs e fficiently.
[The backpropagation computations are fully independent from one training point to another, so it’s
very easy to compute gradients for multiple points in parallel or through vectorization.]
– Better speed because of memory hierarchy.
[You should lay out the activations for the training points in the minibatch next to each other in
memory. With the right memory layout and minibatch size, your use of the caches and memory
hierarchy can be very e fficient. Performing SGD on 64 training points might be almost as fast as
performing SGD on one. The bottleneck in neural network training is memory latency, not arithmetic.]
[Minibatches nearly always work faster than processing just one training point at a time. They are standard
in implementations of neural network training.]
Typically, we shu ffle training pts, partition into ⌈n/b⌉minibatches.
An epoch presents each minibatch once. [Reshu ffling for each epoch is optional.]
[It is important to randomize well so your minibatch is a representative subsample of the training points.
Sometimes practitioners store each class in a separate list, shu ffle each class separately, and build mini-
batches with a proportional number of training points from each class.]
[Be forewarned that the best learning rate ϵwill be di fferent for di fferent values of the minibatch size b, and
there isn’t always a predictable relationship between band the best ϵ.]
(4) To choose learning rate ϵ, use a small random subsample of training data.
[Practitioners have found that the size of the training set has only a weak e ffect on the best choice of ϵ. So
use a subsample to quickly estimate a good learning rate, then apply it to your whole training set. This is
very easy to do, and it can save you a lot of time!]
(5) Emphasizing schemes . [Neural networks learn the most redundant examples quickly, and the most rare
examples slowly. This motivates emphasizing schemes, which repeat the rare examples more often.]
– Stochastic: present examples from rare classes more often, or w /biggerϵ.
– Batch: examples from rare classes have bigger losses.
[Emphasizing schemes are a natural way to incorporate an asymmetric loss function. Suppose you’re devel-
oping a test for a rare disease, and you have a small number of examples of people with the disease, but many
examples of people without it. The classifier might decide to always return negative, “no disease,” because
it can achieve 99% accuracy that way. So we’d like to impose larger losses on examples with positive labels.
For batch gradient descent, we make their losses bigger in the cost function. For SGD there are two options:
use a bigger loss for rare examples, equivalent to presenting them with a larger learning rate ϵ; or present
them more often. It’s not clear which of these two options will train faster, as those extra presentations take
more computation but shorter steps have better convergence properties.]
[Emphasizing schemes can also be used to emphasize misclassified training points, like the Perceptron
Algorithm does, but that can backfire if those points are bad outliers.]
106 Jonathan Richard Shewchuk
(6) Normalizing the training pts.
– Center each feature so mean is zero.
– Then scale each feature so variance ≈1.
normalize.jpg [A 2D example of normalizing points.]
[Remember that the power of neural networks comes from the nonlinearity of the activation function, and
the nonlinearity of a sigmoid or ReLU unit falls where the linear combination of values coming in is close
to zero. Centering makes it easier for the first layer of hidden units to be in the nonlinear operating region.]
[Neural networks are an example of an optimization algorithm whose cost function tends to have better-
conditioned Hessians if the input features are normalized, so it may converge to a local minimum faster.]
-4 -2 2 4w1
-2246w2
-4 -2 2 4w1
-2246w2
-4 -2 2 4w1
-2246w2
illcondition105.pdf, illcondition055.pdf, goodcondition.pdf
[Recall these illustrations from Lecture 5. Gradient descent on a function with an ill-conditioned Hessian
matrix can be slow because a large step size diverges in one direction (left) while a small step size converges
slowly in another direction (center). Normalizing the data might improve the conditioning of the Hessian
(right), thus speeding up gradient descent. Moreover, if you use ℓ2-regularization, normalization makes it
penalize the features more equally.]
[You could go even further and whiten the data, as we discussed in Lecture 9, but whitening takes Θ(nd2+d3)
time for ntraining points with dfeatures, so it takes too much time if dis very large; whereas normalizing
takes Θ(nd) time.]
[Remember that whatever linear transformation you apply to the training points, you must later apply the
same linear transformation to the test points you want to classify!]
Neurobiology; Faster Neural Network Training 107
(7) Initializing weights. [Proper initialization of weights is very important, especially for deep networks.
Consider this carefully for Homework 6!]
[Recall that we initialize a neural network with random weight values to break the symmetry between hidden
units. If we make those random values too small, they might never grow enough, especially if the network is
deep. If we make them too large, we may cause the exploding gradient problem in ReLUs, or the vanishing
gradient problem in sigmoid units. Here are some rules of thumb for initializing random weights.]
Consider the variance of each unit’s output, given random weights.
Principle: output of unit should have same variance as each of its inputs.8
For a unit with fan-in η(not counting bias term), initialize each incoming edge to . . .
[The fan-in of a unit is the number of connections entering the unit.]
For a ReLU unit, a weight in N(0,2/η) orU
−p
6/η,p
6/η
.
[This is called He initialization , after Kaiming He.]
For a sigmoid unit, make it N(0,12.8/η) orU
−p
38.4/η,p
38.4/η
.
For a linear or tanh unit, make it N(0,1/η) orU
−p
3/η,p
3/η
.
[This initialization is sometimes called Xavier initialization , but it isn’t quite what Xavier Glorot originally
proposed. A tanh unit is very similar to a sigmoid unit, but its output is centered at zero, whereas sigmoid
outputs are centered at 0 .5. I don’t recommend you use either sigmoid or tanh units as hidden units, but if
you do, the tanh is preferable for that reason.]
[The reason we divide by the fan-in is because the more inputs a unit has, the greater its incoming signal is.
So we must make the weights smaller to match the unit’s output variance to the variance of each input.]
Set bias terms to zero. [Bias terms can too easily overpower signals coming from earlier layers. For ReLU
units, some people suggest setting the bias terms to a small positive constant so they’re more likely to be
turned on at first, but other people say it gives worse performance in practice.]
Linear output unit: set bias term to the mean label.
Sigmoid output unit: set bias term so default output is the mean label.
[E.g., if 90% of your training points are in class C, set the bias so the sigmoid output defaults to 0 .9. Andrej
Karpathy says that if you don’t initialize the output unit biases, the first few minibatches are largely wasted
learning the mean labels.]
(8) Momentum. Gradient descent changes “momentum” mslowly. [The intuition is that if you’ve taken
many steps in roughly the same direction, you should go faster in that direction.]
m←−ϵ′∇J(w)
repeat
w←w+m
m←βm−ϵ∇J(w)
Good for both batch & stochastic. Choose hyperparameter β<1.
[Here, Jis the cost for the minibatch, which could be anything from a single training point to the whole
training set. The hyperparameter βspecifies how much momentum persists from iteration to iteration.]
[I’ve seen conflicting advice on β. Some researchers set it to 0 .9; some set it close to zero. Geo ffHinton
suggests starting at 0 .5 and slowly increasing it to 0 .9 or higher as the gradients get small.]
[Ifβis large, you should usually choose ϵsmall to compensate, but you might still use a large ϵ′in the first
line so the initial velocity is reasonable.]
8For an explanation of these suggestions, see Siddharth Krishna Kumar, “On Weight Initialization in Deep Neural Networks.”
108 Jonathan Richard Shewchuk
sgdmomentumgodoy.png (Daniel Godoy) [Left: 50 steps of SGD (with 16-point mini-
batches) don’t get very close to the minimum (red). Right: 50 steps with momentum do get
close to the minimum, but overshoot it several times.]
[A problem with momentum is that once it gets close to a good minimum, it overshoots the minimum, then
oscillates around it. But it often gets us close to a good minimum sooner. We see both phenomena above.]
pretzelwaterpark.jpg [How I imagine a neural network’s cost function. It does not resem-
ble a parabolic bowl. The downhill paths from the start to the local minima are sinuous.
The swimmers here employ gradient descent with momentum with great success.]
Convolutional Neural Networks 109
19 Convolutional Neural Networks
CONVOLUTIONAL NEURAL NETWORKS (ConvNets; CNNs)
[Convolutional neural nets drove a big resurgence of interest in neural nets starting in 2012. Often you’ll
hear the buzzword deep learning , which refers to neural nets with many layers. Most image recognition
networks are deep and convolutional. In 2018, the Association for Computing Machinery gave the Alan M.
Turing Award to Geo ffHinton, Yann LeCun, and Yoshua Bengio for their work on deep neural networks.]
Vision: inputs are images. 400 ×400 image =160,000 pixels.
If we connect them all to 160,000 hidden units →25.6 billion connections.
[With so many weights, the network is very slow to train or even to use once trained.]
[Remember that early in the semester, I told you that you can get better performance on the handwriting
recognition task by using edge detectors. Edge detectors have two interesting properties. First, each edge
detector looks at just one small part of the image. Second, the edge detection computation is the same no
matter which part of the image you apply it to. Let’s apply these two properties to neural net design, plus
one new idea: we’ll learn the edge detectors instead of hard-coding them.]
ConvNet ideas:
– Local connectivity: A hidden unit connects only to a small patch of units in previous layer.
[A unit in the first hidden layer doesn’t look at the whole image. It looks only at a small number of
pixels—typically 9, 25, or 49 pixels. This speeds up both training and classification considerably.]
– Shared weights: Groups of hidden units share same set of weights, called a filter aka mask aka kernel .
Each filter operates on every patch of image.
75 09 8 1patches
3
shared weights
filternot a matrix!
5891
0
274
3
5
activation mapinput units hidden units
image3 7 4
2
891
0
24
convlayer.pdf [Applying a filter to an image. Every hidden unit uses the same nine shared
weights. In this example, a 6 ×6 image is covered by 16 overlapping 3 ×3 patches, yielding
a 4×4 activation map of hidden units. We learn the filter by backpropagation.]
110 Jonathan Richard Shewchuk
If image size is J×Kand filter size is M×M, the activation map is (J−M+1)×(K−M+1) hidden
units—one for each patch.
A convolutional layer learns multiple filters. There is one activation map per filter. A channel is an activation
map, OR another dimension such as the red /green /blue channels of an input image.
0 0 00
0-10
0-1-1
-14-1 -2 -1
1 2 1-1 1 0
-2 2 0
-1 1 0
Sobel horizontal Laplacian Sobel vertical
caredges.pdf (Cezanne Camacho) [Two Sobel filters (one horizontal, one vertical) and a Laplacian filter
applied to an image, yielding three activation maps (three channels). Note that these filters were not learned
by a CNN (but they could be).]
8 -9
-2-35
-6
-90-3 4
7 4
2 5 09 8 1
input units
3 channels: red, green, bluehidden units shared weightsfilter = 3D array
4 filters (4D array) 4 channels: activation maps3
convchannels.pdf [A convolutional layer (of edges). If a layer’s input has more than one
channel, then each filter is represented by a three-dimensional matrix. The set of all filters
is represented by a four-dimensional matrix.]
Convolutional Neural Networks 111
[The output of a convolutional layer has multiple channels, and usually so does the input. The layer’s output
has one channel per filter, and these channels becomes inputs to downstream convolutional layers. The input
to the neural network often has multiple channels too; most commonly, the color channels of a color image.]
If edge layer lhasC(l−1)channels in and C(l)channels out ( C(l)filters),
– # of weights /filter =C(l−1)×M×M;
– # of weights in layer =C(l−1)×C(l)×M×M;
– # of units out =C(l)×# of patches =C(l)×(J−M+1)×(K−M+1).
Typically, each convolutional hidden unit ends with a ReLU activation.
[But there are exceptions in many modern CNNs.]
Options for bias terms:
– Untied bias :C(l)×(J−M+1)×(K−M+1) bias terms—one per unit out.
– Tied bias :C(l)bias terms—one per filter /channel out.
– No bias terms. [This option is usually notimmediately followed by a ReLU.]
[Untied bias terms are a lot of extra weights, sometimes more weights than the filters! Sometimes they give
better test accuracy, but not always, so try validating both ways.]
Benefits of shared weights:
– Much less memory needed. [Better cache behavior too.]
– Regularization. [It’s unlikely that a weight will become spuriously large if it’s used in many places.]
– If one filter learns to detect edges, every patch has an edge detector.
[Because the filter that detects edges is applied to every patch.]
ConvNets exploit repeated structure in images, audio.
– A filter destined to become an edge detector learns on edges in every part of every image.
[So it can learn the idea faster.]
[In a neural net, you can think of hidden units as added features that we learn, as opposed to added features
that you code up yourself. Convolutional neural nets take them to the next level by learning features from
multiple patches simultaneously and then applying those features everywhere, not just in the patches where
they were originally learned.]
LeNet5.png Architecture of LeNet5.
[ConvNets were first popularized by the success of Yann LeCun’s “LeNet 5” handwritten digit recognition
software. LeNet 5 has six hidden layers! Hidden layers 1 and 3 are convolutional layers with shared weights.
Layers 2 and 4 are pooling layers that make the image smaller, with no weights at all. Layers 5 and 6 are
fully-connected layers of hidden units with no shared weights. A great deal of experimentation went into
112 Jonathan Richard Shewchuk
figuring out the number of layers and their sizes. At its peak, LeNet 5 was responsible for reading the zip
codes on 10% of US Mail. Another Yann LeCun system was deployed in ATMs and check reading machines
and was reading 10 to 20% of all the checks in the US by the late 90’s. LeCun is one of the Turing Award
winners I told you about earlier.]
[Show Yann LeCun’s video LeNet5.mov, illustrating LeNet 5.]
Downsampling
[At the output of LeNet 5, we have to compress the information down to a single 10-unit output. Experience
shows that this is best done by slowly compressing the information in the image through a sequence of
layers, rather than connecting a very large layer of hidden units directly to the output. This observation
echoes classic image processing techniques that were developed before neural networks. The two popular
methods of downsampling are called pooling andstrided convolution .]
Max pooling : Reduce a J×Kimage to⌈J/2⌉×⌈K/2⌉or⌈J/3⌉×⌈K/3⌉[as illustrated].
Springer Nature 2021 L ATEX template
Article Title 7
Fig. 5 Convolution operation with pooling size of 2x2
3.6 Fully Connected Layer
The output from the ﬁnal (and any) pooling and convolutional layer is ﬂatte ned
and then fed into the fully connected layer as described in Fig 6. After passing
through the fully connected layers, the ﬁnal layer uses the softmax activation
function which is used to get probabilities of the input being in a p articular
class (classiﬁcation).
Fig. 6 Fully Connected Layer
The ﬁnal CNN architecture is summarized in Table 1:
It is also note-worthy that we added the batch normalization and dropout
layers in the CNN architecture. The batch normalization is a regulariz ation
technique that helps to prevent overﬁtting [ 16]. This process is important
because it improves the speed, performance, and stability of traini ng. It does
this by normalizing each layer’s inputs by squashing the values to az e r o
mean and unit variance in the current batch. Dropout layer was also impl e-
mented per-layer in the network. During training 50% of the layer ou tputs
were randomly ignored (dropped out), this helps prevent overﬁttin g as well.
maxavgpool.pdf (Ekpenyong et al.) [Max pooling and average pooling.]
Average pooling : likewise, but each unit out is the average of four units in.
[Average pooling was used in LeNet 5, but max pooling seems more popular now.]
Pooling layers have no weights . Nothing to train!
[But you still have to think carefully about how to do backpropagation through them.]
Strided convolution : Patches overlap less (or not at all).
K(i, j)is the kernel or ﬁlter of size i⇥j,
S(i, j)is the output featured map.
Properties of Convolution operation:
The convolution operation satisﬁes commutative property,
the equation is given below. [12]
S(i,j)=( K⇤I)(i,j)=X
mX
nI(i m,j n)K(m,n))
(5)
The Cross-Correlation can be obtained by ﬂiping the negative
sign or replace the negative sign with positive sign. The Cross-
Correlation can be given as. [12]
S(i,j)=( K⇤I)(i,j)=X
mX
nI(i+m,j+n)K(m,n))
(6)
The convolution operation for an two-dimensional image is
explained in the below Fig. 2. The image of size 5⇥5is taken
as input for the convolution operation. The kernel or the ﬁlter
of size 3⇥3is taken. The output of the convolution operation
is called Convolved Feature map. The convolution operation
is performed by sliding the kernel or ﬁlter on the image pixel.
The convolved output feature map is extracted as shown in
the below Fig. 2. The convolutional layer has several ﬁlters
Fig. 2. Convolution Operation
or kernels. For each input image the convolution operation is
repeated with several ﬁlters of different sizes to obtain the
output featured map.
Filters or Kernels:
The convolution operation is performed with several number
of ﬁlters to obtain different features of the image. Some of
the ﬁlters we use in general are edge detection, sharpen, blur,
Gaussian blur and more. Some of the ﬁlters and the output
feature maps are given in the below Fig. 3.[13]
Fig. 3. Commonly used ﬁlters in Convolution Operation [13]
Strides:
Deﬁned as number of pixels shifted over the input image
to apply ﬁlter during convolution operation. The convolution
operation is performed by shifting ﬁlter with one pixel if the
value of stride is selected as one. The convolution operation
is performed by shifting ﬁlter with two pixel if the value of
stride is selected as two. the example for the stride value with
two is explained in the below Fig. 4. In the below Fig. 4 the
input of size 7⇥7, ﬁlter of size 3⇥3and stride value of 2
are taken to perform the convolution operation [13].
Fig. 4. Convolution Operation with Stride value 2
Padding:
During convolution operation sometimes the ﬁlter is not
perfectly ﬁt to the input image. To avoid this situation Padding
operation is held on the input image. There are two types of
padding operation are applied to the input image during the
convolution operation. [13]
stride.pdf (Vadlamani & Patel) [Strided convolution.]
Convolutional Neural Networks 113
AlexNet
[I told you three lectures ago that neural net research was popular in the 60’s, but the 1969 book Perceptrons
killed interest in them throughout the 70’s. They came back in the 80’s, but interest was partly killed o ffa
second time in the 00’s by . . . guess what? By support vector machines. SVMs work well for a lot of tasks,
they’re much faster to train, and they more or less have only one hyperparameter, whereas neural nets take a
lot of work to tune.]
[Neural nets are now in their third wave of popularity. The single biggest factor in bringing them back is
probably big data. Thanks to the internet, we now have absolutely huge collections of images to train neural
nets with, and researchers have discovered that neural nets often give better performance than competing
algorithms when you have huge amounts of data to train them with. In particular, convolutional neural nets
are now learning better features than hand-tuned features. That’s a recent change.]
[The event that brought attention back to neural nets was the ImageNet Image Classification Challenge in
2012. The winner of that competition was a neural net, and it won by a huge margin, about 10%. It’s called
AlexNet, and it’s surprisingly similarly to LeNet 5, in terms of how its layers are structured. However,
there are some recent innovations that led to their prize-winning performance, in addition to the fact that the
training set had 1.4 million images: they used ReLUs, dropout, data augmentation, and GPUs for training.]
alexnet.pdf (Krizhevsky et al., 2012, 2017) [Architecture of AlexNet.]
[When ConvNets were first applied to image analysis, researchers found that some of the learned filters are
edge detectors! Here are the first layers of filters learned by AlexNet.]
filtersalex.png (Krizhevsky et al., 2012, 2017) [Filters learned by the first layer of AlexNet.]
114 Jonathan Richard Shewchuk
[Not all of the features are edge detectors; many of them are more concerned with color. But more than half
of them resemble mathematical functions called Gabor filters, which detect edges and also textures.]
10.3. Visualizing Trained CNNs 303
Figure 10.11 Examples of Gabor ﬁlters deﬁned
by (10.6). The orientation angle
θvaries from 0in the top row to
π/2in the bottom row, whereas
the frequency varies from ω=1
in the left column to ω= 10 in the
right column.
convolutional neural networks. The neocognitron had multiple layers of processing
comprising local receptive ﬁelds with shared weights followed by local averaging or
max-pooling to confer positional invariance. However, it lacked an end-to-end train-
ing procedure since it predated the development of backpropagation, relying instead
on greedy layer-wise learning through an unsupervised clustering algorithm.
10.3.2 Visualizing trained ﬁlters
Suppose we have a trained deep CNN and we wish to explore what the hidden
units have learned to detect. For the ﬁlters in the ﬁrst convolutional layer this is
relatively straightforward, as they correspond to small patches in the original input
image space, and so we can visualize the network weights associated with these
ﬁlters directly as small images. The ﬁrst convolutional layer computes inner products
between the ﬁlters and the corresponding image patches, and so the unit will have a
large activation when the inner product has a large magnitude.
Figure 10.12 shows some example ﬁlters from the ﬁrst layer of a CNN trained
on the ImageNet data set. We see a remarkable similarity between these ﬁlters and
the Gabor ﬁlters of Figure 10.11 . However, this does not imply that a convolutional
neural network is a good model of how the brain works, because very similar results
can be obtained from a wide variety of statistical methods (Hyv ¨arinen, Hurri, and
Hoyer, 2009). This is because these characteristic ﬁlters are a general property of
the statistics of natural images and therefore prove useful for image understanding
in both natural and artiﬁcial systems.
Although we can visualize the ﬁlters in the ﬁrst layer directly, the subsequent
layers in the network are harder to interpret because their inputs are not patches
of images but groups of ﬁlter responses. One approach, analogous to that used by
Hubel and Wiesel, is to present a large number of image patches to the network and
gabor.pdf (Bishop, Figure 10.11) [Gabor filters. Notlearned; these are math functions.]
[AlexNet learned some simple color-specific edge detectors, but I find it noteworthy that the higher-frequency
texture detectors are not sensitive to color at all. Apparently, the CNN decided it can separate fine texture
from color.]
[Unfortunately, we can’t just draw the filters learned by subsequent convolutional layers, because they’re
3D arrays that don’t carry much visual information. Instead, Zeiler and Ferguson (2013) have a technique
where they determine which patches from the training set most trigger a particular filter, and they draw nine
of those. They also have a technique for determining which pixels of those patches are most relevant to
triggering the filter, and they plot the relevance of each patch pixel. This reveals that in the third example
for convolutional layer 4, the filter is primarily responding to the grass in these images.]
Conv layer 2 (four of the filters):
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Convolutional Neural Networks 115
Conv layer 3:
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Conv layer 4:
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Conv layer 5:
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
Visualizing and Understanding Convolutional Networks
Layer 2Layer 1
Layer 3
Layer 4
Layer 5
Figure 2. Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.
Our reconstructions are notsamples from the model: they are reconstructed patterns from the validation set that cause
high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:
(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of
discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.
116 Jonathan Richard Shewchuk
The V1 Visual Cortex
[The idea to exploit local connectivity in CNNs was inspired by the human visual system, as well as by
techniques used in image processing.]
[Show slides on the visual cortex, available from the CS 189 web page. Sorry, readers, there are too many
images to include here. The narration is below.]
[Neurologists can stick needles into individual neurons in animal brains. After a few hours the neuron dies,
but until then they can record its action potentials. In this way, biologists quickly learned how some of the
neurons in the retina, called retinal ganglion cells , respond to light. They have interesting receptive fields ,
illustrated in the slides, which show that each ganglion cell receives excitatory stimulation from receptors in
a small patch of the retina but inhibitory stimulation from other receptors around it.]
[The signals from these cells propagate to the V1 visual cortex in the occipital lobe at the back of your
skull. The V1 cells proved harder to understand. David Hubel and Torsten Wiesel of the Johns Hopkins
University put probes into the V1 visual cortex of cats, but they had a very hard time getting any neurons to
fire there. However, a lucky accident unlocked the secret and ultimately won them the 1981 Nobel Prize in
Physiology.]
[Show video HubelWiesel.mp4, taken from YouTube: https: //www.youtube.com /watch?v =IOHayh06LJ4 ]
[The glass slide happened to be at the particular orientation the neuron was sensitive to. The neuron doesn’t
respond to other orientations; just that one. So they were pretty lucky to catch that.]
[The simple cells act as line detectors and /or edge detectors by taking a linear combination of inputs from
retinal ganglion cells. It’s fascinating, and surely not a coincidence, that humans and CNNs for vision both
have edge detectors in their early layers.]
[The complex cells act as location-independent line detectors by taking inputs from many simple cells,
which are location dependent. It’s reminiscent of max pooling.]
[Later researchers showed that local connectivity runs through the V1 cortex by projecting certain images
onto the retina and using radioactive tracers in the cortex to mark which neurons had been firing. Those
images show that the neural mapping from the retina to V1 is retinatopic , i.e., locality preserving. This is a
big part of the inspiration for convolutional neural networks!]
[Unfortunately, as we go deeper into the visual system, layers V2 and V3 and so on, we know less and less
about what processing the visual cortex does.]
Unsupervised Learning: Principal Components Analysis 117
20 Unsupervised Learning: Principal Components Analysis
UNSUPERVISED LEARNING
We have sample points, but no labels!
No classes, no y-values, nothing to predict.
Goal: Discover structure in the data.
Examples:
– Clustering: partition data into groups of similar /nearby points.
– Dimensionality reduction: data often lies near a low-dimensional subspace (or manifold) in feature
space; matrices have low-rank approximations.
[Whereas clustering is about grouping similar sample points, dimensionality reduction is about iden-
tifying a continuous variation from sample point to sample point.]
– Density estimation: fit a continuous distribution to discrete data.
[When we use maximum likelihood estimation to fit Gaussians to sample points, that’s density esti-
mation, but we can also fit functions more complicated than Gaussians.]
PRINCIPAL COMPONENTS ANALYSIS (PCA) (Karl Pearson, 1901)
Goal: Given sample points in Rd, find kdirections that capture most of the variation. (Dimensionality
reduction.)
First principal componentSecond principal component
−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0
3dpca.pdf (ISL, Figure 12.2) [Example of 3D points projected to 2D by PCA. The
3D space on the left is the feature space, and the 2D space on the right is the
principal component space .]
118 Jonathan Richard Shewchuk
Introduction
Principal Components Analysis
Laurens van der Maaten and Geo ↵rey Hinton, JMLR 2008 (MCLab) t-SNE October 30, 2014 4 / 33
pcadigits.pdf [The (high-dimensional) MNIST digits projected to a 2D subspace (from
784D). Two dimensions aren’t enough to fully separate the digits, but observe that the digits
0 (red) and 1 (orange) are well on their way to being separated.]
Why?
– Reducing # of dimensions makes some computations cheaper, e.g., regression.
– Remove irrelevant dimensions to reduce overfitting in learning algs.
Like subset selection, but the “features” aren’t axis-aligned;
they’re linear combos of input features.
– Find a small basis for representing variations in complex things, e.g., faces, genes.
[Sometimes PCA is used as a preprocess before regression or classification for the first two reasons.]
LetXben×ddesign matrix. [No fictitious dimension.]
From now on, assume Xis centered:P
iXi=0. (Replace Xwith ˙X.)
[We center the data in the usual way: by computing the sample mean, then subtracting the sample mean
from each sample point.]
[Let’s start by seeing what happens if we pick just one principal direction.]
Letwbe a unit vector.
The orthogonal projection of point xonto vector wis ˜x=(x·w)w.
x
˜xw
Ifwnot unit, ˜ x=x·w
∥w∥2w.
[The idea is that we’re going to pick the best direction w, then project all the data down onto wso we can
analyze it in a one-dimensional space. Of course, we lose a lot of information when we project down from
ddimensions to just one. So, suppose we pick several directions. Those directions span a subspace, and we
want to project points orthogonally onto the subspace. This is easy ifthe directions are orthogonal to each
other.]
Unsupervised Learning: Principal Components Analysis 119
Given orthonormal directions v1,..., vk, ˜x=Pk
i=1(x·vi)vi.
[The word “orthonormal” means they’re all mutually orthogonal and all have length 1.]
v1˜xx
v2
Often we want just the kprincipal coordinates x·viin principal component space.
[Often we don’t actually want the projected point in Rd. Sometimes we do, but often we just want the
principal coordinates.]
X⊤Xis square, symmetric, positive semidefinite, d×dmatrix. [As it’s symmetric, its eigenvalues are real.]
Let 0≤λ1≤λ2≤...≤λdbe its eigenvalues. [sorted]
Letv1,v2,..., vdbe corresponding orthogonal unit eigenvectors. These are the principal components .
[. . . and the most important principal components will be the ones with the greatest eigenvalues. I will show
you this in three di fferent ways.]
PCA derivation 1: Fit a Gaussian to data with maximum likelihood estimation.
Choose kGaussian axes of greatest variance.
first choice
MLEPCA.pdf [A Gaussian fitted to sample points. The principle component with the
greatest eigenvalue is drawn.]
Recall that MLE estimates a covariance matrix ˆΣ =1
nX⊤X. [Presuming Xis centered.]
PCA Alg:
– Center X.
– Optional: Normalize X. Units of measurement di fferent?
– Yes: Normalize.
[Bad for principal components to depend on arbitrary choice of scaling.]
– No: Usually don’t.
[If several features have the same unit of measurement, but some of them have smaller variance
than others, that di fference is usually meaningful. In particular, you should never normalize
image pixels individually.]
– Compute unit eigenvectors /values of X⊤X.
120 Jonathan Richard Shewchuk
– Choose k. (Optional: based on the eigenvalue sizes.)
– For the best k-dimensional subspace, pick eigenvectors vd−k+1,..., vd.
– Compute the kprincipal coordinates x·viof each training /test point.
[When we do this projection, we have two choices: we can project the original, un-centered training
data OR we can project the centered training data. But if we do the latter, we have to translate the test
data by the same vector we used to translate the training data when we centered it.]
[End of algorithm.]
% of variability =dX
i=d−k+1λi
dX
i=1λi
variance.pdf [Plot of # of eigenvectors vs. percentage of sample variance captured for a
17D data set. In this example, just 3 eigenvectors capture 70% of the variance.]
First Principal ComponentSecond Principal Component* **
**
**
*
*
**
**
* **
* ** **
**
**
***
**
**
***
** **
** ***
***
**
*
−0.5 0.0 0.5MurderAssaultUrbanPop
RapeScaled−3 −2 −1 0 1 2 3 
−100 −50 0 50 100 150
First Principal ComponentSecond Principal Component***
*** ***
**
*****
*****
* *
**
***
**
**
*******
*****
***
***
−3 −2 −1 0 1 2 3 −0.5 0.0 0.5
−100 −50 0 50 100 150−0.5 0.0 0.5 1.0
−0.5 0.0 0.5 1.0Murder Assa uUrbanPop
RapeUnscaled
normalize.pdf (ISL, Figure 12.4) [Projection of 4D data onto a 2D subspace. Each point
represents one metropolitan area. Normalized data at left; unnormalized data at right. The
arrows show the four original axes projected on the two principal components. When the
data are not normalized, rare occurrences like murder have little influence on the principal
directions. Which is better? It depends on whether you think that low-frequency events like
murder and rape should have a larger influence.]
[If you are using PCA as a preprocess for a supervised learning algorithm, there’s a more e ffective way to
choose k: validation.]
Unsupervised Learning: Principal Components Analysis 121
PCA derivation 2: Find direction wthat maximizes sample variance of projected data.
[In other words, when we project the data down, we don’t want it all to bunch up; we want to keep it as
spread out as possible.]
project.jpg [Points projected on a line. We wish to choose the orientation of the green line
to maximize the sample variance of the blue points.]
Find wthat maximizes Var( {˜X1,˜X2,..., ˜Xn})=1
nnX
i=1 
Xi·w
∥w∥!2
=1
n∥Xw∥2
∥w∥2=1
nw⊤X⊤Xw
w⊤w|      {z      }
Rayleigh quotient ofX⊤Xandw
[This fraction is a well-known construction called the Rayleigh quotient. When you see it, you should smell
eigenvectors nearby. How do we maximize this?]
Ifwis an eigenvector viofX⊤X, Ray. quo. =λi
→of all eigenvectors, vdachieves maximum variance λd/n.
One can show vdbeats every other vector too.
[Because every vector wis a linear combination of eigenvectors, and so its Rayleigh quotient will be a
convex combination of eigenvalues. It’s easy to prove this, but I don’t have the time. For the proof, look up
“Rayleigh quotient” in Wikipedia.]
[So the top eigenvector gives us the best direction. But we typically want kdirections. After we’ve picked
one direction, then we have to pick a second direction that’s orthogonal to the best direction. But subject to
that constraint, we again pick the direction that maximizes the sample variance.]
What if we constrain wto be orthogonal to vd? Then vd−1is optimal.
[And if we need a third direction orthogonal to vdandvd−1, the optimal choice is vd−2. And so on.]
122 Jonathan Richard Shewchuk
PCA derivation 3: Find direction wthat minimizes mean squared projection distance.
PCAanimation.gif [This is an animated GIF; unfortunately, the animation doesn’t work in
the PDF lecture notes. Find the direction of the black line for which the sum of squares of
the lengths of the red lines is smallest.]
[You can think of this as a sort of least-squares linear regression, with one subtle but important change. In-
stead of measuring the error in a fixed vertical direction, we’re measuring the error in a direction orthogonal
to the principal component direction we choose.]
projlsq.png, projpca.png [Least-squares linear regression vs. PCA. In linear regression,
the projection direction is always “vertical” (measured in the label coordinate); whereas in
PCA, the projection direction is orthogonal to the projection hyperplane. In both methods,
however, we minimize the sum of the squares of the projection distances.]
Find wthat minimizesnX
i=1Xi−˜Xi2=nX
i=1Xi−Xi·w
∥w∥2w2
=nX
i=1∥Xi∥2− 
Xi·w
∥w∥!2
=constant−n(variance from derivation 2) .
Minimizing mean squared projection distance =maximizing variance.
[From this point, carry on with the same reasoning as derivation 2.]
Unsupervised Learning: Principal Components Analysis 123
europegenetics.pdf (Lao et al., Current Biology, 2008.) [Illustration of the first two prin-
cipal components of the single nucleotide polymorphism (SNP) matrix for the genes of
various Europeans. The design matrix has 2,547 people from these locations in Europe
(right), and 309,790 SNPs per person. Each SNP is binary, so think of it as 309,790 dimen-
sions of zero or one. The output (left) shows spots on the first two principal components
where there was a high density of projected people from a particular national type. What’s
amazing about this is how closely the projected genotypes resemble the geography of Eu-
rope.]
Eigenfaces
Xcontains nimages of faces, dpixels each.
[If we have a 200 ×200 image of a face, we represent it as a vector of length 40,000, the same way we
represent the MNIST digit data.]
Face recognition: Given a query face, compare it to all training faces; find nearest neighbor in Rd.
[This works best if you have several training photos of each person you want to recognize, with di fferent
lighting and di fferent facial expressions.]
Problem: Each query takes Θ(nd) time.
Solution: Run PCA on faces. Reduce to much smaller dimension d′.
Now nearest neighbors takes O(nd′) time.
[Possibly even less. We’ll talk about speeding up nearest-neighbor search at the end of the
semester. If the dimension is small enough, you can sometimes do better than linear time.]
[If you have 500 stored faces with 40,000 pixels each, and you reduce them to 40 principal components,
then each query face requires you to read 20,000 stored principal coordinates instead of 20 million pixels.]
124 Jonathan Richard Shewchuk
facerecaverage.jpg, facereceigen0.jpg, facereceigen119.jpg, facereceigen.jpg [Images of
the the eigenfaces with the 32 largest eigenvalues. The “average face” is the mean used
to center the data.]
Unsupervised Learning: Principal Components Analysis 125
eigenfaceproject.pdf [Images of a face (left) projected onto the first 4 and 50 eigenvectors, with
the average face added back. The 50-eigenvector image is blurry but good enough for face recog-
nition. (These projections are in feature space, not in principle components space. The principle
coordinates are what you would use in the nearest neighbor classifier.)]
For best results, equalize the intensity distributions first.
facerecequalize.jpg [Image equalization.]
[Eigenfaces encode both face shape andlighting. Some people say that the first 3 eigenfaces are usually all
about lighting, and you sometimes get better facial recognition by dropping the first 3 eigenfaces.]
[Eigenfaces are not a state-of-the-art face recognition algorithm, not even close. But inspecting these images
can give you some intuition about PCA.]
[Optional: Show Blanz–Vetter face morphing video (morphmod.mpg).]
[Blanz and Vetter use PCA in a more sophisticated way for 3D face modeling. They take 3D scans of
people’s faces and find correspondences between peoples’ faces and an idealized model. For instance, they
identify the tip of your nose, the corners of your mouth, and other facial features, which is something the
original eigenface work did not do. Instead of feeding an array of pixels into PCA, they feed the 3D locations
of various points on your face into PCA. This works more reliably.]
126 Jonathan Richard Shewchuk
21 The Singular Value Decomposition; Clustering
THE SINGULAR V ALUE DECOMPOSITION (SVD) [and its Application to PCA]
Problems with PCA: Computing X⊤Xtakes Θ(nd2) time.
X⊤Xis poorly conditioned →numerically inaccurate eigenvectors.
[The SVD improves both these problems.]
[Earlier this semester, we talked about the eigendecomposition of a square, symmetric matrix. Unfortu-
nately, nonsymmetric matrices don’t have nice eigendecompositions, and non-square matrices don’t have
eigenvectors at all. Happily, there is a similar decomposition that works for all matrices, even if they’re not
symmetric and not square.]
Fact: Every X∈Rn×dhas a singular value decomposition X=UDV⊤of the form
00
vdδ2...v1
n×dX
==
n×dU
u1
n×nV⊤V=I=VV⊤
orthonormal vi’s are
right singular vectors ofX0 0
U⊤U=I=UU⊤unV⊤=min{n,d}X
i=1δiuiv⊤
i|{z}
rank 1
orthonormal ui’s are left singular vectors ofXouter product
matrixD
d×ddiagonal
δ1
δd
[Draw this by hand; write summation at the right last. fullsvd.pdf ]
Diagonal entries δ1,...,δ min{n,d}ofDare nonnegative singular values ofX.
[By convention, singular values are never negative, but some of them might be zero.]
Fact: viis an eigenvector of X⊤Xw/eigenvalueδ2
i, so the SVD solves PCA.
Proof: X⊤X=VD⊤U⊤UDV⊤=VD⊤DV⊤
which is an eigendecomposition of X⊤X, with eachδ2
ion the diagonal of D⊤D.
[The columns of Vare the eigenvectors of X⊤X, which are the principal components we need for PCA. The
SVD also tells us their eigenvalues, which are the squares of the singular values of X. That’s related to why
the SVD is more numerically stable: the ratios between singular values are smaller than the ratios between
eigenvalues, so it’s easier to stably compute the singular values of Xthan the eigenvalues of X⊤X.]
Important: Row iofUDgives the principal coordinates of sample point Xi(i.e.,∀i,∀j,Xi·vj=δjUi j).
[So we don’t need to explicitly compute the inner products Xi·vj; the SVD has already done it for us.]
Proof: XV=UDV⊤V=UD, so ( XV)i j=(UD)i j.
The Singular Value Decomposition; Clustering 127
The Compact SVD
Singular vectors with singular value zero are useless for PCA [and many other applications], motivating the
compact SVD . Let rbe the rank of X.
00δ2...X
n×dV⊤
V⊤V=In×rr×r r×durδrvrv1=rX
i=1δiuiv⊤
i
== U D
U⊤U=Idiagonal
u1δ1
[Draw this by hand. compsvd.pdf ]
δ1,...,δ rare nonzero singular values of the full SVD , & have the same left /right singular vectors.
[There are rnonzero singular values, and we can express Xwith their singular vectors alone. A nice conse-
quence is that Dis invertible. If Xis a centered design matrix for sample points that all lie on a line, then
Xhas rank 1 and there is only one nonzero singular value. If the centered sample points span a subspace of
dimension r,Xhas rank rand there are rnonzero singular values.]
[We might save a fair amount of time by not computing the left and right singular vectors with singular value
zero. Observe that the columns of Uare still orthogonal, but it is no longer true that UU⊤=I. The same
goes for V.]
Fact: We can find the kgreatest singular values & corresponding vectors in O(ndk) time.
[So we can save time by computing some of the singular vectors without computing all of them.]
[There are approximate, randomized algorithms that are even faster, producing an approximate
SVD in O(ndlogk) time. These are starting to become popular in algorithms for very big data.]
[ https: //code.google.com /archive /p/redsvd /]
[In the next lecture, we will use the compact SVD to help us understand the Moore-Penrose pseudoinverse
and its application to least-squares linear regression.]
CLUSTERING
Partition data into clusters so points in a cluster are more similar than across clusters.
Why?
– Discovery: Find songs similar to songs you like; determine market segments
– Hierarchy: Find good taxonomy of species from genes
– Quantization : Compress a data set by reducing choices
– Graph partitioning: Image segmentation; find groups in social networks
128 Jonathan Richard Shewchuk
Barry Zito
60 65 70 75 80 85 90!150!100  !50    0   50  100  150
!150!100 !50   0  50 100 150
Start Speed
Side SpinBack Spin!
!!
!!!
!!
!
!
!!!
!
!!!
!!!
!!
!
!!
!!!
!
!!
!
!
!
!
!!!
!!
!
!
!
!!
!!
!!!!
!!
!!
!!!
!!
!
!!
!!
!!
!!
!!
!
!!!!
!!
!!!
!!
!!
!!!
!!
!!!
!
!!!
!!
!
!
!!
!!!
!!
!!!
!
!!!
!!
!
!!
!!
!
!!!
!
!
!!
!!
!!
!
!!
!!!
!!!
!
!!!
!
!
!!!
!!
!!
!
!!
!!
!!!
!!!!
!!!
!!!
!!!
!!!!
!!
!!!!!!!
!
!!
!!
!
!!
!!
!!
!
!!
!!
!!
!!!
!
!!!
!!
!!!
!
!!
!!!
!
!!!!
!!
!
!!
!
!
!!
!!
!
!!!
!!!
!
!!!
!!
!!
!!!!
!! !
! !!
!
!
!!!
!
!!!
!!
!!
!
!!!
!
!
!!!
!
!!
!
!!!!
!!!
!!
!!!
! !!
!!
!
!!
!!!
!!
!!
!
!!
!
!!!
!!
!!
!!
!!
!!!!
!!
!!!!
!!!
!!!
!!
!!
!!
!!!
!!
!
!
!!
!!
!!!!
!!!!
!!
!!
!!
!!
!
!
!!
!!!
!
!!!
! !! !!
!!
!
!
!
!!
!!!
!!!!
!!
!!!
!
!!!!!
!!!
!
!
!!
!!
!
!!
!!
!!!
!
!!!!!
!
!!
!!
!!!
!!
!!!!
!!
!!
!!!
!!
!!
!
!
!
!!
!!!
!!!
!!
!!
!
!!!
!!!
!!
!!
!!!
!!
!!
!!
!
!!!
!!!
!! !
!
!!
!!
!
!
!!
!! !!
!!!
!!!
!
!!!
!
!!
!
!!
!!
!
!!!
!!
!!
!
!
!!!!!
!!
!!!
!!!
!
!!
!
!
!!
!!!
!
!!
!!
!!
!
!!
!!
!!!
!!!
!!
!
!!!
!
!!
!
!
!!!!!
!
!!!
!!!
!!
!!!!
!!!!!
!!
!!!!
!!!!
!!
!!
!
!!
!!
!! !! !!
!!
!!
!
!!
!!
!!
!
!!
!!!
!!
!
!
!!
!
!!!!
!
!!
!!
!!!!!
!
!
!!!!
!!!
!!
!
!!!!!
!!
!
!!!
!!
!!
!!
!
!!
!!!
!
!!!
!
!!
!!!!
!!!!!
!!!
!
!
!!
!!! !
!
!!!
!!!
!
!!!
!!!
!!
!!!
!!!!
!
!!
!!
!!
!!
!!
!!
!
!!
!!!!
!!!!!
!!
!
!
!
!!!
!
!!
!
!!
!!
!!!
!
!
!!!!!
!
!!
!!
!!
!!!!
!!
!!
!
!!
!!!
!!!
!
!!!!
!!!
!
!!!
!!!
!!
!!
!!
!!
!!
!
!!!!
!
!!
!!!
!
!!
!!
!!
!!!!
!!
!!
!
!!!
!
!!
!!!!
!!
!!
!!!!
!!
!!!
!!
!
!!
!
!!
!!!!
!!
!!
!
!!
!
!!
!!
!!
!
!
!!!
!!
!!!
!!!!
!
!!!
!!
!!
!
!!
!!!
!!!
!!!!
!
!!
!!!!
!!
!
!
!!!
!!
!! !
!!!!
!!
!!
!!
!!
!!
!
!!!
!!!
!!
!!!
!!
!!
!!!!!
!!
!!!
!
!!!
!!!
!
!!
!!!!
!!
!!!
!!!!
!
!!!
!
!!
!!
!!!!
!
!!
!!!
!
!
!!!!!
!
!!!!
!!!
!
!!!
!!!
!!
!!!
! !
!!!
!!!
!!!!
!!
!!
!!!
!!!
!
!!!
!!!
!!
!!!!
!!
!!!
!!
!!
!
!!
!!
!!
!! !
!!
!!
!!
!!!
!
!!!
!!!
!
!
!!!!
!
!
!!!!
!!
!
!
!!
!
!!
!
!!!!
!
!
!!
!!
!!
!!
!
!!!!
!!
!
!!
! !!
!!
!
!! !
!!!
!
!!!
!!!!!
!!
!
!!
!!
!!!
!!
!!!!
!!!
!!
!
!!
!!
!!
!!
!
!!!
!!!
!
!!!
!
!!
!
!
!!!
!!
!!
!!
!!
!
!!
!!
!!!!
!!
!
!!!!!!!
!
!!!
!!!
!!
!
!!!
!!!
!
!!!!
!!!
!
!!!
!! !
!!
!
!!!
!!!!!
!
!!
!
!!!
!!!
!
!!!
!!
!!
!!
!!
!!
!!!
!
!!! !
!
!!!!
!!
!!
!!
!
!!
!!!
!!!
!!
!!!
!!!
!!
!!!!
!
!!
!!!!
!!!
!
!!
!!
!!
!!
!
!!
!!
!
!!
!
!!
!
!!!!!!
!!!
!!
!! !!
!!
!!
!
!
!
!!!!!
!
!!!!
!
!!
!!
!!
!!
!!
!!
!
!!
!!
!!
!
!!!
!
!!
!!!
!!
!!
!!
!!!!
!!
!!
!!!!!
!!!!
!!!!
!!!!
!!
!!!!
!
!!!
!
!!!
!!
!!
! !
!
!!!
!
!!!
!!!!
!!!
!!!
!!!
!!
!!
!
!
!
!!
!!!
!!
!
!!!!
!
!
!
!!
!
!!
!!!
!!
!!!!
!!!
!
!!
!
!!!
!!
!
!!
!!
!!
!!
!!
!
!!
!!
!!!
!
!!
!!
!
!!
!!!!!!
!!
!!!
!
!
!!
!!!
!
!!!
!
!
!!
!!
!!!!!
!!!!!
!!!!!
!
!!!
!!
!
!!
!!!!!
!
!!
!!
!
!!!
!
!!!
!!
!!!
!!!
!!!
!
!!!!!
!!
!
!!!
!
!
!
!!
!!!!
!
!!!
!!
!
!!!
!!
!! !!
! !
! !
!!!
!
!!
!!!
!!
!!!
!
!!!!!
!!!
!!
!!!
!!
!!
!!!
!!!
!!!!
!!!!
!
!!
!!! !
!!!
!
!
!!
!!
!!!
!
!!
!!
!!!
!
!!!
!!!
!!
!!!!
!!
!
!!
!!
!!
!!
!!
!!!
!!!!
!!
!!!
!!
!
!!
!!!
!
!!
!!
!!!
!
!!!
! !
!
!!
!
!!
!!
!!!
!
!!!
!
!!!!
!
!!
! !
!!!
!
!!
!!!
!!!
!!!
!!
!!!!
!!!
!
!!!
!
!
!
!
!!!!
!!
!!!
!!!
!!
!!
!!
!!!
!!
!
!!
!!!
!!
!!
!!
!
!!
!!
!!
!!!
!!
!!!
!!!!
!!
!!
!!!
!!
!
!! !
!!!
!!!
!!
!!
!
!
!!
!!!!
!!!!
!!!
!
!!
!!
!!!
!!!
!!!
!!
!!!
!
!!
!
!
!!!
!
!!
!
!!
!!
!!!!
!!
!!!!!
!
!!!
!!
!
!!!!
!!!!
!!
!!
!
!!!!
!
!!
!!!
!!
!!!!
!!!
!
!!!!
!!!!
!! !!
!
!!
!!
!
!!!!!!
!
!!!
!!
!!
!!
!!
!!
!!
!!!
!!!!!!
!
!
!!
!!
!!
!
!!
!!
!!
!
!!!
!
!! !
!!!!!
!!!
!
!!
!!!
!!!
!
!!
!!
!!!
!!
!
!!!!
!!!
!!
!!!!
!
!!
!
!!
!
!!
!
!
!!
!!!
!!!
!!!!!
!
!!
!!
!
!!
!
!!
!!
!!!!
!!!
!
!!!
!
!!
!
!!
!!
!!!!
!
!!!!
!
!!
!
!
!!!!
!
!!
!!!
!
!!
!!
!
!!
!!!!
!
!!!
!
!!!
!!!
!!
!
!!!!
!
!!
!
!!
!
!!!
!!
!!
!!
!
!!!
!!
!!
!!
!!
!!!
!!
!
!!!
!
!!
!!
!!
!
!!
!
!!
!!!
!
!!!
!!
!!
!!
!!!!
!!!
!!!!
!!!!!!
!!
!!!
!
!!
!
!!
!
!!
!!
!!
!!
!!
!!
!!
!!
!!
!!
!!
!!!
!!!
!!!
!!!
!
!!
!!!
!!
!!
!!!!
!!
!!
!!
!!!
!
!!
!!
!!
!!
!!
!!
!
!!
!
!!
!!!
!
!!!!
!
!!
!
!!
!!
!!!!
!
!
!!!
!
!!!!
!!
!
!!!!!
!!!!!
!!
!!
!!
!!!!
!!
!!
!
!!!
!!
!!!!!
!!!
!!
!!!!
!!!!
!!!!!
!!!!
!!
! !
!
!!!
!!
!!
!
!
!!!!
!!
!!!
!!
!!!
!
!
!!!!
!!
!!
!!!
!
!!
!!
!!
!!!
!
!!
!!
!
!!!
!!!
!!!
!!!
!
!!!
!!
!!!!
!!!
!
!
!!
!!
!
!
!
!!!!
!!
!
!
!!
!!
!!
!
!
!!
!!
!!
!!!
!!
!!!!
!!!
!
!!!
!!
!
!
!!
!!
!!
!
!
!!!
!
!
!!
!!
!!
!!
!!
!
!!
!
!!!
!!!
!
!!
!
!
!!!!
!
!!!
!
!!
!
!!!
!
!!
!!
!
!
!!!
!!
!
!!
!
!
!!
!!!!
!!
! !!!
!
!!
!!
!!
!!
!!
!!
!!
!!!
!
!!
!!!
!!
!!!
!!!
!!
!!!
!!
!!!!
!
!!
!
!!
!!!
!
!!!
!
!!
!!
!
!!
!
!
!!
!!
!!
!!
!!!
!!!!
!!!!
!!
!!!
!!!!
!!!
!! !
!
!!
!!
!!!
!!
!!
!!
!
!
!!
!
! !
!!
!
!!!! !!
!!
!
!
!
!!
!!
!!
!!
!!!!!
!!
!!
!
!!
!!!
!!!!
!!
!
!
!!
!!!
!
!!
!
!!!!
!!
!!!
!!
!
!!!
!
!!!
!!
!!
!!!!
!!!
!
!!
!!!
!!
!!
!
!!!
!!
!!
!!!
!! !
!!
!!
!
!!!
!!
!
!!!!
!
!!!
!!!!
!
!!
!!
!!
!!!
!!!!!
!
!
!!
!!!
!!!
!!
!!
!!
!!
!
!!!
!!!!
!!!
!!
!!
!!
!!!
!!
!
!!!
!
!!
!!!!
!
!
!! !!
!!
!
!!
!
!!
!!!!
!!
!!
!!
!!
! !!!!!
!!
!
!!
!!
!
!!!
!!
!
!!!
!!
!!
!!!!
!!!!!
!
!!! !
!!!!
!!
!!
!!
!!
!!!
!!
!!!
!!
!!!
!!
! !!!
!
!!
!
!!!!!
!!
!!
!
!! !
!!!!
!
!!!
!!
!!!
!!!
!!!!
!
!
!!!! !!
!!
!!!
!!
!!
!
!!!
!!!!!
!
!!!
!!
!!!
!!!
!!
!!!
!! !
!!
!!
!
!
!!
!!
!!!
!
!!
!!
!!!
!!
!! !
! !
!!!
!
!!
!!
!!
!!
!!
!
!! !!!
!!
!!!!
!!!
!
!!
!!
!!
!
!!!
!
!! !
!
!!
!!
!!
!!
!
!!!
!!!!!
!!!!
!!! !
!!!
!
!
!!
!!!
!
!!!!
!!
!!!
!!!
!!
!
!!
!!
!!!
!!
!!
!
!!!
!!!!
!!!!
!
!
!!!
!!
!!
!!
!!
!!
!!!
!!
!!!
!
!
!!
!!!
!!!!
!
!!! !
!
!!!
!!
!
!!!!
!!
!
!!
!
!!
!
!!!!
!
!!
!!
!!!
!!!!
! !!
!!
!!
4-Seam Fastball 2-Seam Fastball Changeup Slider Curveball
Black Red Green Blue Light Blue
zito.pdf (from a talk by Michael Pane) [k-means clusters that classify Barry Zito’s base-
ball pitches. Here we discover that there really are distinct classes of baseball pitches.]
k-Means Clustering aka Lloyd’s Algorithm (Stuart Lloyd, 1957)
Goal: Partition npoints into kdisjoint clusters.
Assign each sample point Xia cluster label yi∈[1,k].
Cluster i’s mean isµi=1
niX
yj=iXj, given nipoints in cluster i.
Find ythat minimizeskX
i=1X
yj=iXj−µi2.[Sum of squared distances from points to their cluster means.]
NP-hard. Solvable in O(nkn) time. [Try every partition.]
k-means heuristic: Alternate between
(1)yj’s are fixed; update µi’s
(2)µi’s are fixed; update yj’s
Halt when step (2) changes no assignments.
[So, we have an assignment of points to clusters. We compute the cluster means. Then we reconsider the
assignment. A point might change clusters if some other’s cluster’s mean is closer than its own cluster’s
mean. Then repeat.]
The Singular Value Decomposition; Clustering 129
Step (1): One can show (calculus) the optimal µiis the mean of the points in cluster i.
[This is easy calculus, so I leave it as a short exercise.]
Step (2): The optimal yassigns each point Xjto the closest mean µi.
[If there’s a tie, and one of the choices is for Xjto stay in the
same cluster as the previous iteration, always take that choice.]
[. . . so both steps minimize the cost function, but they don’t optimize all the variables at once.]
2means.png [An example of 2-means. Odd-numbered steps reassign the data points.
Even-numbered steps compute new means.]
4meansanimation.gif [This is an animated GIF of 4-means with many points. Unfortu-
nately, the animation doesn’t work in the PDF lecture notes.]
130 Jonathan Richard Shewchuk
Both steps decrease objective fn unless they change nothing.
[Therefore, the algorithm never returns to a previous assignment.]
Hence alg. must terminate. [As there are only finitely many assignments.]
[This argument says that Lloyd’s algorithm never loops forever. But it doesn’t say anything optimistic about
the running time, because we might see O(kn) different assignments before we halt. In theory, one can
actually construct point sets in the plane that take an exponential number of iterations, but those don’t come
up in practice.]
Usually very fast in practice. Finds a local minimum, often not global.
[. . . which is not surprising, as this problem is NP-hard.]
4meansbad.png [An example where 4-means clustering fails.]
Getting started:
– Forgy method: choose krandom sample points to be initial µi’s; go to (2).
– Random partition: randomly assign each sample point to a cluster; go to (1).
–k-means ++: like Forgy, but biased distribution.
[Eachµiis chosen with a preference for points far from previous µi’s.]
[k-means ++is a little more work, but it works well in practice and theory. Forgy seems to be better than
random partition, but Wikipedia mentions some variants of k-means for which random partition is better.]
For best results, run k-means multiple times with random starts.
320.9 235.8 235.8
235.8 235.8 310.9
kmeans6times.pdf (ISL, Figure 10.7) [Clusters found by running 3-means 6 times on the
same sample points, each time starting with a di fferent random partition. The algorithm
finds three di fferent local minima.]
The Singular Value Decomposition; Clustering 131
[Why did we choose that particular objective function to minimize? Partly because it is equivalent to mini-
mizing the following function.]
Equivalent objective fn: the within-cluster variation
Find ythat minimizeskX
i=11
niX
yj=iX
ym=iXj−Xm2
[At the minimizer, this objective function is equal to twice the previous one. It’s a worthwhile exercise to
show that—it’s harder than it looks. The nice thing about this expression is that it doesn’t include the means;
it’s a function purely of the sample points and the clusters we assign them to. So it’s more compelling.]
Normalize the data? [before applying k-means]
Same advice as for PCA. Sometimes yes, sometimes no.
[If some features are much larger than others, they will tend to dominate the Euclidean distance. So if you
have features in di fferent units of measurement, you probably should normalize them. If you have features
in the same unit of measurement, you usually shouldn’t, but it depends on context.]
[One di fficulty with k-means is that you have to choose the number kof clusters before you start, and there
isn’t any reliable way to guess how many clusters will best fit the data. The next method, hierarchical
clustering, has the advantage in that respect. By the way, there is a whole Wikipedia article on “Determining
the number of clusters in a data set.”]
Hierarchical Clustering
Creates a tree; every subtree is a cluster.
[So some clusters contain smaller clusters.]
Bottom-up, aka agglomerative clustering :
start with each point a cluster; repeatedly fuse pairs.
Top-down, aka divisive clustering :
start with all pts in one cluster; repeatedly split it.
[When the input is a point set, agglomerative clustering is used much more in practice than divisive cluster-
ing. But when the input is a graph, it’s the other way around: divisive clustering is more common.]
132 Jonathan Richard Shewchuk
We need a distance fn for clusters A,B:
complete linkage :d(A,B)=max{d(w,x) :w∈A,x∈B}
single linkage : d(A,B)=min{d(w,x) :w∈A,x∈B}
average linkage : d(A,B)=1
|A||B|P
w∈AP
x∈Bd(w,x)
centroid linkage :d(A,B)=d(µA,µB) where µSis mean of S
[The first three of these linkages work for any distance function, even if the input is just a matrix of distances
between all pairs of sample points. The centroid linkage only really makes sense if we’re using the Euclidean
distance.]
Greedy agglomerative alg.:
Repeatedly fuse the two clusters that minimize d(A,B)
Naively takes O(n3) time.
[But for complete and single linkage, there are more sophisticated algorithms called CLINK and SLINK,
which run in O(n2) time. A package called ELKI has publicly available implementations.]
Dendrogram : Illustration of the cluster hierarchy (tree) in which the vertical axis encodes all the linkage
distances.
0 2 4 6 8 10
0 2 4 6 8 10
0 2 4 6 8 10
dendrogram.pdf (ISL, Figure 10.9) [Example of a dendrogram cut into 1, 2, or 3 clusters.]
Cut dendrogram into clusters by horizontal line according to your choice of # of clusters OR intercluster
distance.
[It’s important to be aware that the horizontal axis of a dendrogram has no meaning. You could swap some
treenode’s left subtree and right subtree and it would still be the same dendrogram. It doesn’t mean anything
that two leaves happen to be next to each other.]
The Singular Value Decomposition; Clustering 133
Average Linkage Complete Linkage Single Linkage
linkages.pdf (ISL, Figure 10.12) [Comparison of average, complete (max), and single
(min) linkages. Observe that the complete linkage gives the best-balanced dendrogram,
whereas the single linkage gives a very unbalanced dendrogram that is sensitive to outliers
(especially near the top of the dendrogram).]
[Probably the worst of these is the single linkage, because it’s very sensitive to outliers. Notice that if you
cut this example into three clusters, two of them have only one sample point. It also tends to give you a very
unbalanced tree.]
[The complete linkage tends to be the best balanced, because when a cluster gets large, the farthest point in
the cluster is always far away. So large clusters are more resistant to growth than small ones. If balanced
clusters are your goal, this is your best choice.]
[In most applications you probably want the average or complete linkage.]
Warning: centroid linkage can cause inversions where a parent cluster is fused at a lower height than its
children.
[So statisticians don’t like it, but nevertheless, centroid linkage is popular in genomics.]
[As a final note, all the clustering algorithms we’ve studied so far are unstable, in the sense that deleting
a few sample points can sometimes give you very di fferent results. But these unstable heuristics are still
the most commonly used clustering algorithms. And it’s not clear to me whether a truly stable clustering
algorithm is even possible.]
134 Jonathan Richard Shewchuk
22 The Pseudoinverse; Better Generalization for Neural Nets
THE PSEUDOINVERSE AND THE SVD
[We’re done with unsupervised learning. For the rest of the semester, we go back to supervised learning.]
[The singular value decomposition can give us insight into the pseudoinverse and its use in least-squares
linear regression. If you attended Discussion Section 6, you worked through an explanation of this, but now
that I’ve introduced the compact SVD in Lecture 21, I’d like to summarize it.]
LetXbe any n×dmatrix. Let X=UDV⊤be its compact SVD. Let r=rank X.
Recall that U∈Rn×r,D∈Rr×ris diagonal & invertible, V∈Rd×r,U⊤U=I,V⊤V=I.
The Moore–Penrose pseudoinverse ofXisX+=VD−1U⊤. It’s d×n.
[This is a better pseudoinverse than the one I defined in Lecture 10, not least because it’s always defined.]
Observe:
(1)XX+=UU⊤, which is symmetric. Proof: XX+=UDV⊤VD−1U⊤=UDD−1U⊤=UU⊤.
(2)X+X=VV⊤. [The proof is analogous to (1).]
(3) If r=n, then XX+=In×nandX+is a right inverse . Proof: Uis square, U⊤U=I→UU⊤=I; use (1).
(4) If r=d, then X+X=Id×dandX+is a left inverse . [The proof is analogous to (3) and uses (2).]
(5) By (3), ifXis invertible ( r=n=d),X+=X−1. [The pseudoinverse is the inverse when one exists.]
(6) These are compact SVDs: X+=VD−1U⊤,X⊤=VDU⊤, (X+)⊤=UD−1V⊤.
[If a factorization has the form of a compact SVD, it isa compact SVD.]
X+is like X⊤with the nonzero singular values inverted.
(7) Given a compact SVD X=UDV⊤, null X=nullV⊤.
Proof: V⊤w=0→Xw=UDV⊤w=0→D−1U⊤UDV⊤w=0→V⊤w=0.
(8) By (6) & (7), null X+=nullU⊤=nullX⊤and null ( X+)⊤=nullV⊤=nullX.
So row X+=colXand col X+=row X. X+has the same four fundamental subspaces as X⊤.
(9) (1) & (2) give eigendecompositions: XX+=[U U null]hIr×r0
0 0ihU⊤
U⊤
nulli
,X+X=[V V null]hIr×r0
0 0ihV⊤
V⊤
nulli
.
[UnullandVnullhave orthonormal column vectors spanning the null spaces of XX+andX+X.]
(10) By (8) & (9), all have rank r:X,U,V,X+,XX+,X+X.
(11) By (9), every w∈colUis an eigenvector of XX+with eigenvalue 1; all other eigenvalues are 0.
As col U=colX,XX+is identity map on col X. [Symmetrically,] X+Xis identity map on row X.
[In summary, the psuedoinverse is as close to an inverse of Xas anything can be. Let’s visualize what the
pseudoinverse does. When you apply Xto a vector in row X, you get a vector in col X; then when you
apply X+to the result, you get the original vector back.]
u1u2X+X colX
Xv1=δ1u1
X+(δ1u1)=v1rowX
v2
v1
rowcol.pdf [The singular vectors are perpendicular, but we are viewing the planes from oblique angles.]
The Pseudoinverse; Better Generalization for Neural Nets 135
[If we think of Xas a linear function that maps row Xto col X, and we ignore the other dimensions of Rd
andRn, then that linear function is a bijection. The inverse of that bijection is the pseudoinverse X+.]
Linear function f: row X→colX,p7→Xpis a bijection.
Its inverse is f−1: col X→rowX,q7→X+q.
Therright singular vectors viare an orthonormal basis for row X.
Therleft singular vectors uiare an orthonormal basis for col X.
Xvi=δiui. X+ui=1
δivi.
[Xmaps each right singular vector to some scalar multiple of the corresponding left singular vector. The
corresponding singular value tells us how much longer the vector gets when we map it. X+maps each left
singular vector to some scalar multiple of a right singular vector.]
[Usually we don’t think of Xas a function from row space to column space. Usually we think of Xas a
function from some bigger space Rdto a bigger space Rn. In our figure above, Xmight be a 4×3 matrix, but
its rank is only two. Then Xisn’t a bijection any more, and neither is its pseudoinverse X+. SoXmaps every
point in R3to a point on the plane col X. When you map a three-dimensional space down to two dimensions,
it can’t be a bijection, so Xdoesn’t have an inverse. Just a pseudoinverse.]
[You can think of Xas a function that orthogonally projects a three-dimensional point down onto the row
space of X, then uses the bijection above to finish the mapping. Symmetrically, you can think of X+as a
function that orthogonally projects a four-dimensional point down onto the column space of X, then uses the
inverse bijection. Here’s an illustration of mapping ptoXpandqtoX+q.]
u1u2pq
colX
X+qXprowX
v2
v1
rowcolpr.pdf
[With the compact SVD, we can show that the pseudoinverse always gives a solution to least-squares linear
regression, even when X⊤Xis singular.]
Theorem: A solution to the normal equations X⊤Xw=X⊤yisw=X+y.
Proof: X⊤Xw=X⊤XX+y=VDU⊤UDV⊤VD−1U⊤y=VD2D−1U⊤y=VDU⊤y=X⊤y.
If the normal eq’ns have multiple solutions, w=X+yis the least-norm solution ; i.e., it minimizes ∥w∥among
all solutions. [If you attended Discussion Section 6, you might have proven this yourself.]
[This way of solving the normal equations is very helpful when X⊤Xis singular because n<dor the
sample points lie on a subspace of the feature space. But observe that if Xhas a very small singular value,
the reciprocal of that singular value will be very large and have a very large e ffect on w; but when that
singular value is exactly zero, it has no e ffect on w! So when we have a really tiny singular value, should we
pretend it is zero? Ridge regression implements this policy to some degree; review Discussion Worksheet 12
for details.]
136 Jonathan Richard Shewchuk
BETTER GENERALIZATION FOR NEURAL NETWORKS
[Classic methods for preventing overfitting, such as subset selection, ℓ2regularization, and ensembles of
learners, sometimes help neural networks to generalize better to points they haven’t been trained on.]
(1) Get more data. [This is the best method. Andrej Karpathy writes that “It is a very common mistake to
spend a lot of engineering cycles trying to squeeze juice out of a small dataset when you could instead be
collecting more data.”]
(2) Data augmentation. Augment data set with modified versions of training points.
258 9. REGULARIZATION
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
Figure 9.1 Illustration of data set augmentation, showing (a) the original image, (b) horizontal inversion, (c)
scaling, (d) translation, (e) rotation, (f) brightness and contrast change, (g) additive noise, and (h) colour shift.
An example of approach 2 is the technique of tangent propagation (Simard et al. ,
1992) in which a regularisation term is added to the error function during training.
This term directly penalizes changes in the output resulting from changes in the input
variables that correspond to one of the invariant transformations. A limitation of this
technique, in addition to the extra complexity of training, is can only cope with small
transformations (e.g., translations by less than a pixel).
Approach 3 is known as data set augmentation . It is often relatively easy to
implement and can prove to be very effective in practice. It is often applied in the
context of image analysis as it straightforward to create the transformed training data.
Figure 9.1 shows examples of such transformations applied to an image of a cat.
For medical images of soft tissue, data augmentation could also include continuous
‘rubber sheet’ deformations (Ronneberger, Fischer, and Brox, 2015).
For sequential training algorithms, such as stochastic gradient descent, the data
set can be augmented by transforming each input data point before it is presented
to the model so that, if the data points are being recycled, a different transformation
(drawn from an appropriate distribution) is applied each time. For batch methods, a
similar effect can be achieved by replicating each data point a number of times and
transforming each copy independently.
We can analyse the effect of using augmented data by considering transforma-
tions that represent small changes to the original examples and then making a Taylor
expansion of the error function in powers of the magnitude of the transformation
(Bishop, 1995c; Leen, 1995; Bishop, 2006). This leads to a regularized error func-
tion in which the regularizer penalizes the gradient of the network output with respect
augmentation.pdf, (Bishop, Figure 9.1) [Examples of data augmentation applied to an
original image (a). (b) Reflection. (c) Scaling. (d) Translation. (e) Rotation. (f) Changing
brightness and contrast. (g) Added noise. (h) Color shift.]
[You can see that these augmentations do not change the fact that the image should be classified as a cat.]
MethodBaseline
Cutout
Mixup
CutMix
PIXMIX
Corruptions
mCE ( #)50.0
+0.051.5
+1.548.0
 2.051.5
+1.530.5
 19.5
Adversaries
Error ( #)96.5
+0.098.5
+1.097.4
+0.997.0
+0.592.9
 3.9
Consistency
mFR ( #)10.7
+0.011.9
+1.29.5
 1.212.0
+1.35.7
 5.0
Calibration
RMS Error ( #)31.2
+0.031.1
 0.113.0
 18.129.3
 1.88.1
 23.0
Anomaly Detection
AUROC ( ")77.7
+0.074.3
 3.471.7
 6.074.4
 3.389.3
+11.6
Table 1. P IXMIXcomprehensively improves safety measures, providing signiﬁcant improvements over state-of-the-art baselines. We
observe that previous augmentation methods introduce few additional sources of structural complexity. By contrast, P IXMIXincorporates
fractals and feature visualizations into the training process, actively exposing models to new sources of structural complexity. We ﬁnd that
PIXMIXis able to improve both robustness and uncertainty estimation and is the ﬁrst method to substantially improve all existing safety
measures over the baseline.
that existing help with some safety metrics but harm oth-
ers. This raises the question of whether improving all safety
measures is possible with a single model.
While previous augmentation methods create images
that are different (e.g., translations) or more entropic (e.g.,
additive Gaussian noise), we argue that an important under-
explored axis is creating images that are more complex. As
opposed to entropy or descriptive difﬁculty, which is max-
imized by pure noise distributions, structural complexity is
often described in terms of the degree of organization [ 28].
A classic example of structurally complex objects is frac-
tals, which have recently proven useful for pretraining im-
age classiﬁers [ 22,35]. Thus, an interesting question is
whether sources of structural complexity can be leveraged
to improve safety through data augmentation techniques.
We show that Pareto improvements are possible with
PIXMIX, a simple and effective data processing method
that leverages pictures with complex structures and sub-
stantially improves all existing safety measures. P IXMIX
consists of a new data processing pipeline that incorpo-
rates structurally complex “dreamlike” images. These
dreamlike images include fractals and feature visualiza-
tions. We ﬁnd that feature visualizations are a suit-
able source of complexity, thereby demonstrating that theyhave uses beyond interpretability. In extensive experi-
ments, we ﬁnd that P IXMIXprovides substantial gains on
a broad range of existing safety measures, outperform-
ing numerous previous methods. Code is available at
github.com/andyzoujm/pixmix .
2. Related Work
Robustness. Out-of-distribution robustness considers
how to make ML models resistant to various forms of
data shift at test time. Geirhos et al., 2019 [ 11] uncover
a texture bias in convolutional networks and show that
training on diverse stylized images can improve robustness
at test-time. The ImageNet-C(orruptions) benchmark [ 15]
consists of diverse image corruptions known to track
robustness on some real world data shifts [ 13]. ImageNet-C
is used to test models that are trained on ImageNet [ 7]
and is used as a held-out, more difﬁcult test set. They
also introduce ImageNet-P(erturbations) for measuring
prediction consistency under various non-adversarial
input perturbations. Others have introduced additional
corruptions for evaluation called ImageNet- C[33]. The
ImageNet-R(enditions) benchmark measures performance
degradation under various renditions of objects including
paintings, cartoons, grafﬁti, embroidery, origami, sculp-
pixmix.pdf, (Hendrycks et al., “PixMix”, 2022) [More varieties of data augmentation.]
[Hendrycks et al. note that “For state-of-the-art models, data augmentation can improve clean accuracy [on
the test set] comparably to a 10 ×increase in model size. Further, data augmentation can improve out-of-
distribution robustness [on images from a distribution di fferent than the training set] comparably to a 1,000 ×
increase in labeled data.”]
[One point they make is that while adding Gaussian noise is one augmentation that helps improve gener-
alization to new images, it’s even more e ffective to add artifacts that stimulate hidden units, such as the
hidden units that detect edges in an image. So their augmentation methods mix images with other images
that introduce spurious structure, not just Gaussian noise.]
The Pseudoinverse; Better Generalization for Neural Nets 137
(3) Subset selection. [Recall Lecture 13.]
(4)ℓ2regularization, aka weight decay .
Addλ∥w∥2to the cost /loss fn, where wis vector of all weights in network.
[wincludes all the weights in all the weight matrices, rewritten as a vector.]
[We regularize for the same reason we do it in ridge regression: we suspect that overly large weights are
spurious.]
[With a neural network, it’s not clear whether penalizing the bias terms is bad or good. Penalizing the
bias terms has the e ffect, potentially positive, of drawing each ReLU or sigmoid unit closer to the center of
its nonlinear operating region. I would suggest to try both ways and use validation to decide whether you
should penalize the bias terms or not. Also, you could try using a di fferent hyperparameter for the bias terms
than theλyou use for the other weights.]
Effect: step ∆wi=−ϵ∂J
∂wihas extra term−2ϵλwi
Weight widecays by factor 1 −2ϵλif not reinforced by training.
Neural Network - 10 Units, No Weight Decay
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.100
Test Error:       0.259
Bayes Error:    0.210
Neural Network - 10 Units, Weight Decay=0.02 
. . . . . . . . . . . . . . . 
Neural Network - 10 Units, Weight Decay=0.02 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.160
Test Error:       0.223
Bayes Error:    0.210
weightdecayo ff.pdf, weightdecayon.pdf (ESL, Figure 11.4) Write “10 hidden units +soft-
max+cross-entropy loss.” [Examples of 2D classification without (left) and with (right)
weight decay. Observe that in the second example, the decision boundary (black) better
approximates the Bayes optimal boundary (dashed purple curve).]
[AlexNet is a famous example of a network that used both ℓ2regularization and momentum. Just add the
ℓ2penalty to the cost function Jand plug that cost function into the momentum algorithm from Lecture 18.
AlexNet set λ=0.0005 and the momentum decay term to β=0.9. They adjusted ϵmanually throughout
training.]
(5) Train for a very long time. [Andrej Karpathy: “I’ve often seen people tempted to stop the model
training when the validation loss seems to be leveling o ff. In my experience networks keep training for an
unintuitively long time. One time I accidentally left a model training during the winter break and when I got
back in January it was SOTA (“state of the art”).9]
9http://karpathy.github.io /2019/04/25/recipe /
138 Jonathan Richard Shewchuk
(6) Ensemble of neural nets. Random initial weights +SGD +(optionally) bagging.
[Ensembles work well for neural nets, reportedly improving test accuracy by 2–3%. Random initial weights
and random minibatches ensure that each neural net finds a di fferent local minimum. If you have finite
training data, validate to see if bagging helps or not. Obviously, ensembles of neural nets are very slow.]
For speed, sometimes the ensembles share the same early layers.
[Then only the last layers of each neural network are trained separately.]
(7) Dropout emulates an ensemble in one network.
dropout1.pdf, dropout2.pdf
During training, temporarily disable a random subset of the units, along with all edges in and out.
– No forward signal, no weight updates for edges in or out of disabled unit.
– Disable each hidden unit with probability (typically) 0 .5.
– Disable each input unit with probability (typically) 0 .2.
– Disable a different random subset for each SGD minibatch.
After training, before testing: enable all units. If units in a layer were disabled with probability p, multiply
all edge weights out of that layer by p.
[When we disabled units, the edge weights had to grow large to make up for their disabled neighbors. At
test time, all units and edges are enabled, so we have to reduce the weights to compensate.]
[Dropout gives an e ffect similar to averaging over multiple neural networks, but it’s faster to train. Dropout
usually gives better generalization than ℓ2regularization. Geo ffHinton and his co-authors give an example
where they trained a network to classify MNIST digits. Their network without dropout had a 1.6% test error;
it improved to 1.3% with dropout on the hidden units only; and further improved to 1.1% with dropout on
the input units too.]
[Recall Karl Lashley’s rat experiments, where he tried to make rats forget how to run a maze by introduc-
ing lesions in their cerebral cortexes, and it didn’t work. He concluded that the knowledge is distributed
throughout their brains, not localized in one place. Dropout is a way to force neural networks to distribute
knowledge throughout the weights.]
Double Descent
[Early neural network researchers sometimes struggled with their networks falling into bad local minima
and failing to achieve low training errors. But with experience and greater computational power, we’ve
discovered that these problems can usually be solved simply by adding more units to every hidden layer. We
call this “making the network wider.” Sometimes you also have to add more layers. But if your layers are
wide enough, and there are enough of them, a well-designed neural network can typically output exactly the
correct label for every training point, which implies that you’re at a global minimum of the cost function.]
The Pseudoinverse; Better Generalization for Neural Nets 139
Hidden layers are wide enough +numerous enough ⇒network output can interpolate the label (ˆ y=y) for
every training pt⇒find global minimum of cost fn.
[I have pointed out that if you use sigmoid or softmax output units, you can’t set the labels to exactly 1 or 0
and achieve interpolation, as sigmoid and softmax outputs are strictly between 0 and 1; but with labels like
0.1 and 0.9, interpolating the labels is a realistic goal! And the linear output units used for regression can
interpolate arbitrary numbers. Bottom line: if you fall into a bad local minimum, your network is too small.]
[One reason it took so long to make this discovery is that researchers believed that having too many weights
in a neural network would cause overfitting. It turns out that’s only half true. Empirically, we sometimes
observe a phenomenon called “double descent,” illustrated below.]
DEEPDOUBLE DESCENT :
WHERE BIGGER MODELS AND MORE DATAHURT
Preetum Nakkiran⇤
Harvard UniversityGal Kaplun†
Harvard UniversityYamini Bansal†
Harvard UniversityTristan Yang
Harvard University
Boaz Barak
Harvard UniversityIlya Sutskever
OpenAI
ABSTRACT
We show that a variety of modern deep learning tasks exhibit a “double-descent”
phenomenon where, as we increase model size, performance ﬁrst gets worse and
then gets better. Moreover, we show that double descent occurs not just as a
function of model size, but also as a function of the number of training epochs.
We unify the above phenomena by deﬁning a new complexity measure we call
theeffective model complexity and conjecture a generalized double descent with
respect to this measure. Furthermore, our notion of model complexity allows us to
identify certain regimes where increasing (even quadrupling) the number of train
samples actually hurts test performance.
1I NTRODUCTION
Figure 1: Left: Train and test error as a function of model size, for ResNet18s of varying width
on CIFAR-10 with 15% label noise. Right: Test error, shown for varying train epochs. All models
trained using Adam for 4K epochs. The largest model (width 64) corresponds to standard ResNet18.
Thebias-variance trade-off is a fundamental concept in classical statistical learning theory (e.g.,
Hastie et al. (2005)). The idea is that models of higher complexity have lower bias but higher vari-
ance. According to this theory, once model complexity passes a certain threshold, models “overﬁt”
with the variance term dominating the test error, and hence from this point onward, increasing model
complexity will only decrease performance (i.e., increase test error). Hence conventional wisdom
in classical statistics is that, once we pass a certain threshold, “larger models are worse. ”
However, modern neural networks exhibit no such phenomenon. Such networks have millions of
parameters, more than enough to ﬁt even random labels (Zhang et al. (2016)), and yet they perform
much better on many tasks than smaller models. Indeed, conventional wisdom among practitioners
is that “larger models are better’ ’ (Krizhevsky et al. (2012), Huang et al. (2018), Szegedy et al.
⇤Work performed in part while Preetum Nakkiran was interning at OpenAI, with Ilya Sutskever. We espe-
cially thank Mikhail Belkin and Christopher Olah for helpful discussions throughout this work. Correspondence
Email: preetum@cs.harvard.edu
†Equal contribution
1
doubledescent.pdf (Nakkiran et al., “Deep Double Descent”) [A classic double descent
curve (solid blue) for test error. The horizontal axis indicates the number of units in each
hidden layer of a residual neural network used for image recognition, and the vertical axis
measures the the test error (solid curve) and training error (dashed curve).]
[Consider the solid blue curve, showing the test error as the width of a network increases. The horizontal
axis is the number of units per hidden layer. As that number increases, at left the test error exhibits the
classic U-shaped bias-variance “tradeo ff.” But when we pass the point where the network is interpolating
the labels and continue to add more weights, we sometimes see a second “descent,” where the test error
starts to decrease again and ultimately gets even lower than before! The peak in the middle of the curve
tends to be larger when there is more noise in the labels. Observe that the test error continues to fall even
after the training error is zero. The takeaway is, “ bigger models are often better. ”]
[The currently accepted explanation for double descent, per Nakkiran et al., is that “at the interpolation
threshold . . . the model is just barely able to fit the training data; forcing it to fit even slightly-noisy or mis-
specified labels will destroy its global structure, and result in high test error. However for over-parameterized
models, there are many interpolating models that fit the training set, and SGD is able to find one that
‘absorbs’ the noise while still performing well on the distribution.”]
[Double descent has also been observed in decision trees and even in linear regression where we add random
features to the training points (thereby adding more weights to the linear regression model).]
140 Jonathan Richard Shewchuk
23 Residual Networks; Batch Normalization; AdamW
TRAINING DEEP NETWORKS
Most influential ideas: ResNets, batch normalization, layer normalization.
[These ideas enable deep networks to train. They reduce the likelihood of encountering the vanishing gradi-
ent or exploding gradient problems, but don’t quite eliminate them.]
Batch Normalization
[Batch normalization has played a huge role in making it easier to train very deep neural networks since its
introduction in 2015, and it’s still a mainstay today. It seems to make the cost function uglier, though; when
you can train a network without it, it might generalize better.]
Recall batch normalization from Homework 6: For a vector aof activations,
– a batch-norm layer learns parameters βiandγifor each activation ai;
– calculate the sample mean µofaand the sample variances σ2
iofaiover a minibatch;
– for some small ϵ, the layer outputs vector zwith zi=βi+γiai−µiq
σ2
i+ϵ.
[We didn’t say much in the homework about where batch normalization layers are used. There is some
disagreement over whether it’s best to place them before or after a nonlinear activation function. Both ways
are commonly used. The only clear rules are to never use batch normalization for outputs, and never use
ReLUs for inputs.]
ReLUsReLUs
batch normalizationbatch normalizationhidden units
hidden units
batchrelu.pdf [The original authors proposed the left version.]
[Batch normalization is often applied to image data in convolutional neural networks, but not quite like this.
We do not normalize each pixel separately. Remember that in a CNN, we rely heavily on relationships
between adjacent pixels. Normalizing each pixel separately could introduce spurious edges and ruin the
network’s ability to recognize images. But we can normalize an entire image as a whole, and we can
normalize each channel separately.]
Batch-norm for images: compute mean & variance over all images in minibatch AND all pixels in each
image. One βand oneγper channel .
Layer normalization : compute mean & variance over all hidden units (all pixels and all channels), but not
(necessarily) over images. One βand oneγper image .
[Layer normalization ensures that for any one image and any one layer of hidden units, the hidden unit
vector will lie on a sphere with center βand radiusγ. Layer normalization is easier to parallelize than batch
normalization, and it is particularly useful in recurrent neural networks.]
Residual Networks; Batch Normalization; AdamW 141
Residual Neural Networks (ResNets)
[Look at this famous figure depicting two-dimensional cross sections through the cost function of deep
convolutional neural networks. At right is the cost for a residual neural network. At left is the cost if we
remove the “residual connections” that characterize residual networks. You can guess which one is easier to
optimize.]
Visualizing the Loss Landscape of Neural Nets
Hao Li1, Zheng Xu1, Gavin Taylor2, Christoph Studer3, Tom Goldstein1
1University of Maryland, College Park2United States Naval Academy3Cornell University
{haoli,xuzh,tomg}@cs.umd.edu ,taylor@usna.edu ,studer@cornell.edu
Abstract
Neural network training relies on our ability to ﬁnd “good” minimizers of highly
non-convex loss functions. It is well-known that certain network architecture
designs (e.g., skip connections) produce loss functions that train easier, and well-
chosen training parameters (batch size, learning rate, optimizer) produce minimiz-
ers that generalize better. However, the reasons for these differences, and their
effects on the underlying loss landscape, are not well understood. In this paper, we
explore the structure of neural loss functions, and the effect of loss landscapes on
generalization, using a range of visualization methods. First, we introduce a simple
“ﬁlter normalization” method that helps us visualize loss function curvature and
make meaningful side-by-side comparisons between loss functions. Then, using
a variety of visualizations, we explore how network architecture affects the loss
landscape, and how training parameters affect the shape of minimizers.
1 Introduction
Training neural networks requires minimizing a high-dimensional non-convex loss function – a
task that is hard in theory, but sometimes easy in practice. Despite the NP-hardness of training
general neural loss functions [ 2], simple gradient methods often ﬁnd global minimizers (parameter
conﬁgurations with zero or near-zero training loss), even when data and labels are randomized before
training [ 42]. However, this good behavior is not universal; the trainability of neural nets is highly
dependent on network architecture design choices, the choice of optimizer, variable initialization, and
a variety of other considerations. Unfortunately, the effect of each of these choices on the structure of
the underlying loss surface is unclear. Because of the prohibitive cost of loss function evaluations
(which requires looping over all the data points in the training set), studies in this ﬁeld have remained
predominantly theoretical.
(a) without skip connections
 (b) with skip connections
Figure 1: The loss surfaces of ResNet-56 with/without skip connections. The proposed ﬁlter
normalization scheme is used to enable comparisons of sharpness/ﬂatness between the two ﬁgures.
32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.arXiv:1712.09913v3  [cs.LG]  7 Nov 2018
lossskip.pdf, (Hao Li et al., 2018)
Idea: design a network with layers that can easily represent the identity fn, e.g., when all weights are zero.
[There are two observations that help to motivate this idea.]
Motivation 1: Networks with nonlinear activations have great di fficulty representing the identity. Let’s fix
that.
[If a hidden layer has negative unit values, a subsequent hidden layer with ReLUs cannot replicate those
values. You might be able to replicate them at a linear output layer if you’re very clever. Instead, let’s
redesign our networks to make it easy.]
Motivation 2: Consider a linear neural network ˆy=WLWL−1···W1xwith square matrices Wi.
Given an n×ddesign matrix Xand an n×dlabel matrix Y, solve this linear regression problem by gradient
descent [batch or stochastic].
Find matrices that minimize J(WL,WL−1,..., W1)=∥WLWL−1···W1X⊤−Y⊤∥2
F.
The cost fn is very smooth around ( I,I,..., I), but much more complicated in regions close to (0 ,0,..., 0).
[There’s a paper showing that near the identity matrices, every critical point of the cost function is a global
minimum—much like in the figure at top right. So even a simple linear neural network su ffices to show
some of the behavior in the figure.]
Any network satisfying WLWL−1···W1=Y⊤(X⊤)+is a solution.
IfLis sufficiently large, there is a solution or approximate solution where each Wiis “close” to I.
[If you multiply together enough matrices that are close to the identity matrix, you can obtain any square,
invertible matrix, and you can get very close to any square matrix. So for su fficiently many layers, there are
solutions in the “nice” region of the cost function. SGD starting from the identity network will find them.]
142 Jonathan Richard Shewchuk
[Of course, you would never do linear regression this way. But when we add nonlinear activation functions
such as ReLUs between the matrices, the phenomenon persists that the cost function is ugly near the origin
and can be relatively nice where the network is computing an function near the identity function at each
layer.]
Takeaways:
– Initializing near I(plus small random weights) is better than initializing near zero.
– More layers makes it more likely there’s a solution in a “nice” part of the cost fn.
ResNets use residual connections aka skip connections to add hidden layer values to subsequent layers.
Networks are constructed by repeating one of these motifs.
activationshidden units
(fully-connected,
batch normalization,
etc.),
and hidden unitsnonlinear activations,convolutional,residual connectionh[i]residual connection
hidden unitsmost common motif today original ResNet motif
h[j]=h[i]+z[j]h[i]
z[j]
h[j]z[j]mix of linear layers
motifs.pdf
[The motif on the right was used by the original ResNet paper, with ReLU activations after the residual
connection. But the motif on the left seems to be more popular now.]
If all weights are zero, left motif sets h[j]=h[i]by default.
Right motif with ReLUs copies all positive units but zeros out negative ones.
[A big advantage of the motif on the left is that if it is advantageous to send some hidden unit values from
an early layer to a later layer unchanged, the network has the opportunity to learn to do that. The motif on
the right can do that with positive hidden unit values if it uses ReLU activations.]
If the “ideal” mapping from h[i]toh[j]is expressed by a function f, the left motif is trying to learn f(h)−h
(which we hope is small).
Residual Networks; Batch Normalization; AdamW 143
[The first ResNets were CNNs for image classification. The authors won first place in the 2015 ImageNet
Large Scale Visual Recognition Challenge with an ensemble of six ResNets, two of which had 152 layers.
This was the biggest advance in neural network vision performance since the AlexNet paper that changed
modern computer vision in 2012. Here is the building block for one of their smaller ResNets, a 34-layer
model. At right is the authors’ schematic of the whole ResNet-34 network with normalizations omitted.]
3×3convolutions
batch normalizationbatch normalization
3×3convolutions
ReLUsReLUs
64channels×56×56h[i+1]
a[i+2]
h[i+2]residual connection 64×64×3×3weightsResNet-34 building block64channels×56×56
64channels×56×56
64channels×56×56(two convolutional layers)64channels×56×56 a[i+1]64×64×3×3weightsh[i]
/uni0037/uni0078/uni0037/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni002C/uni0020/uni002F/uni0032/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0061/uni0076/uni0067/uni0020/uni0070/uni006F/uni006F/uni006C/uni0066/uni0063/uni0020/uni0031/uni0030/uni0030/uni0030/uni0069/uni006D/uni0061/uni0067/uni0065
/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032
/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032
/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032
/uni0066/uni0063/uni0020/uni0034/uni0030/uni0039/uni0036/uni0066/uni0063/uni0020/uni0034/uni0030/uni0039/uni0036/uni0066/uni0063/uni0020/uni0031/uni0030/uni0030/uni0030/uni0069/uni006D/uni0061/uni0067/uni0065/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0031/uni0031/uni0032/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0032/uni0032/uni0034
/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0035/uni0036
/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0032/uni0038
/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0031/uni0034
/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0037
/uni006F/uni0075/uni0074/uni0070/uni0075/uni0074/uni0020/uni0073/uni0069/uni007A/uni0065/uni003A/uni0020/uni0031/uni0056/uni0047/uni0047/uni002D/uni0031/uni0039 /uni0033/uni0034/uni002D/uni006C/uni0061/uni0079/uni0065/uni0072/uni0020/uni0070/uni006C/uni0061/uni0069/uni006E
/uni0037/uni0078/uni0037/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni002C/uni0020/uni002F/uni0032/uni0070/uni006F/uni006F/uni006C/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0036/uni0034/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0031/uni0032/uni0038/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0032/uni0035/uni0036/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni002C/uni0020/uni002F/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0033/uni0078/uni0033/uni0020/uni0063/uni006F/uni006E/uni0076/uni002C/uni0020/uni0035/uni0031/uni0032/uni0061/uni0076/uni0067/uni0020/uni0070/uni006F/uni006F/uni006C/uni0066/uni0063/uni0020/uni0031/uni0030/uni0030/uni0030/uni0069/uni006D/uni0061/uni0067/uni0065/uni0033/uni0034/uni002D/uni006C/uni0061/uni0079/uni0065/uni0072/uni0020/uni0072/uni0065/uni0073/uni0069/uni0064/uni0075/uni0061/uni006C
Figure 3. Example network architectures for ImageNet. Left: the
VGG-19 model [ 41] (19.6 billion FLOPs) as a reference. Mid-
dle: a plain network with 34 parameter layers (3.6 billion FLOPs).
Right : a residual network with 34 parameter layers (3.6 billion
FLOPs). The dotted shortcuts increase dimensions. Table 1shows
more details and other variants.Residual Network. Based on the above plain network, we
insert shortcut connections (Fig. 3, right) which turn the
network into its counterpart residual version. The identity
shortcuts (Eqn.( 1)) can be directly used when the input and
output are of the same dimensions (solid line shortcuts in
Fig.3). When the dimensions increase (dotted line shortcuts
in Fig. 3), we consider two options: (A) The shortcut still
performs identity mapping, with extra zero entries padded
for increasing dimensions. This option introduces no extra
parameter; (B) The projection shortcut in Eqn.( 2) is used to
match dimensions (done by 1 ⇥1 convolutions). For both
options, when the shortcuts go across feature maps of two
sizes, they are performed with a stride of 2.
3.4. Implementation
Our implementation for ImageNet follows the practice
in [21,41]. The image is resized with its shorter side ran-
domly sampled in [256 ,480] for scale augmentation [ 41].
A 224 ⇥224 crop is randomly sampled from an image or its
horizontal ﬂip, with the per-pixel mean subtracted [ 21]. The
standard color augmentation in [ 21] is used. We adopt batch
normalization (BN) [ 16] right after each convolution and
before activation, following [ 16]. We initialize the weights
as in [ 13] and train all plain/residual nets from scratch. We
use SGD with a mini-batch size of 256. The learning rate
starts from 0.1 and is divided by 10 when the error plateaus,
and the models are trained for up to 60⇥104iterations. We
use a weight decay of 0.0001 and a momentum of 0.9. We
do not use dropout [ 14], following the practice in [ 16].
In testing, for comparison studies we adopt the standard
10-crop testing [ 21]. For best results, we adopt the fully-
convolutional form as in [ 41,13], and average the scores
at multiple scales (images are resized such that the shorter
side is in {224,256,384,480,640}).
4. Experiments
4.1. ImageNet Classiﬁcation
We evaluate our method on the ImageNet 2012 classiﬁ-
cation dataset [ 36] that consists of 1000 classes. The models
are trained on the 1.28 million training images, and evalu-
ated on the 50k validation images. We also obtain a ﬁnal
result on the 100k test images, reported by the test server.
We evaluate both top-1 and top-5 error rates.
Plain Networks. We ﬁrst evaluate 18-layer and 34-layer
plain nets. The 34-layer plain net is in Fig. 3(middle). The
18-layer plain net is of a similar form. See Table 1for de-
tailed architectures.
The results in Table 2show that the deeper 34-layer plain
net has higher validation error than the shallower 18-layer
plain net. To reveal the reasons, in Fig. 4(left) we com-
pare their training/validation errors during the training pro-
cedure. We have observed the degradation problem - the
4
resnet34.pdf, resnet34he.pdf
[Observe that there is a layer of ReLU activations in the middle of the motif, with linear convolutions
before and after it. Each convolution is followed by a batch normalization layer. The biggest ResNet in
the competition-winning ensemble, with 152 layers, uses a motif with three convolution layers, three batch
normalization layers, and three ReLU layers—two within the residual connection and one after.]
[Below is an example of the building block for a more modern convolutional ResNet, called ConvNeXt,
produced by a collaboration between Facebook and Berkeley. Unlike the original ResNet, it does not place
an activation function after the residual connection.]
[The authors found that one layer normalization step per three convolutional layers su ffices, whereas the
original ResNets used a batch normalization step after every convolutional layer. Instead of a ReLU, they
use a GELU, which stands for Gaussian Error Linear Unit . It’s similar in shape to a ReLU but it’s smooth,
with no discontinuity. It appears to give better test accuracy than ReLUs in some circumstances, though not
all. It is popular in transformers for speech generation.]
144 Jonathan Richard Shewchuk
7×7convolutions
layer normalization
GELUs
1×1convolutions1×1convolutions96channels×J×K
h[i+1]96channels×J×Kdepthwise convolution
96×7×7weights
a[i+2]384channels×J×K(channels connect one-to-one)
96×384weights
384×96weights384channels×J×K h[i+2]residual connection
96channels×J×K a[i+3]
96channels×J×K h[i+3]a[i+1]h[i]
(three convolutional layers)96channels×J×K
ConvNeXt building block
convnext.pdf
[The authors found that they get better test accuracy if they use 7 ×7 convolution filters, not just smaller
ones. But these filters use depthwise convolution , which means that each output channel receives input from
only one output channel. By contrast, traditional convolutional layers connect every output channel to every
input channel. The advantage of depthwise convolution is a big savings in weights and time.]
[Another interesting design choice is the use of an inverted bottleneck , where they temporarily increase the
number of channels from 96 to 384 and then decrease it again to 96. This creates very wide layers that lead
to better generalization to test data. To prevent the number of weights from exploding, they use the odd idea
of a 1×1 convolution going into and out of the 384 channels. This simply means that there is an all-to-all
connection from 96 channels to 384 channels, but there are no connections between adjacent “pixels” in
the activation maps. Note that the 1 ×1 convolutions are not depthwise convolutions! A 1 ×1 depthwise
convolution would not permit any mixing of information at all.]
[The combination of depthwise convolutions and 1 ×1 convolutions causes channel mixing to be separated
from spatial mixing. Both kinds of mixing take place, but they take place in di fferent convolutional layers.
This permits us to greatly reduce the number of weights while still allowing information to eventually flow
everywhere.]
Residual Networks; Batch Normalization; AdamW 145
AdamW
“Adaptive moment estimation with weight decay.”
An optimization method faster than SGD. Warning: may reduce test accuracy.
LetJbe losses summed over a minibatch. Let wibe a weight. Intuition:
– Sign of∂J
∂wigives more useful information than its relative magnitude.
– We change wislowly if∂J
∂wichanges sign frequently; otherwise stay fast.
[Roughly speaking, each weight has its own learning rate.]
– Keep exponential moving averages miof∂J
∂wi[first moment] and riof ∂J
∂wi!2
[second raw moment].
Each step has ∆wi∝−mi/√ri. Typically mi/√ri≈±1, but smaller if sign ( ∂J/∂wi) changes often.
m←0;r←0;t=0
repeat
t←t+1
g←∇ J(w)
m←βm+(1−β)g
r←τr+(1−τ)g⊙g [elementwise multiplication; square each component of g]
ˆm←m/(1−βt) [correcting bias, as mwas initialized to zero]
ˆr←r/(1−τt) [correcting bias]
∀i,wi←wi−ϵ αˆmi√ˆri+δ+λwi!
Typical parameters: α=0.001,β=0.9,τ=0.9999,δ=10−8.
Weight decay term λregularizes; set by validation. If λ=0, it’s just called Adam .
sgdadamgodoy.png (Daniel Godoy) [Left: 50 steps of SGD (with 16-point minibatches)
don’t get very close to the minimum (red). Right: 50 steps with Adam.]
146 Jonathan Richard Shewchuk
24 Boosting; Nearest Neighbor Classification
ADABOOST (Yoav Freund and Robert Schapire, 1997)
AdaBoost (“adaptive boosting”) is an ensemble method for classification (or regression) that
–reduces bias [compare with random forests and other ensembles, which reduce variance];
– trains learners on weighted sample points [like bagging];
– uses di fferent weights for each learner;
– increases weights of misclassified training points;
– gives bigger votes to more accurate learners.
Input: n×ddesign matrix X, vector of labels y∈Rnwith yi=±1.
Ideas:
– Train Tclassifiers G1,..., GT. [“ T” stands for “trees”]
– Weight for training point XiinGtgrows according to how many of G1,..., Gt−1misclassified it.
[Moreover, if Xiis misclassified by very accurate learners, its weight grows even more.]
[And, the weight shrinks every time Xiis correctly classified by a learner.]
– Train Gtto try harder to correctly classify training pts with larger weights.
– Metalearner is a linear combination of learners. For test point z,M(z)=PT
t=1βtGt(z).
Each Gtis±1, but Mis continuous. Return sign of M(z).
[Remember that in the previous lecture on ensemble methods, I talked briefly about how to assign di fferent
weights to training points. It varies for di fferent learning algorithms. For example, in regression we usually
modify the risk function by multiplying each point’s loss function by its weight. In a soft-margin support
vector machine, we modify the objective function by multiplying each point’s slack by its weight.]
[Boosting works with many learning algorithms, but it was originally developed for decision trees, and
boosted decision trees are very popular and successful. To weight points in decision trees, we use a weighted
entropy where instead of computing the proportion of points in each class, we compute the proportion of
weight in each class.]
In iteration T, what classifier GTand coe fficientβTshould we choose? Pick a loss fn L(prediction, label).
Find GT&βTthat minimize
Risk =1
nnX
i=1L(M(Xi),yi), M(Xi)=TX
t=1βtGt(Xi).
AdaBoost metalearner uses exponential loss function
L(ˆλ,λ)=e−ˆλλ=e−ˆλλ= +1
eˆλλ=−1
Important: label λis binary, Gtis binary, but ˆλ=M(Xi) is continuous!
[This loss function is for the metalearner only. We will discover later that the ideal cost function for each
individual learner Gtis just the total weight of the misclassified points. In practice, our individual learners
are often classification algorithms like decision trees that don’t explicitly try to minimize any loss function
at all. Even when the practice doesn’t match the theory, boosting still usually works quite well.]
[The exponential loss function has the advantage that it pushes hard against badly misclassified points. So
it’s usually better than the squared error loss function for classification in a metalearner. It’s similar to why
in neural networks we prefer the cross-entropy loss function over the squared error for sigmoid outputs.]
Boosting; Nearest Neighbor Classification 147
n·Risk =nX
i=1L(M(Xi),yi)=nX
i=1e−yiM(Xi)
=nX
i=1exp−yiTX
t=1βtGt(Xi)=nX
i=1TY
t=1e−βtyiGt(Xi)⇐yiGt(Xi)=±1
if−1,Gtmisclassifies Xi
=nX
i=1w(T)
ie−βTyiGT(Xi),where w(T)
i=T−1Y
t=1e−βtyiGt(Xi)
=e−βTX
yi=GT(Xi)w(T)
i+eβTX
yi,GT(Xi)w(T)
i[correctly classified and misclassified]
=e−βTnX
i=1w(T)
i+(eβT−e−βT)X
yi,GT(Xi)w(T)
i.
What GTminimizes the risk? The learner that minimizes the sum of w(T)
iover all misclassified pts Xi!
[This is interesting. By manipulating the formula for the risk, we’ve discovered what weight we should
assign to each training point. To minimize the risk, we should find the classifier that minimizes the sum of
the weights w(T)
i, as specified above, over the misclassified points. It’s a complicated function, but we can
compute it. A useful observation is that each learner’s weights are related to the previous learner’s weights:]
Recursive definition of weights:
w(T+1)
i=w(T)
ie−βTyiGT(Xi)=w(T)
ie−βTyi=GT(Xi),
w(T)
ieβTyi,GT(Xi).
[This recursive formulation is a nice benefit of choosing the exponential loss function. Notice that a weight
shrinks if the point was classified correctly by learner T, and grows if the point was misclassified.]
[Now, you might wonder if we should just pick a learner that classifies all the training points correctly. But
that’s not always possible. If we’re using a linear classifier on data that’s not linearly separable, some points
must be classified wrongly. Moreover, it’s NP-hard to find the optimal linear classifier, so in practice GT
will be an approximate best learner, not the true minimizer of the weighted training error. But that’s okay.]
[You might ask, if we use decision trees, can’t we get zero training error? Usually we can. But interestingly,
boosting is usually used with short, imperfect decision trees instead of tall, pure decision trees, for reasons
I’ll explain later.]
[Now, let’s derive the optimal value of βT.]
To chooseβT, setd
dβTRisk =0.
0=−e−βTnX
i=1w(T)
i+(eβT+e−βT)X
yi,GT(Xi)w(T)
i; [now divide both sides by the first term]
0=−1+(e2βT+1) err T,where err T=P
yi,GT(Xi)w(T)
iPn
i=1w(T)
i;⇐GT’s weighted error rate
βT=1
2ln 1−errT
errT!
.
[So now we have derived the optimal metalearner!]
148 Jonathan Richard Shewchuk
– If err T=0,βT=∞. [So a perfect learner gets an infinite vote.]
– If err T=1/2,βT=0. [So a learner with 50% weighted training error gets no vote at all.]
[More accurate learners get bigger votes in the metalearner. Interestingly, a learner with training error worse
than 50% gets a negative vote. A learner with 60% error is just as useful as a learner with 40% error; the
metalearner just reverses the signs of its votes. It’s so bad, it’s good.]
[Now we can state the AdaBoost algorithm.]
AdaBoost alg:
1. Initialize weights wi←1
n,∀i∈[1,n].
2. for t←1 toT
a. Train Gtwith weights wi.
b. Compute weighted error rate err ←P
misclassified wiP
allwi; coefficientβt←1
2ln 1−err
err!
.
c. Reweight pts: wi←wi·(eβt,Gtmisclassifies Xi
e−βt,otherwise=wi·q
1−err
err,q
err
1−err.
3. return metalearner h(z)=signTX
t=1βtGt(z).
boost.pdf [At left, all the training points have equal weight. After choosing a first linear
classifier, we increase the weights of the misclassified points and decrease the weights of the
correctly classified points (center). We train a second classifier with these weighted points,
then again adjust the weights of the points according to whether they are misclassified by
the second classifier.]
Why boost decision trees? [As opposed to other learning algorithms?] Why short trees?
–Boosting reduces bias reliably, but not always variance. AdaBoost trees are impure to reduce
overfitting. [Recall again that random forests use ensembles to reduce variance, but AdaBoost uses
them to reduce bias. The AdaBoost variance is more complicated: it often decreases at first, because
successive trees focus on di fferent features, but often it later increases. Sometimes boosting overfits
after many iterations, and sometimes it doesn’t; it’s hard to predict when it will and when it won’t.]
– Fast. [We’re training many learners, and running many learners at classification time too. Short
decision trees that only look at a few features are very fast at both training and testing.]
– No hyperparameter search needed. [Unlike SVMs, neural nets, etc.] [UC Berkeley’s Leo Breiman
called AdaBoost with decision trees “the best o ff-the-shelf classifier in the world.”]
– Easy to make a tree beat 45% training error [or some other threshold] consistently.
Boosting; Nearest Neighbor Classification 149
– AdaBoost +short trees is a form of subset selection.
[Features that don’t improve the metalearner’s predictive power enough aren’t used at all. This helps
reduce overfitting and running time, especially if there are a lot of irrelevant features.]
– Linear decision boundaries don’t boost well.
[It takes a lot of boosting to make linear classifiers model really nonlinear decision boundaries well, so
SVMs aren’t a great choice. Recall from Discussion Section 9 that ensembles of depth-one stumps are
an even worse choice, because they can’t even do XOR. Methods with nonlinear decision boundaries
benefit more from boosting, because they allow boosting to reduce the bias faster much. Even depth-
two decision trees boost substantially better than depth-one decision trees.]
More about AdaBoost:
– Posterior prob. can be approximated: P(Y=1|x)≈1
1+e−2M(x).
– Exponential loss is vulnerable to outliers; for corrupted data, use other loss.
[Loss functions have been derived for dealing with outliers. Unfortunately, they have more compli-
cated weight computations.]
– If every learner beats error µforµ < 50%, metalearner training error will eventually be zero. [You
will prove this in Homework 7.]
– [The AdaBoost paper and its authors, Freund and Schapire, won the 2003 G ¨odel Prize, a prize for
outstanding papers in theoretical computer science.]
0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0
Boosting IterationsTraining Error
Misclassification RateExponential Loss
0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5
Boosting IterationsTest ErrorSingle Stump
244 Node Tree
trainboost.pdf, testboost.pdf (ESL, Figures 10.2, 10.3) [Training and testing errors for
AdaBoost with stumps , depth-one decision trees that make only one decision each. At
left, observe that the training error eventually drops to zero, and even after that the average
loss (which is continuous, not binary) continues to decay exponentially. At right, the test
error drops to 5.8% after 400 iterations, even though each learner has an error rate of about
46%. AdaBoost with more than 25 stumps outperforms a single 244-node decision tree. In
this example no overfitting is observed, but there are other datasets for which overfitting is
a problem.]
150 Jonathan Richard Shewchuk
NEAREST NEIGHBOR CLASSIFICATION
[I saved the simplest classifier for the end of the semester.]
Idea: Given query point q, find the ktraining pts nearest q.
Distance metric of your choice.
Regression: Return average label of the kpts.
Classification: Return class with the most votes from the kpts OR
return histogram of class probabilities.
[The histogram of class probabilities tries to estimate the posterior probabilities of the classes. Obviously,
the histogram has limited precision. If k=3, then the only probabilities you’ll ever return are 0, 1 /3, 2/3,
or 1. You can improve the precision by making klarger, but you might underfit. The histogram works best
when you have a huge amount of data.]
o
o
o
oo
oo
o
o
oo
oo
oo
oo
oo
oo
oooo
o
oo
o
oo
oo
oo
o
oo
oo
o
oo
o
ooo
o
ooooo
ooo
o
o
oo
oo
oo
oo
ooo
oo
o
oo
o
oo
ooo
ooooo
o oo
oooo
ooo
oo
oo
oo
oo
o
oo
o
oo
oo
o
oooo
o
o
oooo
oo
oo
o
oo
o
oooo
oo
oo
oo
oo
o
oo
o
oo
oooo
o
oo
ooo
ooo
o
oooo
o
oooo
o
o
ooo
ooo
ooo
ooo
oo
oo
o
oo
oo
oooo
ooKNN: K=10
o
o
o
oo
oo
o
o
oo
oo
oo
oo
oo
oo
oooo
o
oo
o
oo
oo
oo
o
oo
oo
o
oo
o
ooo
o
ooooo
ooo
o
o
oo
oo
oo
oo
ooo
oo
o
oo
o
oo
ooo
ooooo
o oo
oooo
ooo
oo
oo
oo
oo
o
oo
o
oo
oo
o
oooo
o
o
oooo
oo
oo
o
oo
o
oooo
oo
oo
oo
oo
o
oo
o
oo
oooo
o
oo
ooo
ooo
o
oooo
o
oooo
o
o
ooo
ooo
ooo
ooo
oo
oo
o
oo
oo
oooo
ooKNN: K=1
o
o
o
oo
oo
o
o
oo
oo
oo
oo
oo
oo
oooo
o
oo
o
oo
oo
oo
o
oo
oo
o
oo
o
ooo
o
ooooo
ooo
o
o
oo
oo
oo
oo
ooo
oo
o
oo
o
oo
ooo
ooooo
o oo
oooo
ooo
oo
oo
oo
oo
o
oo
o
oo
oo
o
oooo
o
o
oooo
oo
oo
o
oo
o
oooo
oo
oo
oo
oo
o
oo
o
oo
oooo
o
oo
ooo
ooo
o
oooo
o
oooo
o
o
ooo
ooo
ooo
ooo
oo
oo
o
oo
oo
oooo
ooKNN: K=100
allnn.pdf (ISL, Figures 2.15, 2.16) [Examples of 1-NN, 10-NN, and 100-NN. A larger k
smooths out the boundary. In this example, the 1-NN classifier is overfitting the data, and
the 100-NN classifier is badly underfitting. The 10-NN classifier does well: it’s reasonably
close to the Bayes decision boundary (purple). Generally, the ideal kdepends on how dense
your data is. As your data gets denser, the best kincreases.]
[There are theorems showing that if you have a lot of data, nearest neighbors can work quite well.]
Theorem (Cover & Hart, 1967):
Asn→∞ , the 1-NN error rate is <2B−B2where B=Bayes risk.
if only 2 classes,≤2B−2B2
[There are a few technical requirements of this theorem. The most important is that the training points and
the test points all have to be drawn independently from the same probability distribution. Here, we are using
the 0-1 loss to define the Bayes risk; so the Bayes risk is the smallest possible error rate over that distribution.
The theorem applies to any separable metric space, so it’s not just for the Euclidean metric.]
Theorem (Fix & Hodges, 1951):
Asn→∞ ,k→∞ ,k/n→0,k-NN error rate converges to B. [Which means Bayes optimal.]
Nearest Neighbor Algorithms: V oronoi Diagrams and k-d Trees 151
25 Nearest Neighbor Algorithms: Voronoi Diagrams and k-d Trees
NEAREST NEIGHBOR ALGORITHMS
Exhaustive k-NN Alg.
Given query point q:
– Scan through all ntraining pts, computing distances to q.
– Maintain a max-heap with the kshortest distances seen so far.
[Whenever you encounter a training point closer to qthan the point at the top of the heap, you remove
the heap-top point and insert the better point. Obviously you don’t need a heap if k=1 or even 5, but
ifk=99 a heap will substantially speed up keeping track of the 99th-shortest distance.]
Time to train classifier: 0 [This is the only O(0)-time algorithm we’ll learn this semester.]
Query time: O(nd+nlogk)
expected O(nd+klognlogk) if random pt order
[It’s a cute theoretical observation that you can slightly improve the expected running time by randomizing
the point order so that only expected O(klogn) heap operations occur. But in practice I can’t recommend it;
you’ll probably lose more from cache misses than you’ll gain from fewer heap operations.]
Can we preprocess training pts to obtain sublinear query time?
2–5 dimensions: V oronoi diagrams
Medium dim (up to ∼30): k-d trees
Large dim: exhaustive k-NN, but can use PCA or random projection
locality sensitive hashing [still researchy, not widely adopted]
Voronoi Diagrams
LetXbe a point set. The V oronoi cell ofw∈Xis
V orw={p∈Rd:∥p−w∥≤∥ p−v∥ ∀v∈X}
[A V oronoi cell is always a convex polyhedron or polytope.]
The V oronoi diagram ofXis the set of X’s V oronoi cells.

152 Jonathan Richard Shewchuk
voro.pdf, vormcdonalds.jpg, voronoiGregorEichinger.jpg, saltflat3.jpg
[V oronoi diagrams sometimes arise in nature (salt flats, gira ffe, crystallography).]
giraffe-1.jpg, srsi2.png (Vladislav Blatov), vortex.pdf (Ren ´e Descartes)
[Perhaps the first frequent users of V oronoi cells were crystallographers, who call them V oronoi–Dirichlet
polyhedra. Above we see how the polyhedra can clarify the crystal structure of the low-temperature phase
of strontium silicide, α-SrSi 2.]
[Believe it or not, the first published V oronoi diagram dates back to 1644, in the book “Principia Philosophiae”
by the mathematician and philosopher Ren ´e Descartes. He claimed that the solar system consists of vortices.
In each region, matter is revolving around one of the fixed stars (vortex.pdf). His physics was wrong, but
his idea of dividing space into polyhedral regions has survived.]
Size (e.g., # of vertices) ∈O(n⌈d/2⌉)
[This upper bound is tight when dis a small constant. As dgrows, the tightest asymptotic upper bound is
somewhat smaller than this, but the complexity still grows exponentially with d.]
. . . but often in practice it is O(n).
[Here I’m leaving out a “constant” that may grow exponentially with d.]
Nearest Neighbor Algorithms: V oronoi Diagrams and k-d Trees 153
Point location : Given query point q∈Rd, find the point w∈Xfor which q∈V orw.
[We need a second data structure that can perform this search on a V oronoi diagram e fficiently.]
2D: O(nlogn) time to compute V .d. and a trapezoidal map for pt location
O(logn) query time [because of the trapezoidal map]
[That’s a pretty great running time compared to the linear query time of exhaustive search.]
dD: Use binary space partition tree (BSP tree) for pt location. [Unfortunately, it’s di fficult to characterize
the running time of this strategy, although it is often logarithmic in 3–5 dimensions.]
1-NN only! [A standard V oronoi diagram supports only 1-nearest neighbor queries. If you want the knearest
neighbors, there is something called an order- kV oronoi diagram that has a cell for each possible knearest
neighbors. But nobody uses those, for two reasons. First, the size of an order- kV oronoi diagram is Θ(k2n)
in 2D, and worse in higher dimensions. Second, there’s no reliable software available to compute one.]
[There are also V oronoi diagrams for other distance metrics, like the ℓ1andℓ∞norms.]
[V oronoi diagrams are good for 1-nearest neighbor queries in two dimensions, and maybe up to 5 dimen-
sions, and they’re a great concept for understanding the problem of nearest neighbor search. But k-d trees
are much simpler, and probably faster in 6 or more dimensions.]
k-d Trees
“Decision trees” for NN search. [Just like in a decision tree, each treenode in a k-d tree represents a
rectangular box in feature space, and we split a box by choosing a splitting feature and a splitting value. But
we use di fferent criteria for choosing splits.] Di fferences:
– Choose splitting feature w /greatest width: feature iin max i,j,k(Xji−Xki).
[With nearest neighbor search, we don’t care about the entropy. Instead, what we want is that if we
draw a sphere around the query point, it won’t intersect very many boxes of the decision tree. So it
helps if the boxes are nearly cubical, rather than long and thin.]
Cheap alternative: rotate through the features. [At depth 1 we split on the first feature, at depth 2 we
split on the second feature, and so on. This builds the tree faster, by a factor of O(d).]
– Choose splitting value: median point for feature i; OR midpointXji+Xki
2.
Median guarantees ⌊log2n⌋tree depth; O(ndlogn) tree-building time.
[. . . or just O(nlogn) time if you rotate through the features. An alternative to the median is splitting
at the box center, which improves the aspect ratios of the boxes, but it could unbalance your tree.
A compromise strategy is to alternate between medians at odd depths and centers at even depths,
which also guarantees an O(logn) depth.]
– Each internal node stores a training point. [. . . that lies in the node’s box. Usually the splitting point.]
[Some k-d tree implementations have points only at the leaves, but it’s better to have points in internal
nodes too, so when we search the tree, we often stop searching earlier.]
represents a box 8 4632 15 7 9
10
116
710 1
5 4 8
11 9 3 2root represents R2
right halfplane
lower right
quarter plane
every subtree
[Draw this by hand. kdtreestructure.pdf ]
154 Jonathan Richard Shewchuk
[Just like in a decision tree, each subtree represents an axis-aligned box in feature space. All the training
points stored in a subtree are in that box.]
[Once the tree is built, the classification algorithm is very di fferent from decision trees. Most importantly,
you usually have to visit multiple leaves of the tree to find the nearest neighbor. To save time, we sometimes
use an approximate nearest neighbor algorithm, instead of demanding the exact nearest neighbor.]
After tree is built (training), classify test pts:
Goal: given query pt q, find a training pt wsuch that∥q−w∥≤(1+ϵ)∥q−u∥,
where∥·∥isℓpnorm for some p∈[1,∞] [k-d trees are not limited to the Euclidean ( ℓ2) norm.]
&uis the nearest training pt in that norm.
ϵ=0⇒exact NN; ϵ >0⇒approximate NN.
Each subtree represents a box B=[s1,t1]×[s2,t2]×···× [sd,td]. [An sican be−∞, and a tican be∞.]
[Think of sas the lower left corner of the box, and tas the opposite corner.]
Think of Bas an infinite point set.
The distance from qtoBis dist( q,B)=min
z∈B∥q−z∥. [This norm is the same norm we seek neighbors in.]
B
dist(q3,B)=0q3
q2dist(q2,B)q1 dist(q1,B)
[Draw this by hand. dist.pdf ] [Point-to-box distances.]
The minimizer’s components are zi=si,qi<si,
qi,qi∈[si,ti],
ti,qi>ti.
Query alg. maintains:
– Nearest neighbor found so far (or knearest). goes down ↓
– Binary min-heap of unexplored boxes /subtrees, keyed by distance from q. goes up ↑
nearest so farq
[Draw this by hand. query.pdf ] [A query in progress.]
[We search the boxes nearest qfirst, hoping that we will never need to search most of the boxes or their
associated subtrees. The binary heap makes it fast to find the box nearest q, because each box in the heap
has a numerical key, the distance from qto the box. The search stops when the distance from qto the
kth-nearest neighbor found so far ≤the distance from qto the nearest unexplored box (times 1 +ϵ). For
example, in the figure above, the query will never visit the box at far lower right, because it doesn’t intersect
the circle. That’s how we avoid searching most of the tree—when we’re lucky.]
Nearest Neighbor Algorithms: V oronoi Diagrams and k-d Trees 155
Alg. for 1-NN query. Interpret each Bas both a box and a treenode.
Q←min-heap containing root node with key zero
r←∞
while Qnot empty and (1 +ϵ)·minkey( Q)<r
B←removemin( Q)
v←B’s training point
if∥q−v∥<rthen{w←v;r←∥q−v∥}
B′,B′′←child boxes of B
if (1+ϵ)·dist(q,B′)<rthen insert( Q,B′,dist(q,B′)) [The key for B′is dist( q,B′)]
if (1+ϵ)·dist(q,B′′)<rthen insert( Q,B′′,dist(q,B′′))
return w
Fork-NN, replace “ r” and “ w” with a max-heap holding the knearest neighbors.
Whyϵ-approximate NN?
q
[Draw this by hand. kdtreeproblem.pdf ] [A worst-case exact NN query.]
[In the worst case, we may have to visit every node in the k-d tree to find the exact nearest neighbor. In that
case, the k-d tree is slower than simple exhaustive search. This is an example where an approximate nearest
neighbor search can be much faster. In practice, settling for an approximate nearest neighbor sometimes
improves the speed by a factor of 10 or even 100, because you don’t need to look at most of the tree to do a
query. This is especially true in high dimensions—in a high-dimensional space, the nearest point often isn’t
much closer than a lotof other points.]
[I want to emphasize the fact that exhaustive nearest neighbor search really is one of the first classifiers you
should try in practice, even if it seems too simple. So here’s an example of a research paper that uses a
120-nearest neighbor classifier to solve a problem.]
im2gpspress.pdf
156 Jonathan Richard Shewchuk
[In 2008, James Hays and our own Prof. Alexei Efros wrote a paper on geolocalization , where the goal is to
take a query photograph and determine where on earth the photo was taken. Their training data was 6 million
GPS-tagged photos downloaded from Flickr. The bottom line is that by using 120-nearest neighbors, they
came within 64 km of the correct location about 50% of the time. That was good for the time, but I think
that in 2025, you could do much better with the data available to us today.]
RELATED CLASSES [if you like machine learning, consider these courses in 2024–25]
CS 180 /280A (fall): Computer Vision /Photography
CS 182 /282A (fall): Deep Neural Networks
EECS 183 (fall?): Natural Language Processing
CS 185 /285 (fall?): Deep Reinforcement Learning
CS 194-196 /294-196 (fall): Agentic AI (D. Song)
CS C281A (fall): Statistical Learning Theory [C281A is the most direct continuation of CS 189 /289A.]
EECS 127 (both), 227AT (both): Numerical Optimization [a core part of ML]
[It’s hard to overemphasize the importance of numerical optimization to machine learning, as well as other
CS fields like graphics, theory, and scientific computing.]
EECS 126 (both): Random Processes [Markov chains, expectation maximization, PageRank]
EE C106A /B (fall /spring): Intro to Robotics [dynamics, control, sensing]
Math 110 (both): Linear Algebra [but the real gold is in Math 221]
Math 221 (fall): Matrix Computations [how to solve linear systems, compute SVDs, eigenvectors, etc.]
CS C281B (spring): Learning & Decision Making
CS C267 (spring): Scientific Computing [parallelization, practical matrix algebra, some graph partitioning]
CS C280 (spring): Computer Vision (Efros, Kanazawa)
CS 294-162 (fall): ML Systems (Gonzalez /Stoica /Zaharia)
CS 294-286 (fall): Machine Learning in Social Settings (Chang)
NEU 100A (fall): Cellular and Molecular Neurobiology
VS 265 (?): Neural Computation
Bonus Lecture: Learning Theory 157
A Bonus Lecture: Learning Theory
LEARNING THEORY: WHAT IS GENERALIZATION?
[One thing humans do well is generalize. When you were a young child, you only had to see a few examples
of cows before you learned to recognize cows, including cows you had never seen before. You didn’t have
to see every cow. You didn’t even have to see log nof the cows.]
[Learning theory tries to explain how machine learning algorithms generalize, so they can classify data
they’ve never seen before. It also tries to derive mathematically how much training data we need to general-
ize well. Learning theory starts with the observation that if we want to generalize, it helps to constrain what
hypotheses we allow our learner to consider.]
A range space (aka set system ) is a pair ( P,H), where
Pis set of all possible test /training points (can be infinite)
His hypothesis class , a set of hypotheses (aka ranges , aka classifiers ):
each hypothesis is a subset h⊆Pthat specifies which points hpredicts are in class C.
[So each hypothesis his a 2-class classifier, and His a set of sets of points.]
Examples:
1. Power set classifier: Pis a set of knumbers; His the power set ofP, containing all 2ksubsets of P.
e.g., P={1,2},H={∅,{1},{2},{1,2}}
2. Linear classifier: P=Rd;His the set of all halfspaces ; each halfspace has the form {x:w·x≥−α}.
[In this example, both PandHare infinite. In particular, Hcontains every possible halfspace—that
is, every possible linear classifier in ddimensions.]
[The power set classifier sounds very powerful, because it can learn every possible hypothesis. But the
reality is that it can’t generalize at all. Imagine we have three training points and three test points in a row.]
C C N ? ? ?
[The power set classifier can classify these three test points any way you like. Unfortunately, that means it
has learned nothing about the test points from the training points. By contrast, the linear classifier can learn
only two hypotheses that fit this training data. The leftmost test point must be classified class C, and the
rightmost test point must be classified class Not-C. Only the test point in the middle can swing either way.
So the linear classifier has a big advantage: it can generalize from a few training points. That’s also a big
disadvantage if the data isn’t close to linearly separable, but that’s another story.]
[Now we will investigate how well the training error predicts the test error, and how that di ffers for these
two classifiers.]
Suppose all training pts & test pts are drawn independently from same prob. distribution Ddefined on
domain P. [Dalso determines each point’s label. Classes C and Not-C may have overlapping distributions.]
Leth∈Hbe a hypothesis [a classifier]. hpredicts a pt xis in class C if x∈h.
The risk aka generalization error R(h) of his the probability that hmisclassifies a random pt xdrawn
fromD—i.e., the prob. that x∈C but x<hor vice versa.
[Risk is almost the same as the test error. To be precise, the risk is the mean test error for test points drawn
randomly fromD. For a particular test set, sometimes the test error is higher, sometimes lower, but on
average it is R(h). If you had an infinite amount of test data, the risk and the test error would be the same.]
158 Jonathan Richard Shewchuk
LetX⊆Pbe a set of ntraining pts drawn from D
The empirical risk aka training error ˆR(h) is % of Xmisclassified by h.
[This matches the definition of empirical risk I gave you in Lecture 12, if you use the 0-1 loss function.]
hmisclassifies each training pt w /prob. R(h), so total misclassified has a binomial distribution.
Asn→∞ ,ˆR(h) better approximates R(h).
5 10 15 200.050.100.150.20
100 200 300 400 5000.010.020.030.04
binom20.pdf, binom500.pdf Consider a hypothesis whose risk of misclassification is 25%.
[Plotted are probability mass functions of the number of misclassified training points for 20
points and 500 points, respectively. For 20 points, the training error is not a reliable estimate
of the risk: the hypothesis might get “lucky” with misleadingly low training error.]
[If we had infinite training data, this distribution would become infinitely narrow and the training error
would always be equal to the risk. But we can’t have infinite training data. So, how well does the training
error approximate the risk?]
Hoeffding’s inequality tells us prob. of bad estimate:
Pr(|ˆR(h)−R(h)|>ϵ)≤2e−2ϵ2n.
[Hoeffding’s inequality is a standard result about how likely it is that a number drawn from a binomial
distribution will be far from its mean. If nis big enough, then it’s very unlikely.]
0 50 100 150 200 250 300points0.20.40.60.81.0bad estimate probability
hoeffding.pdf [Hoeffding’s bound for the unambitious ϵ=0.1. It takes at least 200 training
points to have high confidence of attaining that error bound.]
[One reason this matters is because we will try to choose the best hypothesis. If the training error is a bad
estimate of the test error, we might choose a hypothesis we think is good but really isn’t. So we are happy
to see that the likelihood of that decays exponentially in the amount of training data.]
Bonus Lecture: Learning Theory 159
Idea for learning alg: choose ˆh∈Hthat minimizes ˆR(ˆh)! Empirical risk minimization .
[None of the classification algorithms we’ve studied actually do this, but only because it’s computationally
infeasible to pick the best hypothesis. Support vector machines can find a linear classifier with zero training
error when the training data is linearly separable. But when it isn’t, SVMs try to find a linear classifier with
low training error, but they don’t generally find the one with minimum training error. That’s NP-hard.]
[Nevertheless, for the sake of understanding learning theory, we’re going to pretend that we have the com-
putational power to try every hypothesis and pick the one with the lowest training error.]
Problem: if too many hypotheses, some hwith high R(h) will get lucky and have very low ˆR(h)!
[This brings us to a central idea of learning theory. You might think that the ideal learning algorithm would
have the largest class of hypotheses, so it could find the perfect one to fit the data. But the reality is that you
can have so many hypotheses that some of them just get lucky and score far lower training error than their
actual risk. That’s another way to understand what “overfitting” is.]
[More precisely, the problem isn’t too many hypotheses . Usually we have infinitely many hypotheses, and
that’s okay. The problem is too many dichotomies.]
Dichotomies
A dichotomy ofXisX∩h, where h∈H.
[A dichotomy picks out the training points that hpredicts are in class C. Think of each dichotomy as a
function assigning each training point to class C or class Not-C.]
C C N C C N C C N C C N
[Draw this by hand. dichotomies.pdf ] [Three examples of dichotomies for three points in
a hypothesis class of linear classifiers, and one example (right) that is not a dichotomy.]
[For ntraining points, there could be up to 2ndichotomies. The more dichotomies there are, the more likely
it is that one of them will get lucky and have misleadingly low empirical risk.]
Extreme case: if Hallows all 2npossible dichotomies, ˆR(ˆh)=0 even if every h∈Hhas high risk.
[If our hypothesis class permits all 2npossible assignments of the ntraining points to classes, then one of
them will have zero training error. But that’s true even if all of the hypotheses are terrible and have a large
risk. Because the hypothesis class imposes no structure, we overfit the training points.]
IfHinduces Πdichotomies, Pr(at least one dichotomy has |ˆR−R|>ϵ)≤δ, whereδ=2Πe−2ϵ2n.
[Let’s fix a value of δand solve for ϵ.] Hence with prob. ≥1−δ, for every h∈H,
|ˆR(h)−R(h)|≤ϵ=r
1
2nln2Π
δ.
[This tells us that the smaller we make Π, the number of possible dichotomies, and the larger we make n,
the number of training points, the more accurately the training error will approximate how well the classifier
performs on test data.]
smaller Πor larger n⇒training error probably closer to true risk (& test error).
160 Jonathan Richard Shewchuk
[Smaller Πmeans we’re less likely to overfit. We have less variance, but more bias. This doesn’t necessarily
mean the risk will be small. If our hypothesis class Hdoesn’t fit the data well, both the training error and
the test error will be large. In an ideal world, we want a hypothesis class that fits the data well, yet doesn’t
produce many dichotomies.]
Leth∗∈Hminimize R(h∗); “best” classifier.
[Remember we picked the classifier ˆhthat minimizes the empirical risk. We really want the classifier h∗that
minimizes the actual risk, but we can’t know what h∗is. But if Πis small and nis large, the hypothesis ˆh
we have chosen is probably nearly as good as h∗.]
With prob.≥1−δ, our chosen ˆhhas nearly optimal risk:
R(ˆh)≤ˆR(ˆh)+ϵ≤ˆR(h∗)+ϵ≤R(h∗)+2ϵ, ϵ =r
1
2nln2Π
δ.
[This is excellent news! It means that with enough training data and a limit on the number of dichotomies,
empirical risk minimization usually chooses a classifier close to the best one in the hypothesis class.]
Choose aδand anϵ.
The sample complexity is the # of training pts needed to achieve this ϵwith prob.≥1−δ:
n≥1
2ϵ2ln2Π
δ.
[IfΠis small, we won’t need too many training points to choose a good classifier. Unfortunately, if Π = 2n
we lose, because this inequality says that nhas to be bigger than n. So the power set classifier can’t learn
much or generalize at all. We need to severely reduce Π, the number of possible dichotomies. One way to
do that is to use a linear classifier.]
The Shatter Function & Linear Classifiers
[How many ways can you divide npoints into two classes with a hyperplane?]
# of dichotomies: ΠH(X)=|{X∩h:h∈H}| ∈ [1,2n] where n=|X|
shatter function :ΠH(n)= max
|X|=n,X⊆PΠH(X) [The most dichotomies of any point set of size n]
Example: Linear classifiers in plane. H=set of all halfplanes. ΠH(3)=8:
N C C
N N C C C C C CC
[Draw this by hand. shatter.pdf ] [Linear classifiers can induce all eight dichotomies of
these three points. The other four dichotomies are the complements of these four.]
Bonus Lecture: Learning Theory 161
ΠH(4)=14:
[Instead of showing you all 14 dichotomies, let me show you dichotomies that halfplanes cannot learn,
which illustrate why no four points have 16 dichotomies.]
C C CN
NC
N
C CN C C
[Draw this by hand. unshatter.pdf ] [Examples of dichotomies of four points in the plane
that no linear classifier can induce.]
[This isn’t a proof that 14 is the maximum, because we have to show that 15 is not possible for anyfour
points in the plane. The standard proof uses a famous result called Radon’s Theorem.]
Fact: for all range spaces, either ΠH(n) is polynomial in n, orΠH(n)=2n∀n≥0.
[This is a surprising fact with deep implications. Imagine that you have npoints, some of them training
points and some of them test points. Either a range space permits every possible dichotomy of the points,
and the training points don’t help you classify the test points at all; or the range space permits only a
polynomial subset of the 2npossible dichotomies, so once you have labeled the training points, you have
usually cut down the number of ways you can classify the test points dramatically. No shatter function ever
occupies the no-man’s-land between polynomial and 2n.]
[For linear classifiers, we know exactly how many dichotomies there can be.]
Cover’s Theorem [1965]: linear classifiers in Rdallow up to ΠH(n)=2dX
i=0 n−1
i!
dichotomies of npts.
Forn≤d+1,ΠH(n)=2n.
Forn≥d+1,ΠH(n)≤2e(n−1)
dd[Observe that this is polynomial in n! With exponent d.]
and the sample complexity needed to achieve R(ˆh)≤ˆR(ˆh)+ϵ≤R(h∗)+2ϵwith prob.≥1−δis
n≥1
2ϵ2 
dlnn−1
d+d+ln4
δ!
.[Observe that the logarithm turned the exponent dinto a factor!]
Corollary: linear classifiers need only n∈Θ(d) training pts
for training error to accurately predict risk or test error.
[In a d-dimensional feature space, we need more than dtraining points to train an accurate linear classifier.
But it’s reassuring to know that the number we need is linear in d. By contrast, if we have a classifier that
permits all 2npossible dichotomies however large nis, then no amount of training data will guarantee that
the training error of the hypothesis we choose approximates the true risk.]
[The constant hidden in that big- Θnotation can be quite large. For example, if you choose ϵ=0.1 and
δ=0.1, then setting n=550dwill always su ffice. (For very large d,n=342dwill do.) If you want a lot of
confidence that you’ve chosen one of the best hypotheses, you have to pay for it with a large sample size.]
[This sample complexity applies even if you add polynomial features or other features, but you have to count
the extra features in d. So the number of training points you need increases with the number of polynomial
terms.]
162 Jonathan Richard Shewchuk
VC Dimension
The Vapnik–Chervonenkis dimension of (P,H) is
VC(H)=max{n:ΠH(n)=2n}.⇐Can be∞.
Say that Hshatters a set Xofnpts if ΠH(X)=2n.
VC(H) is size of largest XthatHcan shatter.
[This means that Xis a point set for which all 2ndichotomies are possible.]
[I told you earlier that if the shatter function isn’t 2nfor all n, then it’s a polynomial in n. The VC dimension
is motivated by an observation that sometimes makes it easy to bound that polynomial.]
Theorem: ΠH(n)≤VC(H)X
i=0 n
i!
. Hence for n≥VC(H),ΠH(n)≤ en
VC(H)!VC(H)
.
[So the VC dimension is an upper bound on the exponent of the polynomial. This theorem is useful because
often we can find an easy upper bound on the VC dimension. You just need to show that for some number n,
no set of npoints can have all 2ndichotomies.]
Corollary: O(VC( H)) training pts su ffice for accuracy. [Again, the hidden constant is big.]
[If the VC dimension is finite, it tells us how the sample complexity grows with the number of features.
If the VC dimension is infinite, no amount of training data will make the classifier generalize well.]
Example: Linear classifiers in plane.
Recall ΠH(3)=8: there exist 3 pts shattered by halfplanes.
ButΠH(4)=14: no 4 pts are shattered.
Hence:
– VC( H)=3 [The VC dimension of halfplanes is 3.]
–ΠH(n)≤e3
27n3[The shatter function is polynomial.]
–O(1) sample complexity.
[The VC dimension doesn’t always give us the tightest bound. In this example, the VC dimension promises
that the number of ways halfplanes can classify the points is at worst cubic in n; but Cover’s Theorem says
it’s quadratic in n. In general, linear classifiers in ddimensions have VC dimension d+1, which is one
dimension looser than the exponent Thomas Cover proved. That’s not a big deal, though, as the sample
complexity and the accuracy bound are both based on the logarithm of the shatter function. So if we get the
exponent wrong, it only changes a constant in the sample complexity.]
[The important thing is simply to show that there is some polynomial bound on the shatter function at all.
VC dimension is not the only way to do that, but often it’s the easiest.]
[The main point you should take from this lecture is that if you want to have generalization, you need to
limit the expressiveness of your hypothesis class so that you limit the number of possible dichotomies of a
point set. This may or may not increase the bias, but if you don’t limit the number of dichotomies at all, the
overfitting could be very bad. If you limit the hypothesis class, your artificial child will only need to look at
O(d) cows to learn the concept of cows. If you don’t, your artificial child will need to look at every cow in
the world, and every non-cow too.]
Bonus Lecture: The Kernel Trick 163
B Bonus Lecture: The Kernel Trick
KERNELS
Recall featurizing map Φ:Rd→RD.dinput features; Dfeatures after featurization ( Φ).
Degree- ppolynomials blow up to D∈Θ(dp) features.
[When dandpare not small, this gets computationally intractable really fast. As I said in Lecture 4, if you
have 100 features per feature vector and you want to use degree-4 polynomial decision functions, then each
featurized feature vector has a length of roughly 4 million.]
Today, magically, we use those features without computing them!
Observation: In many learning algs,
– the weights can be written as a linear combo of training points, &
– we can use inner products of Φ(x)’s only⇒don’t need to compute Φ(x)!
Suppose w=X⊤a=nX
i=1aiXifor some a∈Rn.
Substitute this identity into alg. and optimize ndual weights a(aka dual parameters )
instead of Dprimal weights w.
Kernel Ridge Regression
Center Xandyso their means are zero: Xi←Xi−µX,yi←yi−µy,Xi,d+1=1 [don’t center the 1’s!]
This lets us replace I′with Iin normal equations:
(X⊤X+λI)w=X⊤y.
[To kernelize ridge regression, we need the weights to be a linear combination of the training points. Unfor-
tunately, that only happens if we penalize the bias term wd+1=α, as these normal equations do. Fortunately,
when we center Xandy, the “expected” value of the bias term is zero. The actual bias won’t usually be
exactly zero, but it will often be close enough that we won’t do much harm by penalizing the bias term.]
Suppose ais a solution to
(XX⊤+λI)a=y. [Always has a solution if λ>0.]
Then X⊤y=X⊤XX⊤a+λX⊤a=(X⊤X+λI)X⊤a.
Therefore, w=X⊤ais a solution to the normal equations, andwis a linear combo of training points!
ais a dual solution ; solves the dual form of ridge regression:
Find athat minimizes∥XX⊤a−y∥2+λ∥X⊤a∥2.
[We obtain this dual form by substituting w=X⊤ainto the original ridge regression cost function.]
Training: Solve ( XX⊤+λI)a=yfora.
Testing: Regression fn is
h(z)=w⊤z=a⊤Xz=nX
i=1ai(X⊤
iz)⇐weighted sum of inner products
164 Jonathan Richard Shewchuk
Letk(x,z)=x⊤zbe kernel fn .
[Later, we’ll replace xandzwithΦ(x) and Φ(z), and that’s where the magic will happen.]
LetK=XX⊤ben×nkernel matrix . Note Ki j=k(Xi,Xj).
Kmay be singular. If so, probably no solution if λ=0. [Then we must choose a positive λ. But that’s okay.]
Always singular if n>d+1. [But don’t worry about the case n>d+1, because you would only want to
use the dual form when d>n, i.e., for polynomial features. But Kcould still be singular when d>n.]
Dual/kernel ridge regr. alg:
∀i,j,Ki j←k(Xi,Xj)⇐O(n2d) time
Solve ( K+λI)a=yfora⇐O(n3) time
for each test pt z
h(z)←Pn
i=1aik(Xi,z)⇐O(nd) time /test pt
Does not use Xidirectly! Only k. [This will become important soon.]
[Important : dual ridge regression produces the same predictions as primal ridge regression (with a penal-
ized bias term)! The di fference is the running time; the dual algorithm is faster if d>n, because the primal
algorithm solves a d×dlinear system, whereas the dual algorithm solves an n×nlinear system.]
The Kernel Trick (aka Kernelization )
[Here’s the magic part. We can compute a polynomial kernel without actually computing the features.]
The polynomial kernel of degree pisk(x,z)=(x⊤z+1)p.
Theorem: ( x⊤z+1)p= Φ(x)⊤Φ(z) for some Φ(x) containing every monomial in xof degree 0 ...p.
Example for d=2,p=2:
(x⊤z+1)2=x2
1z2
1+x2
2z2
2+2x1z1x2z2+2x1z1+2x2z2+1
=[x2
1x2
2√
2x1x2√
2x1√
2x21] [z2
1z2
2√
2z1z2√
2z1√
2z21]⊤
= Φ (x)⊤Φ(z) [This is how we’re defining Φ.]
[Notice the factors of√
2. If you try a higher polynomial degree p, you’ll see a wider variety of these
constants. We have no control of the constants that appear in Φ(x), but they don’t matter much, because the
primal weights wwill scale themselves to compensate. Even though we don’t directly compute the primal
weights, they implicitly exist in the form w=X⊤a.]
Key win : compute Φ(x)⊤Φ(z) inO(d) time instead of O(D)=O(dp), even though Φ(x) has length D.
Kernel ridge regr. replaces XiwithΦ(Xi): let k(x,z)= Φ(x)⊤Φ(z),
but doesn’t compute Φ(x) orΦ(z); it computes k(x,z)=(x⊤z+1)p.
Running times for 3 ridge algs:
primal dual (no kernel trick) kernel
train O(D3+D2n)O(n3+n2D) O(n3+n2d)
test (per test pt) O(D) O(nD) O(nd)
[I think what we’ve done here is pretty mind-blowing: we can now do polynomial regression with an expo-
nentially long, high-order polynomial in less time than it would take even to write out the final polynomial.
The running time can be asymptotically smaller than D, the number of terms in the polynomial.]
Bonus Lecture: The Kernel Trick 165
Kernel Logistic Regression
LetΦ(X) ben×Dmatrix with rows Φ(Xi)⊤. [ Φ(X) is the design matrix of the featurized training points.]
Featurized logi. regr. with batch grad. descent:
w←0 [starting point is arbitrary]
repeat until convergence
w←w+ϵΦ(X)⊤(y−s(Φ(X)w)) apply scomponent-wise to vector Φ(X)w
for each test pt z
h(z)←s(w⊤Φ(z))
Dualize with w= Φ(X)⊤a.
Then the code “ a←a+ϵ(y−s(Φ(X)w))” has same e ffect as “ w←w+ϵΦ(X)⊤(y−s(Φ(X)w))”.
LetK= Φ(X)Φ(X)⊤. [The n×nkernel matrix; but we don’t compute Φ(X)—we use the kernel trick.]
Note that Ka= Φ(X)Φ(X)⊤a= Φ(X)w. [And Φ(X)wappears in the algorithm above.]
Dual/kernel logistic regression:
a←0 [starting point is arbitrary]
∀i,j,Ki j←k(Xi,Xj) ⇐O(n2d) time (kernel trick)
repeat until convergence
a←a+ϵ(y−s(Ka)) ⇐O(n2) time /iteration [apply scomponent-wise]
for each test pt z
h(z)←snX
i=1aik(Xi,z)⇐O(nd) time /test pt [kernel trick]
[For classification, you can skip the logistic function s(·) and just compute the sign of the summation.]
[Kernel logistic regression computes the same answer as the primal algorithm, but the running time changes.]
Important: running times depend on original dimension d, not on length DofΦ(·)! Training for jiterations:
Primal: O(nD j) time Dual (no kernel trick): O(n2D+n2j) time Kernel: O(n2d+n2j) time
Alternative training: stochastic gradient descent (SGD). Primal logistic SGD step is
w←w+ϵ yi−s(Φ(Xi)⊤w)Φ(Xi).
Dual logistic SGD maintains a vector q=Ka∈Rn. Note that qi=(Φ(X)w)i= Φ(Xi)⊤w.
LetK∗idenote column iofK.
a←0;q←0;∀i,j,Ki j←k(Xi,Xj) [If you choose a di fferent starting point, set q←Ka.]
repeat until convergence
choose random i∈[1,n]
ai←ai+ϵ(yi−s(qi)) ⇐O(1) time
q←q+ϵ(yi−s(qi))K∗i⇐computes q=KainO(n) time, not O (n2) time
[SGD updates only one dual weight aiper iteration; that’s a nice benefit of the dual formulation. We cleverly
update q=Kain linear time instead of performing a quadratic-time matrix-vector multiplication.]
Primal: O(D j) time Dual (no kernel trick): O(n2D+n j) time Kernel: O(n2d+n j) time
Alternative testing: If # of training points and test points both exceed D/d, classifying with primal weights w
may be faster. [This applies to ridge regression as well.]
w= Φ(X)⊤a ⇐O(nD) time (once only)
for each test pt z
h(z)←s w⊤Φ(z)⇐O(D) time /test pt
166 Jonathan Richard Shewchuk
The Gaussian Kernel
[Mind-blowing as the polynomial kernel is, I think our next trick is even more mind-blowing. Since we can
now do fast computations in spaces with exponentially large dimensions, why don’t we go all the way and
generate feature vectors in an infinite-dimensional space?]
Gaussian kernel , aka radial basis fn kernel : there exists a Φ:Rd→R∞such that
k(x,z)=exp 
−∥x−z∥2
2σ2!
[This kernel takes O(d) time to compute.]
[In case you’re curious, here’s the feature vector that gives you this kernel, for the case where you have only
one input feature per sample point.]
e.g., for d=1,
Φ(x)=exp 
−x2
2σ2!"
1,x
σ√
1!,x2
σ2√
2!,x3
σ3√
3!,...#⊤
[This is an infinite vector, and Φ(x)·Φ(z) is a series that converges to k(x,z). Nobody actually uses this value
ofΦ(x) directly, or even cares about it; they just use the kernel function k(·,·).]
[At this point, it’s best notto think of points in a high-dimensional space. It’s no longer a useful intuition.
Instead, think of the kernel kas a measure of how similar or close together two points are to each other.]
Key observation: hypothesis h(z)=Pn
j=1ajk(Xj,z) is a linear combo of Gaussians centered at training pts.
[The dual weights are the coe fficients of the linear combination.]
[The Gaussians are a basis for the hypothesis.]
gausskernel.pdf [A hypothesis hthat is a linear combination of Gaussians centered at four
training points, two with positive weights and two with negative weights. If you use ridge
regression with a Gaussian kernel, your “linear” regression will look something like this.]
Bonus Lecture: The Kernel Trick 167
Very popular in practice! Why?
– Gives very smooth h. [In fact, his infinitely di fferentiable; it’s C∞-continuous.]
– Behaves somewhat like k-nearest neighbors, but smoother.
– Oscillates less than polynomials (depending on σ).
–k(x,z) interpreted as a similarity measure . Maximum when z=x; goes to 0 as distance increases.
– Training points “vote” for value at z, but closer points get weightier vote.
[The “standard” kernel k(x,z)=x·zassigns more weight to training point vectors that point in roughly the
same direction as z. By contrast, the Gaussian kernel assigns more weight to training points near z.]
Chooseσby validation.
σtrades o ffbias vs. variance:
largerσ→wider Gaussians & smoother h→more bias & less variance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
o ooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
•
•••
•
••••
••••
••••
••
••
••••
••
•
••
•
Training Error: 0.160
Test Error:       0.218
Bayes Error:    0.210
gausskernelsvm.pdf (ESL, Figure 12.3) [The decision boundary (solid black) of a soft-
margin SVM with a Gaussian kernel. Observe that in this example, it comes reasonably
close to the Bayes optimal decision boundary (dashed purple). The dashed black curves are
the boundaries of the margin. The small black disks are the support vectors that lie on the
margin boundary.]
[By the way, there are many other kernels that, like the Gaussian kernel, are defined directly as kernel
functions without worrying about Φ. But not every function can be a kernel function. A function is qualified
only if it always generates a positive semidefinite kernel matrix, for every sample. There is an elaborate
theory about how to construct valid kernel functions. However, you probably won’t need it. The polynomial
and Gaussian kernels are the two most popular by far.]
[As a final word, be aware that not every featurization Φleads to a kernel function that can be computed
faster than Θ(D) time. In fact, the vast majority cannot. Featurizations that can are rare and special.]
168 Jonathan Richard Shewchuk
C Bonus Lecture: Spectral Graph Clustering
SPECTRAL GRAPH CLUSTERING
Input: Weighted, undirected graph G=(V,E). No self-edges.
wi j=weight of edge ( i,j)=(j,i); zero if ( i,j)<E.
[Think of the edge weights as a similarity measure. A big weight means that the two vertices want to be
in the same cluster. So the circumstances are the opposite of the last lecture on clustering. Then, we had a
distance or dissimilarity function, so small numbers meant that points wanted to stay together. Today, big
numbers mean that vertices want to stay together.]
Goal: Cut Ginto 2 (or more) pieces Giof similar sizes,
but don’t cut too much edge weight.
[That’s a vague goal. There are many ways to make this precise.
Here’s a typical goal, which we’ll solve approximately.]
e.g., Minimize the sparsityCut(G1,G2)
Mass( G1) Mass( G2), aka cut ratio
where Cut( G1,G2)=total weight of cut edges
Mass( G1)=# of vertices in G1OR assign masses to vertices
[The denominator “Mass( G1) Mass( G2)” penalizes imbalanced cuts.]
minimum
bisection
sparsest
cut
minimum
cut
maximum
cut
graph.pdf [Four cuts. All edges have weight 1.
Upper left: the minimum bisection ; a bisection is perfectly balanced.
Upper right: the minimum cut . Usually very unbalanced; not what we want.
Lower left: the sparsest cut , which is good for many applications.
Lower right: the maximum cut ; in this case also the maximum bisection.]
Sparsest cut, min bisection, max cut all NP-hard.
[Today we will look for an approximate solution to the sparsest cut problem.]
[We will turn this combinatorial graph cutting problem into algebra.]
Bonus Lecture: Spectral Graph Clustering 169
Letn=|V|. Let y∈Rnbe an indicator vector :
yi=(1 vertex i∈G1,
−1 vertex i∈G2.
Then wi j(yi−yj)2
4=(wi j(i,j) is cut,
0 ( i,j) is not cut.
Cut(G1,G2)=X
(i,j)∈Ewi j(yi−yj)2
4[This is quadratic, so let’s try to write it with a matrix.]
=1
4X
(i,j)∈E
wi jy2
i−2wi jyiyj+wi jy2
j
=1
4X
(i,j)∈E−2wi jyiyj
|              {z              }
off-diagonal terms+nX
i=1y2
iX
k,iwik
|          {z          }
diagonal terms
=y⊤Ly
4,
where Li j=(−wi j, i,j,P
k,iwik,i=j.
Lis symmetric, n×nLaplacian matrix forG.
[Draw this by hand graphexample.png ]
[Lis effectively a matrix representation of G. For the purpose of partitioning a graph, there is no need to
distinguish edges of weight zero from edges that are not in the graph.]
[We see that minimizing the weight of the cut is equivalent to minimizing the Laplacian quadratic form
y⊤Ly. This lets us turn graph partitioning into a problem in matrix algebra.]
[Usually we assume there are no negative weights, in which case Cut( G1,G2) can never be negative, so it
follows that Lis positive semidefinite.]
Define 1=[1 1... 1]⊤; then L1=0, so [It’s easy to check that each row of Lsums to zero.]
1is an eigenvector of Lwith eigenvalue 0.
[IfGis a connected graph and all the edge weights are positive, then this is the only zero eigenvalue. But if
Gis not connected, Lhas one zero eigenvalue for each connected component of G. It’s easy to prove, but
time prevents me.]
170 Jonathan Richard Shewchuk
Bisection : exactly n/2 vertices in G1,n/2 inG2. Write 1⊤y=0.
[So we have reduced graph bisection to this constrained optimization problem.]
Minimum bisection:
Find ythat minimizes y⊤Ly
subject to∀i,yi=1 oryi=−1
and 1⊤y=0←binary constraint
←balance constraint
Also NP-hard. We relax the binary constraint. →fractional vertices!
[A very common approach in combinatorial optimization algorithms is to relax some of the constraints so
a discrete problem becomes a continuous problem. Intuitively, this means that you can put 1 /3 of vertex 7
in graph G1and the other 2 /3 of vertex 7 in graph G2. You can even put −1/2 of vertex 7 in graph G1and
3/2 of vertex 7 in graph G2. This sounds crazy, but the continuous problem is much easier to solve than the
combinatorial problem. After we solve the continuous problem, we will round the vertex values to +1/−1,
and we’ll hope that our solution is still close to optimal.]
[But we can’t just drop the binary constraint. We still need some constraint to rule out the solution y=0.]
New constraint: ymust lie on hypersphere of radius√n.
[Draw this by hand. circle.pdf ] [Instead of constraining yto lie at a vertex of the hyper-
cube, we constrain yto lie on the hypersphere through those vertices.]
Relaxed problem:
Minimize y⊤Ly
subject to y⊤y=n
and 1⊤y=0)
=Minimizey⊤Ly
y⊤y=Rayleigh quotient of L&y
(subject to same two constraints)
y3y⊤Ly=8
y⊤Ly=16
y⊤Ly=24
1⊤y=011
5
3132 y⊤Ly=0 y2
y1
cylinder.pdf [The isosurfaces of y⊤Lyare elliptical cylinders. The gray cross-section is
the hyperplane 1⊤y=0. We seek the point that minimizes y⊤Ly, subject to the constraints
that it lies on the gray cross-section and that it lies on a sphere centered at the origin.]
Bonus Lecture: Spectral Graph Clustering 171
v3 y⊤Ly=12
y⊤Ly=6
y1y2
y351
312
3y⊤y=3
v2y⊤Ly=16.6077
endview.pdf [The same isosurfaces restricted to the hyperplane 1⊤y=0. The solution is
constrained to lie on the outer circle.]
[You should remember this Rayleigh quotient from the lecture on PCA. As I said then, when you see a
Rayleigh quotient, you should smell eigenvectors nearby. The ythat minimizes this Rayleigh quotient is the
eigenvector with the smallest eigenvalue. We already know what that eigenvector is: it’s 1. But that violates
our balance constraint. As you should recall from PCA, when you’ve used the most extreme eigenvector
and you need an orthogonal one, the next-best optimizer of the Rayleigh quotient is the next eigenvector.]
Letλ2=second-smallest eigenvalue of L.
Eigenvector v2is the Fiedler vector .v2solves the relaxed problem.
[It would be wonderful if every component of the Fiedler vector was 1 or −1, but that happens more or less
never. So we round v2. The simplest way is to round all positive entries to 1 and all negative entries to −1.
But in both theory and practice, it’s better to choose the threshold as follows.]
Spectral partitioning alg:
– Compute Fiedler vector v2ofL
– Round v2with a sweep cut :
=Sort components of v2.
=Try the n−1 cuts between successive components. Choose min-sparsity cut.
[If we’re clever about updating the sparsity, we can try all these cuts in time linear in the number
of edges in G.]
5 10 15 20
-0.4-0.20.20.40.6
specgraph.pdf, specvector.pdf [Left: example of a graph partitioned by the sweep cut.
Right: what the un-rounded Fiedler vector looks like.]
172 Jonathan Richard Shewchuk
[One consequence of relaxing the binary constraint is that the balance constraint no longer forces an exact
bisection. But that’s okay; we’re cool with a slightly unbalanced cut if it means we cut fewer edges. Even
though our discrete problem was the minimum bisection problem, our relaxed, continuous problem will be
an approximation of the sparsest cut problem. This is a bit counterintuitive.]
lopsided.pdf [A graph for which an unbalanced cut (left) is sparser than a balanced one
(right).]
Vertex Masses
[Sometimes you want the notion of balance to accord more prominence to some vertices than others. We
can assign masses to vertices.]
LetMbe diagonal matrix with vertex masses on diagonal.
New balance constraint: 1⊤My=0.
[This new balance constraint says that G1andG2should each have the same total mass. It turns out that this
new balance constraint is easier to satisfy if we also revise the sphere constraint a little bit.]
New ellipsoid constraint: y⊤My=Mass( G)=PMii.
[Instead of a sphere, now we constrain yto lie on an axis-aligned ellipsoid.]
[Draw this by hand. ellipse.pdf ] [The constraint ellipsoid passes through the points of the
hypercube.]
Now solution is Fiedler vector of generalized eigensystem Lv=λMv.
[Most algorithms for computing eigenvectors and eigenvalues of symmetric matrices can easily be adapted
to compute eigenvectors and eigenvalues of symmetric generalized eigensystems too.]
[For the grad students, here’s the most important theorem in spectral graph partitioning.]
Fact: Sweep cut finds a cut w /sparsity≤q
2λ2max iLii
Mii: Cheeger’s inequality .
The optimal cut has sparsity ≥λ2/2.
[So the spectral partitioning algorithm is an approximation algorithm, albeit not one with a constant factor
of approximation. Cheeger’s inequality is a very famous result in spectral graph theory, because it’s one of
the most important cases where you can relax a combinatorial optimization problem to a continuous opti-
mization problem, round the solution, and still have a provably decent solution to the original combinatorial
problem.]
Bonus Lecture: Spectral Graph Clustering 173
Vibration Analogy
vibrate.pdf
[For intuition about spectral partitioning, think of the eigenvectors as vibrational modes in a physical system
of springs and masses. Each vertex models a point mass that is constrained to move freely along a vertical
rod. Each edge models a vertical spring with rest length zero and sti ffness proportional to its weight, pulling
two point masses together. The masses are free to oscillate sinusoidally on their rods. The eigenvectors of the
generalized eigensystem Lv=λMvare the vibrational modes of this physical system, and their eigenvalues
are proportional to their frequencies.]
v3 v2 v1 v4
grids.pdf [Vibrational modes in a path graph and a grid graph.]
[These illustrations show the first four eigenvectors for two simple graphs. On the left, we see that the first
eigenvector is the eigenvector of all 1’s, which represents a vertical translation of all the masses in unison.
That’s not really a vibration, which is why the eigenvalue is zero. The second eigenvector is the Fiedler
vector, which represents the vibrational mode with the lowest frequency. Each component indicates the
amplitude with which the corresponding point mass oscillates. At any point in time as the masses vibrate,
roughly half the mass is moving up while half is moving down. So it makes sense to cut between the positive
components and the negative components. The third eigenvector also gives us a nice bisection of the grid
graph, entirely di fferent from the Fiedler vector. Some more sophisticated graph clustering algorithms use
multiple eigenvectors.]
[I want to emphasize that spectral partitioning takes a global view of a graph. It looks at the whole gestalt
of the graph and finds a good cut. By comparison, the clustering algorithms we saw last lecture were much
more local in nature, so they’re easier to fool.]
174 Jonathan Richard Shewchuk
Greedy Divisive Clustering
Partition Ginto 2 subgraphs; recursively partition them.
[The sparsity is a good criterion for graph clustering. Use G’s sparsest cut to divide it into two subgraphs,
then recursively cut them. You can stop when you have the right number of clusters. Alternatively, you can
make a finer tree and then prune it back.]
The Normalized Cut
Set vertex i’s mass Mii=Lii. [Sum of edge weights adjoining vertex i.]
[That is how we define a normalized cut , which turns out to be a good choice for many di fferent applica-
tions.]
Popular for image segmentation .
[Image segmentation is the problem of looking at a photograph and separating it into di fferent objects. To
do that, we define a graph on the pixels.]
For pixels with coordinate pi, brightness bi, use graph weights
wi j=exp−∥pi−pj∥2
α−|bi−bj|2
β or zero if∥pi−pj∥large.
[We choose a distance threshold, typically less than 4 to 10 pixels apart. Pixels that are far from each other
aren’t connected. αandβare empirically chosen constants. It often makes sense to choose βproportional
to the variance of the brightness values.]
baseballsegment.pdf (Shi and Malik, “Normalized Cut and Image Segmentation”)
[A segmentation of a photo of a scene from a baseball game (upper left). The other figures
show segments of the image extracted by recursive spectral partitioning.]
Bonus Lecture: Spectral Graph Clustering 175
baseballvectors.pdf (Shi and Malik) [Eigenvectors 2–9 from the baseball image.]
Invented by [our own] Prof. Jitendra Malik and his student Jianbo Shi.
176 Jonathan Richard Shewchuk
D Bonus Lecture: Multiple Eigenvectors; Latent Factor Analysis
Clustering w /Multiple Eigenvectors
[When we use the Fiedler vector for spectral graph clustering, it tells us how to divide a graph into two
graphs. If we want more than two clusters, we can use divisive clustering: we repeatedly cut the subgraphs
into smaller subgraphs by computing their Fiedler vectors. However, there are several other methods to
subdivide a graph into kclusters in one shot that use multiple eigenvectors rather than just the Fiedler
vector v2. These methods sometimes give better results. They use keigenvectors in a natural way to cluster
a graph into ksubgraphs.]
Forkclusters, compute first keigenvectors v1=1,v2,..., vkof generalized eigensystem Lv=λMv.
Scale them so that v⊤
iMvi=1. E.g., v1=1√PMii1. Now V⊤MV=I. [The eigenvectors are M-orthogonal .]
V=v1
n×k=
vkV1
Vn[V’s columns are the eigenvectors with the k
smallest eigenvalues.]
[Yes, we do include the all-1’s vector v1as one of
the columns of V.]
[Draw this by hand. eigenvectors.pdf ]
Row Viis spectral vector [my name] for vertex i. [The rows are vectors in a k-dimensional space I’ll call the
“spectral space.” When we were using just one eigenvector, it made sense to cluster vertices together if their
components were close together. When we use more than one eigenvector, it turns out that it makes sense to
cluster vertices together if their spectral vectors point in similar directions.]
Normalize each row Vito unit length.
[Now you can think of the spectral vectors as points on a unit sphere centered at the origin.]
[Draw this by hand vectorclusters.png ] [A 2D example showing two clusters on a circle.
If the graph has kcomponents, the points in each cluster will have identical spectral vectors
that are exactly orthogonal to all the other components’ spectral vectors (left). If we modify
the graph by connecting these components with small-weight edges, we get vectors more
like those at right—not exactly orthogonal, but still tending toward distinct clusters.]
k-means cluster these vectors.
[Because all the spectral vectors lie on the sphere, k-means clustering will cluster together vectors that are
separated by small angles.]
Bonus Lecture: Multiple Eigenvectors; Latent Factor Analysis 177
compkmeans.png, compspectral.png [Comparison of point sets clustered by k-means—
justk-means by itself, that is—vs. a spectral method. To create a graph for the spectral
method, we use an exponentially decaying function to assign weights to pairs of points, like
we used for image segmentation but without the brightnesses.]
Invented by [our own] Prof. Michael Jordan, Andrew Ng [when he was still a student at Berkeley], Yair
Weiss.
[This wasn’t the first algorithm to use multiple eigenvectors for spectral clustering, but it has become one of
the most popular.]
178 Jonathan Richard Shewchuk
LATENT FACTOR ANALYSIS [aka Latent Semantic Indexing]
[You can think of this as dimensionality reduction for matrices.]
Suppose Xis a term-document matrix : [aka bag-of-words model ]
rowirepresents document i; column jrepresents term j. [Term =word.]
[Term-document matrices are usually sparse , meaning most entries are zero.]
Xi j=occurrences of term jin doc i
better: log (1 +occurrences) [So frequent words don’t dominate.]
[Better still is to weight the entries so rare words give big entries and common words like “the” give small
entries. To do that, you need to know how frequently each word occurs in general. I’ll omit the details, but
this is the common practice.]
Recall SVD X=UDV⊤=dX
i=1δiuiv⊤
i.Supposeδi≤δjfori≥j.
Unlike PCA, we usually don’t center X.
For largeδi,uiandvirepresent a cluster of documents & terms.
– Large components in uimark docus using similar /related terms, i.e., a genre.
– ” ” ” vimark frequent terms in that genre.
– E.g., u1might have large components for the romance novels,
– v1” ” ” ” for terms “passion,” “ravish,” “bodice” . . .
[. . . andδ1would give us an idea how much bigger the romance novel market is than the markets for every
other genre of books.]
[v1andu1tell us that there is a large subset of books that tend to use the same large subset of words. We
can read o ffthe words by looking at the larger components of v1, and we can read o ffthe books by looking
at the larger components of u1.]
[The property of being a romance novel is an example of a latent factor . So is the property of being the sort
of word used in romance novels. There’s nothing in Xthat tells you explicitly that romance novels exist,
but the similar vocabulary is a hidden connection between them that gives them a large singular value. The
vector u1reveals which books have that genre, and v1reveals which words are emphasized in that genre.]
Like clustering, but clusters overlap: if u1picks out romances &
u2picks out histories, they both pick out historical romances.
[So you can think of latent factor analysis as a sort of clustering that permits clusters to overlap. Another
way in which it di ffers from traditional clustering is that the u-vectors contain real numbers, and so some
points have stronger cluster membership than others. One book might be just a bit romance, another a lot.]
Bonus Lecture: Multiple Eigenvectors; Latent Factor Analysis 179
Application in market research:
identifying consumer types (hipster, suburban mom) & items bought together.
[For applications like this, the first few singular vectors are the most useful. Most of the singular vectors are
mostly noise, and they have small singular values to tell you so. This motivates approximating a matrix by
using only some of its singular vectors.]
Truncated SVD X′=rX
i=1δiuiv⊤
iis a low-rank approximation ofX, of rank r. [Assuming δr>0.]
[We choose the singular vectors with the largest singular values, because they carry the most information.]
δr
r×r...v1
vr
r×d00
ur=
n×du1
X′
n×rδ1
[Draw this by hand. truncate.pdf ]
X′is the rank- rmatrix that minimizes the [squared] Frobenius norm
∥X−X′∥2
F=X
i,j
Xi j−X′
i j2
Applications:
– Fuzzy search. [Suppose you want to find a document about gasoline prices, but the document you
want doesn’t have the word “gasoline”; it has the word “petrol.” One cool thing about the reduced-
rank matrix X′is that it will probably associate that document with “gasoline,” because the SVD tends
to group synonyms together.]
– Denoising. [The idea is to assume that Xis a noisy measurement of some unknown matrix that
probably has low rank. If that assumption is partly true, then the reduced-rank matrix X′might be
better than the input X.]
– Matrix compression. [As you can see above, if we use a low-rank approximation with a small rank
r, we can express the approximate matrix as an SVD that takes up much less space than the original
matrix. Often this low-rank approximation supports faster matrix computations.]
– Collaborative filtering: fills in unknown values, e.g., user ratings.
[Suppose the rows of Xrepresents Netflix users and the columns represent movies. The entry Xi jis
the review score that user igave to movie j. But most users haven’t reviewed most movies. We want
to fill in the missing values. Just as the rank reduction will associate “petrol” with “gasoline,” it will
tend to associate users with similar tastes in movies, so the reduced-rank matrix X′can predict ratings
for users who didn’t supply any.]
180 Jonathan Richard Shewchuk
PREDICTING PERSONALITY FROM FACES
ǤǤȀ
 
͹
ͷ ͷǡ͸ǡ ͹ ͷ ͺ ͺͷ
 
ǯǤǡ 
Ǥ 
;͹ͺȋͺͶͻͺ͸ͿȌǡ ƤǦ
ȋǮ	ǯȌǡ͹Ǥ 
͹ Ǧ	
	

	

hu.pdf
Hu et. al (2017).
Big Five (BF) model of personality:
– O: openness
– C: conscientiousness
– E: extraversion
– A: agreeableness
– N: neuroticism
[Researchers have found that these five personality factors are approximately orthogonal to each other. They
are highly heritable and highly stable during adulthood.]
Can we predict these traits from 3D faces?
[Studies have shown that people looking at photographs of static faces with neutral expressions can iden-
tify the traits better than chance, especially for conscientiousness, extraversion, and agreeableness. This
experiment asks whether machine learning can do the same with 3D reconstructions of faces. The subjects
were 834 Han Chinese volunteers in Shanghai, China. We don’t know whether any of these results might
generalize to people who are not Han Chinese.]
[The faces were scanned in high-resolution 3D and a non-rigid face registration system was used to fit a
grid of 32,251 vertices to each face in a manner that maps each vertex to an appropriate landmark on the
face. (They call this “anatomical homology.”) So the design matrix Xwas 834×100,053, representing 834
subjects with 32,251 3D features each.]
[Subject personalities were evaluated with a self-questionnaire, namely our own Berkeley Personality Lab’s
Big Five Inventory, translated into Chinese. The authors treated men and women separately.]
Bonus Lecture: Multiple Eigenvectors; Latent Factor Analysis 181
Uses partial least squares (PLS) to find associations between personality & faces.
[Everything from here to the end is spoken, not written.]
Partial least squares (PLS) is like a supervised version of PCA. It takes in two matrices XandYwith the
same number of rows. In our example, Xis the face data and Yis the personality data for the 834 subjects.
Like PCA, PLS finds a set of vectors in face space that we think of as the most important components. But
whereas PCA looks for the directions of maximum variation in X, PLS looks for the directions in Xthat
maximize the correlation with the personality traits in matrix Y.
The researchers found the top 20 or so PLS components and used cross-validation to decide which compo-
nents have predictive power for each personality trait. They found that the top two components for extraver-
sion in women were predictive, but no components for the other four traits in women were predictive. Men
are easier to analyze: they found two or three components were predictive for each of extraversion, agree-
ableness, conscientiousness, and neuroticism in men. However, the correlations were statistically significant
only for agreeableness and conscientiousness.
male.pdf [The relationship between male faces, agreeableness, and conscientiousness.
The large, colored faces are the mean faces. Colors indicate the values in the most pre-
dictive PLS component vector.]
More agreeable men correlate with much wider mouths that look a bit smiley even when neutral; stronger,
forward jaws; wider noses; and shorter faces, especially shorter in the forehead, compared to less agreeable
men. More conscientious men tend to have higher, wider eyebrows; wider, opened eyes; a withdrawn upper
lip with more mouth tension; and taller faces with more pronounced brow ridges (the bone protuberance
above the eyes). The authors note that men with low A and C scores look both more relaxed and more
indifferent.
182 Jonathan Richard Shewchuk
female.pdf [The relationship between female faces and extraversion. The large, colored
face is the mean face. Colors illustrate the most predictive PLS component vector.]
More extraverted women correlate with rounder faces, especially in profile, with a more protruding nose
and lips but a recessed chin, whereas the introverts have more flat, square-shaped faces. To my eyes, the
extraverts also have more expressive mouths.
It’s interesting is that physiognomy , the art of judging character from facial shape, used to be considered
a pseudoscience, but it’s been making a comeback in recent years with the help of machine learning. One
reason it fell into disrepute is because, historically, it was sometimes applied across races in fallacious and
insulting ways. But if you want to train classifiers that guess people’s personalities with some accuracy, you
probably need a di fferent classifier for each race. This is a classifier trained exclusively for one race, Han
Chinese, which is probably part of why it works as well as it does. If you tried to train one classifier to work
on many di fferent races, I suspect its performance would be much worse.
Another thing that’s notable is that the authors were able to find statistically significant correlations for some
personality traits, but the majority of traits defeated them. So while physiognomy has some predictive power,
it’s only weakly predictive. It’s an open question whether machine learning will ever be able to predict
personality from visual information substantially better than this or not. Adding a time dimension and
incorporating people’s movements and dynamic facial expressions seems like a promising way to improve
personality predictions.
Tools like this raise some ethical issues. The one that concerns me the most is that, if tools like this are
emerging now, many governments probably already had similar tools ten years ago, and have probably been
using them to profile us.
One student asked whether these methods might be used by employers to screen prospective employees.
I think that tools like this are inferior to simply giving an interviewee a personality test. Such tests are legal
in the USA, so long as their questions are not found to violate an employee’s right to privacy and the results
are not used to discriminate against legally protected groups. The most troubling part of using physiognomy
to screen employees would not be that personality testing is unlawful. (It isn’t, and quite a few companies
do it.) It would be that physiognomy isn’t nearly accurate enough. An employer who uses a poorly designed
or unvalidated personality test to make personnel decisions might run a higher risk that a court might rule
that the test could have a discriminatory e ffect, violating Title VII of the Civil Rights Act of 1964. Also,
they probably won’t make good decisions. But perhaps in the future, better measurements, better statistical
procedures, and better algorithms might overcome these problems.
Bonus Lecture: High Dimensions; Random Projection 183
E Bonus Lecture: High Dimensions; Random Projection
THE GEOMETRY OF HIGH-DIMENSIONAL SPACES
[High-dimensional geometry sometimes acts in ways that are completely counterintuitive, defying our intu-
itions from low-dimensional geometry.]
Consider a random point p∼N(0,I)∈Rd.
What is the distribution of its length?
[Looking at the one-dimensional normal distribu-
tion, you would expect it to be very common that
the length is close to zero, a bit less common that
the length is close to 1 or −1, and not rare for the
length to be close to 2 or −2. But in high dimen-
sions, that intuition is completely wrong.]
-3 -2 -1 1 2 3x0.10.20.30.4f(x) normal.pdf [A one-dimensional normal distribution.]
[If the dimension is very high, the vast majority of the random points are at approximately the same distance
from the mean. So they lie in a thin shell. Why? To answer that, let’s study the square of the distance. By
Pythagoras’ Theorem, the squared distance from pto the mean is]
∥p∥2=p2
1+p2
2+...+p2
d
[Each component piis sampled independently from a univariate normal distribution with mean zero and
variance one. The square of a component, p2
i, is said to come from a chi-squared distribution . So is∥p∥2.]
pi∼N(0,1),p2
i∼χ2(1),E[p2
i]=1,Var(p2
i)=2,∥p∥2∼χ2(d)
[Recall that when you add dindependent, identically distributed random numbers, you scale their mean and
variance by d, and the standard deviation is the square root of the variance.]
E[∥p∥2]=d E[p2
1]=d
Var(∥p∥2)=dVar(p2
1)=2d
SD(∥p∥2)=√
2d
For large d,∥p∥is concentrated in a thin shell around radius√
dwith a thickness proportional to4√
2d.
[The mean value of ∥p∥isn’t exactly√
d, but it is close, because the mean of ∥p∥2isdand the standard
deviation is much, much smaller. Likewise, the standard deviation of ∥p∥isn’t exactly4√
2d, but it’s close.]
[So if dis about a million, imagine a million-dimensional egg whose radius is about 1,000, and the thickness
of the shell is about 67, which is about 10 times the standard deviation. The vast majority of random points
are in the eggshell. Not inside the egg; actually in the shell itself. It is counterintuitive that random vectors
sampled from a high-dimensional normal distribution almost all have almost the same length.]
[There is a statistical principle hiding here. Suppose you want to estimate the mean of a distribution—in
this case, the distribution χ2(1). The standard way to do that is to sample very many numbers from the
distribution and take their mean. The more numbers you sample, the more accurate your estimate is—that
is, the smaller the standard deviation of your sample mean is. When we sample a vector from a million-
dimensional normal distribution and compute its length, that’s exactly what we’re doing!]
184 Jonathan Richard Shewchuk
What about a uniform distribution? Consider concentric spheres of radii r&r−ϵ.
r−ϵ
r
[Draw this by hand concentric.pdf ] [Concentric balls. In high dimensions, almost every
point chosen uniformly at random in the outer ball lies outside the inner ball.]
V olume of outer ball ∝rd
V olume of inner ball ∝(r−ϵ)d
Ratio of inner ball volume to outer =
(r−ϵ)d
rd=
1−ϵ
rd
≈exp 
−ϵd
r!
which is small for large d.
E.g., ifϵ
r=0.1 &d=100,inner ball has 0 .9100=0.0027% of volume.
Random points from uniform distribution in ball: nearly all are in thin outer shell.
” ” ” Gaussian ” : nearly all are in some thin shell.
Consequences:
– In high dimensions, sometimes the nearest neighbor and 1,000th-nearest neighbor don’t di ffer much!
–k-means clustering and nearest neighbor classifiers are less e ffective for large d.
Angles between Random Vectors
What is the angle θbetween a random p∼N(0,I)∈Rdand an arbitrary q∈Rd?
Without loss of generality, set q=[1 0 0...0]⊤.
[The value of qdoesn’t matter, because the direction that ppoints in is uniformly distributed over all possible
directions. By a formula we learned early this semester, the angle between pandqisθ, where . . . ]
cosθ=p·q
∥p∥∥q∥=p1
∥p∥
E[cosθ]=0; SD(cos θ)≈SD(p1)√
d=1√
d
Ifdis large, cos θis almost always very close to zero; θis almost always very close to 90◦!
[In high-dimensional spaces, two random vectors are almost always very close to orthogonal. To put it
another way, an arbitrary vector is almost orthogonal to the vast majority of all the other vectors!]
[A former CS 189 /289A head TA, Marc Khoury, has a nice short essay entitled “Counterintuitive Properties
of High Dimensional Space”, which you can read at https: //people.eecs.berkeley.edu /∼jrs/highd ]
Bonus Lecture: High Dimensions; Random Projection 185
RANDOM PROJECTION
An alternative to PCA as preprocess for clustering, classification, regression.
Approximately preserves distances between points!
[We project onto a random subspace instead of the PCA subspace, but sometimes it preserves distances
better than PCA. Because it roughly preserves the distances, algorithms like k-means clustering and nearest
neighbor classifiers will give similar results to what they would give in high dimensions, but they run much
faster. It works best when you project a very high-dimensional space to a medium-dimensional space.]
Pick a small ϵ, a smallδ, and a random subspace S⊂Rdof dimension k, where k=&2 ln(1/δ)
ϵ2/2−ϵ3/3'
.
For any pt q, let ˆqbe orthogonal projection of qonto S, multiplied byq
d
k.
[The multiplication by√d/khelps preserve the distances between points after you project.]
Johnson–Lindenstrauss Lemma (modified):
For any two pts q,w∈Rd, (1−ϵ)∥q−w∥2≤∥ˆq−ˆw∥2≤(1+ϵ)∥q−w∥2with probability≥1−2δ.
Typical values: ϵ∈[0.02,0.5],δ∈[1/n3,0.05]. [You choose ϵandδaccording to your needs.]
[The squared distance between two points after projecting changes by less than 2%, or less than 50%, as you
wish. In practice, experiment with kto find the best speed-accuracy tradeo ff. If you want all inter-sample-
point distances to be accurate, you should set δsmaller than 1 /n2, so you need a subspace of dimension
Θ(logn). Reducing δdoesn’t cost much (because of the logarithm), but reducing ϵcosts more. You can
bring 1,000,000 sample points down to a 10,000-dimensional space with at most a 6% error in the distances.]
[What is remarkable about this result is that the dimension dof the input points doesn’t matter!]
JL Experiments
Data: 20-newsgroups, from 100.000 features to 1.000 (1%)
MATLAB implementation: 1/sqrt(k).*randn(k,N)%*%X .
100000to1000.pdf [Comparison of inter-point distances before and after projecting points
in 100,000-dimensional space down to 1,000 dimensions.]
[Why does this work? A random projection of q−wis like taking a random vector and selecting kcompo-
nents. The mean of the squares of those kcomponents approximates the mean for the whole population.]
[How do you get a uniformly distributed random projection direction? You can choose each component
from a univariate Gaussian distribution, then normalize the vector to unit length. How do you get a random
subspace? You can choose krandom directions, then use Gram–Schmidt orthogonalization to make them
mutually orthonormal. Interestingly, Indyk and Motwani show that if you skip the expensive normalization
and Gram–Schmidt steps, random projection still works almost as well, because random vectors in a high-
dimensional space are nearly equal in length and nearly orthogonal to each other with high probability.]
